==== notes sur videos oreilly ansible v2 ====

=== Yaml syntax :  ===
Yet Another Markup Language : c'est le language utilisé pour ecrire des playbooks : blocs de code ansible.

un doc yaml commence par ---

---

# les commentaires sont possibles et commencent par un #
on a plusieurs type de données dans un yaml. ex :

-dictionaires de type clé/valeur.
name: gorgonzola youki
occupation:  cheese tester
state: paris

- list :
elles s'ecrivent sous la forme :
- gorgonzola
- brie
- gouda

dans ansible les lists sont de ce type :

cheeses:
 - gorgonzola
 - brie
 - gouda
 
on peut egalement pour gagner en place ou en clarté ecrire les listes entre [ ] 
cheeses: ['gorgonzola', 'brie', 'gouda']

on va pouvoir également quotter nos chaines de caracteres quand il y a des caractères spéciaux et même des variables.
Dans yaml et ansible tout ce qui est situé après un ":" est considéré comme appartenant a un dictionnaire.
ex :

cheese: "{{my_cheese}}" < ici on encadre une variable ansible {{}} par des " " 


== creation de notre env de test : ==

on va passer par vagrant et virtualbox en provider 
2 box vont nous servir :

vagrant box add  bento/centos-7.2
vagrant box add  ubuntu/xenial64

== installation de ansible : ==

1/ sur notre ubuntu 

on se loggue en ssh : vagrant ssh :

on va pouvoir passer par les repos de type ppa: 

apt-add-repository ppa:ansible/ansible

ubuntu@ubuntu-xenial:~$ sudo apt-add-repository ppa:ansible/ansible
 Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. Avoid writing scripts or custom code to deploy and update your applications— automate in a language that approaches plain English, using SSH, with no agents to install on remote systems.

http://ansible.com/
 More info: https://launchpad.net/~ansible/+archive/ubuntu/ansible
Press [ENTER] to continue or ctrl-c to cancel adding it

gpg: keyring `/tmp/tmpbrtp7rr4/secring.gpg' created
gpg: keyring `/tmp/tmpbrtp7rr4/pubring.gpg' created
gpg: requesting key 7BB9C367 from hkp server keyserver.ubuntu.com
gpg: /tmp/tmpbrtp7rr4/trustdb.gpg: trustdb created
gpg: key 7BB9C367: public key "Launchpad PPA for Ansible, Inc." imported
gpg: Total number processed: 1
gpg:               imported: 1  (RSA: 1)
OK


on update les repos et on install ansible. On verifie ensuite la version de notre appli :

sudo apt-get update && sudo apt install ansible
ubuntu@ubuntu-xenial:~$ ansible --version
ansible 2.3.2.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.12 (default, Nov 19 2016, 06:48:10) [GCC 5.4.0 20160609]

on a donc la derniere version dispo en cours sur notre server

Sur centos / redhat il va falloir gérer un peu différement :

- yum repolist : 

[root@localhost ~]# yum repolist
Modules complémentaires chargés : fastestmirror
base                                              | 3.6 kB     00:00     
extras                                            | 3.4 kB     00:00     
updates                                           | 3.4 kB     00:00     
(1/4): base/7/x86_64/group_gz                       | 155 kB   00:01     
(2/4): extras/7/x86_64/primary_db                   | 191 kB   00:02     
(3/4): base/7/x86_64/primary_db                     | 5.6 MB   00:13     
(4/4): updates/7/x86_64/primary_db                  | 7.8 MB   00:18     
Determining fastest mirrors
 * base: mirror.guru
 * extras: mirror.guru
 * updates: centos.mirrors.ovh.net
id du dépôt                     nom du dépôt                       statut
base/7/x86_64                   CentOS-7 - Base                    9 363
extras/7/x86_64                 CentOS-7 - Extras                    450
updates/7/x86_64                CentOS-7 - Updates                 2 146
repolist: 11 959

Nous n'avons pas de paquet ansible, on va devoir ajouter un repo fedora :

[root@localhost ~]# wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm

puis l'installer 
[root@localhost ~]# rpm -ivh epel-release-latest-7.noarch.rpm
on update :
[root@localhost ~]# yum repolist
puis on install ansible :
[root@localhost ~]# yum install ansible

[root@localhost ~]# ansible --version
ansible 2.2.9.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.5 (default, Nov 20 2015, 02:00:19) [GCC 4.8.5 20150623 (Red Hat 4.8.5-4)]



Si la version n'est pas assez elevée on peut intégrer les versions testing :
on install le package qui va contenir le binaire nous permettant de gérer les repo "testing" 
[root@localhost ~]# yum install yum-utils.noarch
on active le repo testing :
[root@localhost ~]# yum-config-manager --enable epel-testing

[root@localhost ~]# yum-config-manager --enable epel-testing
Modules complémentaires chargés : fastestmirror
========================== repo: epel-testing ===========================
[epel-testing]
async = True
bandwidth = 0
base_persistdir = /var/lib/yum/repos/x86_64/7
baseurl = 
cache = 0
cachedir = /var/cache/yum/x86_64/7/epel-testing
check_config_file_age = True
compare_providers_priority = 80
cost = 1000
deltarpm_metadata_percentage = 100
deltarpm_percentage = 
enabled = 1
enablegroups = True
exclude = 
failovermethod = priority
ftp_disable_epsv = False
gpgcadir = /var/lib/yum/repos/x86_64/7/epel-testing/gpgcadir
gpgcakey = 
gpgcheck = True
gpgdir = /var/lib/yum/repos/x86_64/7/epel-testing/gpgdir
gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
hdrdir = /var/cache/yum/x86_64/7/epel-testing/headers
http_caching = all
includepkgs = 
ip_resolve = 
keepalive = True
keepcache = False
mddownloadpolicy = sqlite
mdpolicy = group:small
mediaid = 
metadata_expire = 21600
metadata_expire_filter = read-only:present
metalink = https://mirrors.fedoraproject.org/metalink?repo=testing-epel7&arch=x86_64
minrate = 0
mirrorlist = 
mirrorlist_expire = 86400
name = Extra Packages for Enterprise Linux 7 - Testing - x86_64
old_base_cache_dir = 
password = 
persistdir = /var/lib/yum/repos/x86_64/7/epel-testing
pkgdir = /var/cache/yum/x86_64/7/epel-testing/packages
proxy = False
proxy_dict = 
proxy_password = 
proxy_username = 
repo_gpgcheck = False
retries = 10
skip_if_unavailable = False
ssl_check_cert_permissions = True
sslcacert = 
sslclientcert = 
sslclientkey = 
sslverify = True
throttle = 0
timeout = 30.0
ui_id = epel-testing/x86_64
ui_repoid_vars = releasever,
   basearch
username = 

on upgrade 
[root@localhost ~]# yum upgrade ansible
[root@localhost ~]# ansible --version
ansible 2.3.1.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.5 (default, Nov 20 2015, 02:00:19) [GCC 4.8.5 20150623 (Red Hat 4.8.5-4)]
[root@localhost ~]# ansible --version
ansible 2.3.1.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides
  python version = 2.7.5 (default, Nov 20 2015, 02:00:19) [GCC 4.8.5 20150623 (Red Hat 4.8.5-4)]


== ansible configuration =

Pour nos exemples  on va utiliser une conf speciale de ansible couplée a vagrant . Ansible va être notre provisionner par lequel nos vm pourront être contactée
On va specifier a vagrant qu'on va utiliser ansible sur notre box et utiliser une arbo présente sur notre server local :
http://docs.ansible.com/ansible/latest/guide_vagrant.html

on dit a vagrant d'utiliser ansible pour executer le fichier playbook.yml présent dans notre arbo locale provisioning/playbook.yml : qui contiendra notre conf ansible. On ajoute agalement un fichier hosts dans lequel on stockera notre parc server.
On rajoute egalement l'option verbose qui est particulièrement utile pour le debug ansible pour vagrant.
  config.vm.provision "ansible" do |ansible|
  ansible.verbose = "v"
  ansible.playbook = "provisioning/playbook.yml"
  end

on va donc creer notre arbo qui nous servira de base de travail sur notre server ansible : 
 tree working_dir/
working_dir/
├── provisioning
│   └── playbooks
│       └── hosts
├── ubuntu-xenial-16.04-cloudimg-console.log
└── Vagrantfile

cette arbo sera donc visible depuis notre box vagrant via le point de montage : 
ubuntu@ubuntu-xenial:~$ ls -l /vagrant/
total 48
drwxr-xr-x 1 ubuntu ubuntu  4096 Aug 20 09:01 provisioning
-rw------- 1 ubuntu ubuntu 40390 Aug 20 09:22 ubuntu-xenial-16.04-cloudimg-console.log
-rw-r--r-- 1 ubuntu ubuntu  3121 Aug 20 08:52 Vagrantfile


On va copier la clé pub de notre user dans l'authorized key du user root de notre box
On renseigne le host de la manière suivante pour effectuer les premiers tests :

/Documents/lab/ansible/ansible_oreilly_video/vm/provisioning/playbooks$ cat hosts 
server {{ ntpserver }}
Debianubuntu-ansible ansible_ssh_host=127.0.0.1 ansible_ssh_port=2222 ansible_ssh_user=root ansible_ssh_private_key_file=/home/boogie/.ssh/id_rsa

/!\ on peut overrider la config ansible dont la base se situe dans le fichier /etc/ansible/ansible.cfg
on peut par exemple setter en direct la clé priv avec laquelle les authent seront faites dans le fichier /etc/ansible/ansible.cfg
private_key_file = /home/boogie/.ssh/id_rsa
et donc alleger notre fichier hosts :

ubuntu-ansible ansible_ssh_host=127.0.0.1 ansible_ssh_port=2222 ansible_ssh_user=root

on va donc faire notre premier test :
on invoque ansible en lui passant comme argument le server renseigné dans le host <ubuntu-ansible> -i fichier d'inventaire <hosts> ici et -m module utilisé <ping> ici 
:


ansible ubuntu-ansible -i hosts -m ping
ubuntu-ansible | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}

= host settings : =

on peut definir dans notre fichier d'inventaire plusieurs type d'info 
un host, une ip , un host écoutant sur un port particulier , un groupe de server , definir des variables qui seront héritées pour tout un groupe de server .....(même si ce n'est pas une pratique recommandée il peut être utile de le savoir.)

#Making a comment in my hosts file

rainbows.unicorns.com

[testServer]
52.36.167.137 ansible_user=ec2-user ansible_ssh_user=ec2-user  <<< setting de connection particulier pour notre server
test1.example.com:5555  <<< port ssh différent du standart 

[testServer:vars]
ansible_user=ec2-user <<< setting de variables héritées pour le groupe testServer

[webServers]
apache[01:50].example.com <<< range de server avec range en chiffre
nginx[50:100].example.com <<< range de server avec range en chiffre.

[appServers]
app[a:f].example.com    <<< range de server avec range en lettre.

== fichier de conf ansible : =

ordre de priorité de lecture :

$ANSIBLE_CONFIG path de la variable $ANSIBLE_CONFIG si elle est définie
$PWD repertoire courant
$HOME_DIR /home/user
Install par defaut (/etc/ansible)

Ce concept est essentiel dans ansible  : on peut donc overrider notre conf a plusieurs endroits.

les differents blocs de config sont nombreux on peut voir les param généraux, ssh ....

[defaults]  correspondont au param généraux.

ask_pass < permet de demander ou pas un passwd a la connex
fork < permet de définir le nombre de process lancé pendant la connexion a un host . Plus on a de host a gérer dans notre playbook plus ce param est a augmenter pour améliorer le temps de traitement du run.
library < permet de définir l'endroit dans lequel sont stockés les modules.on peut biensur en ajouter et modifier ce path 
remote_user = root < permet de definir le user qui executera le code ansible
private_key_file = /home/boogie/.ssh/id_rsa < definir la clé priv du user qui gérera la connexion 
timeout < permet de definir le temps max pour etablir une connexion avec un host 
transport = smart < permet de definir la maniere dont les run seront traités : openssh en premier sinon on devra définir specifiquement.

inventory      = /etc/ansible/hosts
library        = /usr/share/my_modules/
forks          = 5
ask_pass       = True
transport      = smart
remote_port    = 22
remote_user    = root 
private_key_file = /home/boogie/.ssh/id_rsa

== commandes integrée : ==

on va pouvoir executer des commandes directement :
l'enchainement sera toujours a peu pret identique :

ansible <machine/groupe de machine> -i <inventaire pour le run> -m module < option de modules> -b < permet de devenir become root


- commandes systeme :
le module command nous permet de passer des commandes systeme à nos hosts :

ooks$ ansible ubuntu -i hosts -m command -a "w"
ubuntu | SUCCESS | rc=0 >>
 17:27:20 up  8:05,  2 users,  load average: 0.00, 0.00, 0.00
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
ubuntu   pts/0    10.0.2.2         09:24   16.00s  0.18s  0.18s -bash
root     pts/1    10.0.2.2         17:27    0.00s  0.07s  0.00s w

boogie@apollo:~/Documents/lab/ansible/ansible_oreilly_video/working_dir/provisioning/playbooks$ ansible ubuntu -i hosts -m command -a "service nginx status"
ubuntu | SUCCESS | rc=0 >>
● nginx.service - A high performance web server and a reverse proxy server
   Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)
   Active: active (running) since Sun 2017-08-20 17:23:13 UTC; 4min 35s ago
  Process: 4085 ExecStop=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid (code=exited, status=0/SUCCESS)
  Process: 4182 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS)
  Process: 4179 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS)
 Main PID: 4184 (nginx)
    Tasks: 3
   Memory: 2.2M
      CPU: 58ms
   CGroup: /system.slice/nginx.service
           ├─4184 nginx: master process /usr/sbin/nginx -g daemon on; master_process on
           ├─4185 nginx: worker process                           
           └─4186 nginx: worker process                           

Aug 20 17:23:13 ubuntu-xenial systemd[1]: Starting A high performance web server and a reverse proxy server...
Aug 20 17:23:13 ubuntu-xenial systemd[1]: Started A high performance web server and a reverse proxy server.

- setup : va nous permettre de récuperer les infos du/des  server(s) 
ansible ubuntu -i hosts -m setup |head
ubuntu | SUCCESS => {
    "ansible_facts": {
        "ansible_all_ipv4_addresses": [
            "10.0.2.15"
        ], 
        "ansible_all_ipv6_addresses": [
            "fe80::2b:21ff:fe2a:210d"
        ], 
        "ansible_architecture": "x86_64", 
        "ansible_bios_date": "12/01/2006", 

- apt / yum : va nous permettre de gerer via les providers de packages des install a des versions precises de packages :

ansible ubuntu -i hosts -m apt -a "name=tree state=latest" -b 
ubuntu | SUCCESS => {
    "cache_update_time": 1503248938, 
    "cache_updated": false, 
    "changed": true, 
    "stderr": "", 
    "stdout": "Reading package lists...\nBuilding dependency tree...\nReading state information...\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 5 not upgraded.\nNeed to get 40.6 kB of archives.\nAfter this operation, 138 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu xenial/universe amd64 tree amd64 1.7.0-3 [40.6 kB]\nFetched 40.6 kB in 0s (268 kB/s)\nSelecting previously unselected package tree.\r\n(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 56949 files and directories currently installed.)\r\nPreparing to unpack .../tree_1.7.0-3_amd64.deb ...\r\nUnpacking tree (1.7.0-3) ...\r\nProcessing triggers for man-db (2.7.5-1) ...\r\nSetting up tree (1.7.0-3) ...\r\n", 
    "stdout_lines": [
        "Reading package lists...", 
        "Building dependency tree...", 
        "Reading state information...", 
        "The following NEW packages will be installed:", 
        "  tree", 
        "0 upgraded, 1 newly installed, 0 to remove and 5 not upgraded.", 
        "Need to get 40.6 kB of archives.", 
        "After this operation, 138 kB of additional disk space will be used.", 
        "Get:1 http://archive.ubuntu.com/ubuntu xenial/universe amd64 tree amd64 1.7.0-3 [40.6 kB]", 
        "Fetched 40.6 kB in 0s (268 kB/s)", 
        "Selecting previously unselected package tree.", 
        "(Reading database ... ", 
        "(Reading database ... 5%", 
        "(Reading database ... 10%", 
        "(Reading database ... 15%", 
        "(Reading database ... 20%", 
        "(Reading database ... 25%", 
        "(Reading database ... 30%", 
        "(Reading database ... 35%", 
        "(Reading database ... 40%", 
        "(Reading database ... 45%", 
        "(Reading database ... 50%", 
        "(Reading database ... 55%", 
        "(Reading database ... 60%", 
        "(Reading database ... 65%", 
        "(Reading database ... 70%", 
        "(Reading database ... 75%", 
        "(Reading database ... 80%", 
        "(Reading database ... 85%", 
        "(Reading database ... 90%", 
        "(Reading database ... 95%", 
        "(Reading database ... 100%", 
        "(Reading database ... 56949 files and directories currently installed.)", 
        "Preparing to unpack .../tree_1.7.0-3_amd64.deb ...", 
        "Unpacking tree (1.7.0-3) ...", 
        "Processing triggers for man-db (2.7.5-1) ...", 
        "Setting up tree (1.7.0-3) ..."
    ]
}

si on relance notre commande il ne se passera rien car le paquet est dejà installé :

ansible ubuntu -i hosts -m apt -a "name=tree state=latest" 
ubuntu | SUCCESS => {
    "cache_update_time": 1503248938, 
    "cache_updated": false, 
    "changed": false
}

ansible est indempotent
on va installer nginx et verifier si le service est démarré : 
ansible ubuntu -i hosts -m apt -a "name=nginx state=latest" 

on arrete nginx sur notre host :
ubuntu@ubuntu-xenial:/vagrant$ sudo service nginx stop

ubuntu@ubuntu-xenial:/vagrant$ pgrep nginx
ubuntu@ubuntu-xenial:/vagrant$ 

 ansible ubuntu -i hosts -m service -a "name=nginx state=started" -b
ubuntu | SUCCESS => {
    "changed": true, 
    "name": "nginx", 
    "state": "started", 
    "status": {
on voit bien notre service up maintenant :

ubuntu@ubuntu-xenial:/vagrant$ pgrep nginx
4184
4185
4186


== help == 


- help sur la doc : 
ansible-doc --help

- recherche de doc sur keyword générique : 
ansible-doc -l |grep bigip
bigip_device_dns                   Manage BIG-IP device DNS settings                  
bigip_device_ntp                   Manage NTP servers on a BIG-IP                     
...
- doc sur un module : 
ansible-doc bigip_device_dns


== playbook intro ==

les playbooks sont les blocks de construction de code ansible.On peut faire de la config, de l'orchestration et du deploiement via des playbooks.
Ils sont rédigés en yaml et on des structures définis : les plays 
exemple :
play.yml

---
- name: ici on decrit ce que notre playbook/play va faire pour avoir du code lisible.
  hosts: ici on defini notre / nos targer : host, group etc ...
  remote_user: user_dedie si on ne l'a pas deja renseigné dans notre conf globale ou hosts
  become: yes : ici on peut definir si notre user dedié pourra executer les cmds en root

  vars:
    vardefinie: on declare ici des variable qui pourront être alimentée
  tasks: ici on defini les taches qui seront faite sequentiellement et qui pourront être rejouees tant qu'il y a des erreurs.
    - name : on decrit les tasks
      template: src=mylocal_file dst=path_of_file/on_remote_server :on peut utiliser un template : un fichier source qui sera pri de notre env local et poussé dans l'arbo désiree sur notre server.
      notify: on pourra eventuellement faire une action si notre conf est modifier : restarter un service par exemple : pour cela on devra creer une section handler qui actionnera le redemarrage
  handlers: 
  - name : on donne une description
    service: name=nomduservice state=action voulue 


exemple : 
---
- name: install and configure mariadb
  hosts: centos
  become: yes

  vars:
    mysql_port: 3307

  tasks:
    - name: install mariadb server
      yum: name=mariadb-server state=latest
      
    - name: add the config file
      template: src=my.cnf.j2 dest=/etc/my.cnf
      notify: restart mariadb

    - name: create mariadb log file
      file: path=/var/log/mysqld.log state=touch owner=mysql group=mysql mode=0775
    
    - name: start mariadb service
      service: name=mariadb state=started enabled=yes

  handlers:  
  - name: restart mariadb
    service: name=mariadb state=restarted


On va créer notre template my.cnf 

 ~/Documents/lab/ansible/provisionning  $  cat my.cnf.j2 
[mysqld)
datadir=/var/lib/mysql
socket=/var/lib/mysql/mysql.sock
user=mysql

# disabled symlinks 
symbolic-links=0
port={{mysql_port}}

[mysql_safe]
log-error=/var/log/mysqld.log
pid-file=/var/run/mariadb/mysql.pid

Pour executer notre playbook on lance donc :
ansible-playbook -i fichier_inventaire nom_du_playbook.yml

exemple : 

ansible-playbook -i hosts mariadb.yml 

PLAY [install and configure mariadb] *******************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [install mariadb server] **************************************************
changed: [centos]

TASK [prepare main conf file] **************************************************
changed: [centos]

TASK [create mariadb log file] *************************************************
changed: [centos]
..


== variables : ==

on va pouvoir utiliser des variables dans ansible pour appliquer des conf differentes en fonction de nos serveurs.
Il y a toute une serie de parcours d'arbo hierarchiques pour appliquer une variable si elle est trouvée :
on part de la liste suivante
on override dans l'ordre suivant : 

-> roles par defaut
-> variables d'inventaire
-> groupe de variables d'inventaire
-> variable de host d'inventaire
-> groupe de variable de playbooks
-> variable de host d'un playbook
-> hosts facts
-> facts enregistrés
-> facts definis
-> variable de play
-> variable de prompt de play
-> variable de fichier de play
-> variable de Role et include 
-> variables de bloc
-> variables de tasks
-> extra variables  



On peut definir des variables au sein de notre playbook .

ex : 

on alimente notre playbook de variables au sein d'une section: 
...
vars:
  log_path: "/var/log"

qu'on appelle ensuite plus loin dans notre conf :

...
    - name: create mariadb log file
      file: path={{ log_path }}/mysqld.log state=touch owner=mysql group=mysql mode=0775

On peut pour plus de clarter definir un fichier de variables externe à notre playbook et l'appeller au sein de notre conf :


cat maria_vars.yml 
---
mysql_port: 3307
log_path: "/var/log"


dans notre playbook on remplacera la section vars par vars_file qui contiendra notre fichier de déclaration de variables; 

  vars_files:
    - maria_vars.yml

On peut aussi definir les variable dans la section hosts :
ex : 
---
- name: install and configure mariadb
  hosts: "{{ my_host }}"

qui pourra ensuite être overrider / configurer en cli :
on va donc pouvoir passer notre run sur une selection precise de machine qu'on passe en argument et qui seront ensuite recupérer pour l'execution de notre playbook : 
ansible-playbook -i hosts mariadb.yml --extra-vars "host=centos"  


- Variables "register" : souvent utilisées avec les commandes du module  shell

ex :

---
- name: testing misc variables
  hosts: centos

  tasks:
  - name: get the date of our server
    shell: date
    register: output
     
  -debug: msg="the date is {{ output.stdout }}" 

on va donc enregister dans la variable output notre date puis l'afficher via une task debug : 

 ansible-playbook -i hosts vars_cases.yml 

PLAY [testing misc variables] **************************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [get the date of our server] **********************************************
changed: [centos]

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": "the date is lun. août 28 11:01:31 UTC 2017"
}

PLAY RECAP *********************************************************************
centos                     : ok=3    changed=1    unreachable=0    failed=0   
 

- Variables facts : 
on va pouvoir travailler avec les variables récupérées des facts présents sur nos serveurs : 

ex on peut regrouper nos servers via des facts qui serviront pour le tri : 

  - debug: var=ansible_distribution_version
  
  - name: group some machines temporaly
    group_by: key=rhel_{{ansible_distribution_version}}
    register: group_result
 
  - debug: var=group_result  

ex : 
 $  ansible-playbook -i hosts vars_cases.yml 
..

TASK [debug] *******************************************************************
ok: [centos] => {
    "ansible_distribution_version": "7.3.1611"
}

TASK [group some machines temporaly] *******************************************
ok: [centos]

TASK [debug] *******************************************************************
ok: [centos] => {
    "group_result": {
        "add_group": "rhel_7.3.1611", 
        "changed": false
    }
}

PLAY RECAP *********************************************************************
centos                     : ok=6    changed=1    unreachable=0    failed=0   


==  conditionals : ==

On peut controler l'execution de nos playbooks avec des tests qui permettront ou pas de realiser l'action 
ex : 

 ~/Documents/lab/ansible/provisionning  $  cat conditionals.yml 
---
- name: testing some conditionals 
  hosts: centos
  become: yes

  vars:
    lapin: true   << on set une variable a vrai 

  tasks:
  - name: do not check on debian server
    yum: name=tree state=latest
    #when: ansible_os_family=="Redhat"   <<< on test si notre os est une red hat 
    when: (ansible_os_family=="Redhat" and ansible_distribution_major_version=="6")   <<< on peut cumuler avec des opérateurs booleens AND OR ....

  - name: lapin is ral or fake ?
    shell: echo "lapin is fake!!" 
    when: not lapin      <<<< on va tester et afficher lapin is fake si on a pas defini lapin en variable 


 ansible-playbook -i hosts conditionals.yml 

PLAY [testing some conditionals] ***********************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [do not check on debian server] *******************************************
skipping: [centos]

TASK [lapin is ral or fake ?] **************************************************
skipping: [centos]

PLAY RECAP *********************************************************************
centos                     : ok=1    changed=0    unreachable=0    failed=0   


on va pouvoir afficher notre message si on enregistre le dans une variable notre commande shell et qu'on l'affiche ensuite ;

  - name: lapin is ral or fake ?
    shell: echo "lapin is fake!!" 
    register: output
    when: lapin

  - debug: msg="for sure "{{ output.stdout }}""


 ~/Documents/lab/ansible/provisionning  $  ansible-playbook -i hosts conditionals.yml 

PLAY [testing some conditionals] ***********************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [do not check on debian server] *******************************************
skipping: [centos]

TASK [lapin is ral or fake ?] **************************************************
changed: [centos]

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": "for sure \"lapin is fake!!\""
}

PLAY RECAP *********************************************************************
centos                     : ok=3    changed=1    unreachable=0    failed=0   


== loops : ==
on va  pouvoir se servir des bloucles pour permettre la gestion multiples d'actions.Plusieurs types de boucles existent.

- with_items : est une forme de boucle très utilisée dans ansible.
On va pouvoir itérer un nombre le nombre de fois nécéssaire sur une liste.

exemple pour l'installation de plusieurs paquets on va definir une variable qui va récupérer chaque entrée d'une liste et procéder à l'install :

  tasks: 
  - name: install tools 
    yum: name={{ item }} state=installed
    with_items: 
    - lsof
    - tree
    - nmap
- with_dict : va nous permettre d'iterer sur un dictionnaire 


- name: testing loops
host: centos

  tasks:
  - name: looping other env facts
    debug: msg={{item.key}}=>>{{item.value}}
    with_dict: "{{ ansible_env }}"

- with_files , with_fileglob etc ..: on va opérer sur les elements de fichier : 

  - name: looping over files
    copy: src={{item}} dest=/tmp/loops   <<< on defini nos rep de source et destination
    with_fileglob: "/tmp/*.conf"    <<<< ici on defini le pattern de nos fichiers source qui seront copiés sur la dest.


- until : une autre forme de loop :

  - name: do until something 
    shell: echo "hello!!"
    register: output
    retries: 5
    delay: 5
    until: output.stdout.find('hello!!') != -1

    
== blocks : ==

Ils vont nous permettre de regrouper des tasks logiquement entre elles.C'est utile avec les tests condtionnels et les escalades de privileges nécéssaire pour l'execution des taches multiples.

ex : on va pouvoir ici installer toute une conf mariadb si notre os est de type RedHat 



- name: install and configure mariadb
  hosts: centos
  strategy: debug

  vars_files:
    - maria_vars.yml

  tasks:
    - blocks:
       - name: install mariadb server
         yum: name=mariadb-server state=latest
 
       - name: prepare main conf file
         template: src='my.cnf.j2' dest='/etc/my.cnf'
         notify: restart mariadb
 
       - name: create mariadb log file
         file: path='/var/log/mysqld.log' state=touch owner=mysql group=mysql mode=0775
 
       - name: start mariadb service
         service: name=mariadb state=started enabled=yes
     when: ansible_os_family=="Redhat"
     become: yes

Les blocs permettent egalement une tres bonne gestion des erreurs.



---
- name: testing blocks for debug
  hosts: centos
  

  tasks:
    - block: 
      - name: copy files to dst
        copy: src=/tmp/lapin dest=/tmp/loops
      rescue:   <<<<<<<<<<<<<<<<<<<<<<< ici on pose un rescue 
        - debug: msg="Always stop for debug"   <<<< le message s'affichera en cas d'erreur
      always:   <<<<<<<<<<<<<<<<<<<<<< ici on pose un always 
        - debug: msg="always show the message !" <<<<< le message sera toujours affiché


    - block:   <<<<< on peut imbriquer des blocs
       - block:
          - block:
              - debug: msg="nesting some blocs!"

ce qui nous donne lors de notre run :

 ~/Documents/lab/ansible/provisionning  $  ansible-playbook -i hosts blocks.yml 

PLAY [testing blocks for debug] ************************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [copy files to dst] *******************************************************
changed: [centos]

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": "always show the message !"
}

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": "nesting some blocs!"
}

PLAY RECAP *********************************************************************
centos                     : ok=4    changed=1    unreachable=0    failed=0 

et  en cas d'erreur (ex on supprime notre fichier en src) : le message de debug d'erreur s'affiche bien :


 ~/Documents/lab/ansible/provisionning  $  ansible-playbook -i hosts blocks.yml 

PLAY [testing blocks for debug] ************************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [copy files to dst] *******************************************************
fatal: [centos]: FAILED! => {"changed": false, "failed": true, "msg": "Unable to find '/tmp/lapin' in expected paths."}

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": "Always stop for debug"
}

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": "always show the message !"
}

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": "nesting some blocs!"
}

PLAY RECAP *********************************************************************
centos                     : ok=4    changed=0    unreachable=0    failed=1   


=== templates : ==

le templating est tres puissant et il est possible de consulter la doc jinja pour voir l'ensemle des possibilités.
jinja.pocoo.org
On prend pour habitude de suffixer nos tpls avec l'extension j2 pour lisibilité.

On va utiliser les templates souvent pour alimenter la creation de fichier de conf avec des variables définies dans nos playbooks.
ex : 
 ~/Documents/lab/ansible/provisionning  $  cat my.cnf.j2 
[mysqld)
datadir=/var/lib/mysql
socket=/var/lib/mysql/mysql.sock
user=mysql

# disabled symlinks 
symbolic-links=0
port={{ mysql_port }}

[mysqld_safe]
log-error=/var/log/mysqld.log
pid-file=/var/run/mariadb/mysql.pid


On va aussi pouvoir être beaucoup plus précis et définir des regles de matching au sein du tpl.
exemple ici on va pouvoir appliquer des regles iptables en fonctions des infos matchant dans le fichier d'inventaire (ouvrir le port 80 pour les serveurs http, le 3306 pour les db ...)
$  cat iptables.j2 

# {{ ansible_managed }}   <<< ici on met un flag prouvant que le fichier est poussé par ansible
# Manual customization of this file is not recommended.
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]

{% if (inventory_hostname in groups['webservers']) or (inventory_hostname in groups['monitoring']) %}   <<<  on utilise les fonction décisionnelle de jinja pour appliquer la regle en fonction du group dans lequel se trouve le host avec {% if  .... %}
-A INPUT -p tcp  --dport 80 -j ACCEPT
{% endif %}

{% if inventory_hostname in groups['dbservers'] %}
-A INPUT -p tcp  --dport 3306 -j  ACCEPT
{% endif %}

{% if inventory_hostname in groups['lbservers'] %}
-A INPUT -p tcp  --dport {{ listenport }} -j  ACCEPT  <<< on voit ici la definition d'une variable {{ listenport }} qu'il faudra biensur avoir déclarée dans notre playbook.
{% endif %}

{% for host in groups['monitoring'] %}  <<< ici on a une boucle jinja2 : ici on boucle sur tous les hosts du group monitoring 
-A INPUT -p tcp -s {{ hostvars[host].ansible_default_ipv4.address }} --dport 5666 -j ACCEPT   <<< ici on va utiliser les variables des hosts correspondant au fact de l'ip recupérée par ansible ( ex fact: 
        "ansible_default_ipv4": {
            "address": "10.0.2.15", 
         ...
{% endfor %}
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited



on rempli ensuite notre fichierplaybook yaml pour alimenter notre template avec la variable et proceder aux actions : 
cat templates.yml
---
- name: testing templates
  hosts: testServer
  remote_user: ec2-user
  become: yes
  
  vars:
    listenport: 8888

  tasks:
  - name: insert iptables template
    template: src=iptables.j2 dest=/etc/sysconfig/iptables
    notify: restart iptables

  handlers: 
  - name: restart iptables
    service: name=iptables state=restarted  

== jinja2 filters : ==

Les fonctions jinja vont alimenter nos fichiers de conf et tpl : il y a beaucoup de fonctionalités.

On va pouvoir utiliser les filtres jinja dans les playbooks egalement 
Ex filtre avec structures conditionnelles 
on va pouvoir utiliser les filtres jinja pour agir avec le contenu d'une variables :

ex :
... 
  tasks:
  - name: quick echo
    shell: echo $PATH
    register: result   <<< le resultat de la commande shell est enregitré dans la var result 
  
  - debug: msg="The play failed"
    when: result|failed     <<< on va executer une action en fonction des test de conditions 
...ce message de debug ne s'affichera que si la commande echo $PATH echoue 


on va pouvoir definir des variables par défaut dans la conf de notre playbook : ce qui permet au run de continuer si une variable n'est pas définie en fallbackant sur la valeur de la variable par defaut

  hosts: "{{database_host | default('centos')}}"  <<< ex notre run passera sur le hosts database_host ..mais si celui-ci n'existe pas alors on fallback sur la valeur par default testServer.


on va pouvoir egalement grace aux filtres jinja appliquer des traitement sur nos variables 
ex deux listes de variables dans notre playbook : 

  vars:
    cheese: ['american', 'cheddar', 'cheddar', 'mozzarella', 'bleu', 'manchego', 'mozzarella', 'brie']
    pizza_toppings: ['mozzarella', 'anchovies', 'pineapple', 'bleu', 'pepperoni', 'mushrooms', 'ham']

on va pouvoir par ex recupérer les elements unique de la liste "cheese" : 
en passant a notre liste le | puis la fonctin unique : 

  - debug: msg="{{cheese | unique}}"

on peut recupérer les elements différents entre la liste cheese et pizza_toppings : 

  - debug: msg="{{cheese | difference(pizza_toppings)}}"

et on peut faire la liste globale des elements uniques des deux listes cheese et pizza_toppings : 

  - debug: msg="{{cheese | union(pizza_toppings)}}"



 cat filters.yml 
---
- name: testing filter options
  hosts: "{{database_host | default('centos')}}"
  become: yes

  vars:
    cheese: ['american', 'cheddar', 'cheddar', 'mozzarella', 'bleu', 'manchego', 'mozzarella', 'brie']
    pizza_toppings: ['mozzarella', 'anchovies', 'pineapple', 'bleu', 'pepperoni', 'mushrooms', 'ham']
  
  tasks:
  - name: quick echo
    shell: echo $PATH
    register: result

  - debug: msg="The play failed"
    when: result|failed

  - debug: msg="The play changed"
    when: result|changed

  - debug: msg="The play succeeded"
    when: result|success

  - debug: msg="The play skipped"
    when: result|skipped

  - debug: msg="{{cheese | unique}}"

  - debug: msg="{{cheese | difference(pizza_toppings)}}"

  - debug: msg="{{cheese | union(pizza_toppings)}}"

ce qui nous donne lors de l'execution du run : 

 ~/Documents/lab/ansible/provisionning  $  ansible-playbook -i hosts filters.yml 

PLAY [testing filter options] **************************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [quick echo] **************************************************************
changed: [centos]

TASK [debug] *******************************************************************
skipping: [centos]

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": "The play changed"
}

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": "The play succeeded"
}

TASK [debug] *******************************************************************
skipping: [centos]

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": [
        "american", 
        "cheddar", 
        "mozzarella", 
        "bleu", 
        "manchego", 
        "brie"
    ]
}

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": [
        "american", 
        "cheddar", 
        "manchego", 
        "brie"
    ]
}

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": [
        "american", 
        "cheddar", 
        "mozzarella", 
        "bleu", 
        "manchego", 
        "brie", 
        "anchovies", 
        "pineapple", 
        "pepperoni", 
        "mushrooms", 
        "ham"
    ]
}

PLAY RECAP *********************************************************************
centos                     : ok=7    changed=1    unreachable=0    failed=0   

on recupére bien le status success et changed et le resultat des traitement de liste.

== playbooks best practice : = 

on doit s'assurer que pour chaque projet tous les elements de ce projet sont bien présents dans un seul repertoire ex: 
├── ampedHostsFile
├── ansible.cfg
├── group_vars
│   ├── all
│   └── dbservers
├── hosts
├── roles
│   └── mariadb
│       ├── defaults
│       │   └── main.yml
│       ├── files
│       ├── handlers
│       │   └── main.yml
│       ├── meta
│       │   └── main.yml
│       ├── README.md
│       ├── tasks
│       │   └── main.yml
│       ├── templates
│       ├── tests
│       │   ├── inventory
│       │   └── test.yml
│       └── vars
│           └── main.yml

le fichier d'inventaire
le fichier playbook principal qu'on peut nommer par convention site.yml
le fichier de config ansible
un repertoire comportant les differents variables dédiées a certains hosts/ group
un repertoire role qui contiendra tous les éléments nécéssaires à notre playbook

on va pouvoir regrouper precisement nos serveurs dans le fichier d'inventaire ex 
..
[webServers:children]
east-webservers.example.com
west-webservers.example.com
mais il peut être plus judicieux de grouper les variables correspondantes aux hosts dediés dans les rep group_vars
ex : les variables des server web , celle des server de l'east ...

on peut definir des variables qui s'appliqeront a tout nos servers matchant se groupe 
ex : 
cat group_vars/all
---
httpd_port: 80

cat group_vars/dbservers
---
mysql_port: 3306

La best practice est cependant d'utiliser l'arbo des roles 

Pour l'ecriture des playbooks il est capital de tout garder simple : less is more 
-> nommer les playbooks , les tasks, espacer d'une ligne les tasks , toujours utiliser le parametre state et surtout versionner toutes les confs.



== includes : == 

Il va être possible de rassembler plusieurs playbooks et gérer les actions (tasks etc ...) des differents playbook au sein d'un seul fichier.
Ansible va executer toutes les playbooks de manière sequentielles du haut vers le bas.
En fonction de l'endroit ou on place notre include on aura donc des actions differentes 

pour utiliser un include il suffit de placer le keyword include suivi du playbook a jouer à l'endroit precis de notre playbook principal.
ex :
on va ajouter un playbook selinux qui va contenir simplement deux taches ,  dans notre playbook mariadb.yml au debut de notre playbook 

mariadb.yml : 
---
- name: install and configure mariadb
  hosts: testServer
  remote_user: ec2-user
  become: yes 

  vars_files:
   - maria_vars.yml

  tasks:
    - include: selinux.yml  <<< on defini ici notre include : sans preciser le path de notre fichier qui se trouve au même niveau que notre playbook mariadb.yml dans notre ex. Sinon il faut preciser le chemin complet.

    - block:  
       - name: install mariadb
         yum: name=mariadb-server state=latest

       - name: create mysql configuration file
         template: src=my.cnf.j2 dest=/etc/my.cnf
         notify: restart mariadb
..
...
Biensur si on place l'include a la fin du fichier les taches habituelles de notre playbook mariadb.yml seront executées en premier.
ex : rajouter le playbook d'un webserver 
- include: webserver.yml


Les includes sont les precurseurs des roles : ont peu utiliser les deux.

Le detail de notre fichier selinux.yml nous montre qu'il ne contient que deux tasks simples.

cat selinux.yml : 

---
- name: Install python bindings for SE Linux
  yum: name={{ item }} state=present
  with_items:
   - libselinux-python
   - libsemanage-python
   
- name: test to see if selinux is running
  command: getenforce


== roles : ==


- structures : 

La structure du repertoire role est très importante car elle va comporter tous les éléments nécéssaires à notre environement et ansible va automatiquement y chercher les variables, tasks, templates et handlers ...et va donc nous permettre de partager differents composants a travers differents groupes.
Une commande va nous permettre de créer notre arbo automatiquement :

ansible-galaxy -h 

Usage: ansible-galaxy [delete|import|info|init|install|list|login|remove|search|setup] [--help] [options] ...

Options:
  -h, --help     show this help message and exit
  -v, --verbose  verbose mode (-vvv for more, -vvvv to enable connection
                 debugging)
  --version      show program's version number and exit

On doit pour pouvoir piloter nos roles tout d'abord créer un repertoire "roles" dans notre projet puis s'y déplacer pour pouvoir créer nos roles /modules: 

 ~/Documents/lab/ansible/provisionning  $  mkdir roles ; cd roles
exemple ici on va créer un role mariadb et y backporter progressivement tous les fichiers de conf crées auparavant 

ansible-galaxy init <nom_du_role> 

ansible-galaxy init mariadb

tree :
├── roles
│   └── mariadb
│       ├── defaults
│       │   └── main.yml
│       ├── files
│       ├── handlers
│       │   └── main.yml
│       ├── meta
│       │   └── main.yml
│       ├── README.md
│       ├── tasks
│       │   └── main.yml
│       ├── templates
│       ├── tests
│       │   ├── inventory
│       │   └── test.yml
│       └── vars
│           └── main.yml
On a donc un rep principal contenant des sous rep avec très souvent un fichier main.yml egalement (on peut changer le nom de ce fichier ou le changer ceci n'a pas d'importance)

Ces repertoires et fichiers seront apellés directement quand on les definira dans notre playbook principal : tout y sera correctement evalué et dans le même ordre qu'avec l'execution d'un playbook normal.
Un fichier vide sera tout simplement ignoré.

- defaults : dans ce rep on va setté tout ce qui sera défini par défaut dans notre role  : on va donc ici définir souvent des variables  / valeurs par default en cas de probleme d'evaluation de variables  : ces valeurs serviront de fallback.
- files : ici on va stocker tout ce qui est nécéssaire pour nos rôles : script, fichiers de conf, rpm etc ...
- meta : ici on va definir les dépendances de nos roles.
- tests : ce rep est utiliser pour les tests d'intégration continue avec des outils comme travis etc ...et contient des valeurs par defaut
- vars  :
- templates : 
> pas de surprise pour les deux repertoires précédents qui stockent ce que leur nom indique.

 -split de config de playbook existant dans une structure de role : 

On va donc maintenant splitter les confs précedentes de mariadb dans les repertoires dédiés de nos roles.
on va donc laisser dans un fichier main playbook les infos de haut niveau : 

---
- name: install and configure mariadb
  hosts: centos
  strategy: debug 


- vars : on commence par deplacer le contenu de notre fichier maria_vars.yaml dans le fichier main.yml dédié dans le rep vars :

On va placer le contenu de notre fichier :
cat maria_vars.yml
---
mysql_port: 3307
log_path: "/var/log"

dans le fchier 
roles/mariadb/vars/main.yml

on supprime donc apres le fichier de vars : maria_vars.yml puis les lignes:
  vars_files:
    - maria_vars.yml
devenues inutiles dans notre fichier de conf.

- tasks : on va maintenant opérer la même conf pour les tasks présentes dans notre fichier mariadb.yml qu'on va injecter dans le fichier main.yaml du rep tasks :

 ~/Documents/lab/ansible/provisionning/roles/mariadb  $  cat tasks/main.yml 
---
# tasks file for mariadb
- name: install mariadb server
  yum: name=mariadb-server state=latest

- name: prepare main conf file
  template: src='my.cnf.j2' dest='/etc/my.cnf'
  notify: restart mariadb

- name: create mariadb log file
  file: path='/var/log/mysqld.log' state=touch owner=mysql group=mysql mode=0775

- name: start mariadb service 
  service: name=mariadb state=started enabled=yes

- templates : on va deplacer le template my.cnf.j2 présent au même niveau que notre playbook initial dans le repertoire dédié :

 mv my.cnf.j2 roles/mariadb/templates/

- handlers : on va maintenant déplacer notre conf de handler dans la section dédiée du role :

/Documents/lab/ansible/provisionning  $  cat roles/mariadb/handlers/main.yml 
---
# handlers file for mariadb
- name: restart mariadb 
  service: name=mariadb state=restarted


on peut donc s'inspirer de cet exemple pour l'organisation de nos roles.


- creation de role from scratch : 

on va pour l'exemple créer un role common  qui va servir de base a tous nos servers quelques soient leur fonctions.

 ~/Documents/lab/ansible/provisionning/roles  $  ansible-galaxy init common 
- common was created successfully
 
on va creer plusieurs section dans ce role qui seront héritées par tous nos servers :

- install et conf de ntp :

 ~/Documents/lab/ansible/provisionning/roles  $  cat common/tasks/ntp.yml
---
- name: install ntp 
  yum: name=ntp state=present

- name: configure ntp file
  template: src=ntp.conf.j2 dest=/etc/ntp.conf
  notify: restart ntp

- name: start the ntp service
  service: name=ntp state=started enabled=yes

on renseigne notre fichier de variable avec l'adresse de notre ntp : 

 ~/Documents/lab/ansible/provisionning/roles  $  cat common/vars/main.yml 
---
# vars file for common
ntpserver: 0.fr.pool.ntp.org

on definie notre handler ( on recupere le nom defini dans la section notify de nos tasks : )  
 ~/Documents/lab/ansible/provisionning/roles  $  cat common/handlers/main.yml 
---
# handlers file for common
- name: restart ntp
  service: name=ntpd state=restarted

on renseigne notre template avec le nom de la variable définie dans le fichier main.yaml du rep vars : 
cat ntp.conf.j2 
driftfile /var/lib/ntp/drift

restrict 127.0.0.1 
restrict -6 ::1

server {{ ntpserver }}

includefile /etc/ntp/crypto/pw

keys /etc/ntp/keys
~                    

on va maintenant inclure une section selinux :


 $  cat common/tasks/selinux.yml 
---
- name: Install python bindings for SE Linux
  yum: name={{ item }} state=present
  with_items:
   - libselinux-python
   - libsemanage-python
   
- name: test to see if selinux is running
  command: getenforce




on va pouvoir ajouter differentes taches, action dans notre role et si on veut les appeller il suffira de les inclure dans notre fichier main.yml du rep task : ex ici on ajoute nos task selinuxet ntp : 

 ~/Documents/lab/ansible/provisionning/roles  $  cat common/tasks/main.yml 
---
# tasks file for common
- include selinux.yml 
- include: ntp.yml


== Declaration de roles dans un playbook : 

On va pouvoir déclarer plusieurs roles dans notre palybook :

 ~/Documents/lab/ansible/provisionning  $  cat site.yml 
---
- name : deploy common and mariadb role 
  hosts: centos
  become: yes

  roles:   <<< on place donc ici le keyword role qui contiendra la liste des roles que l'on veux executer sur notre / nos hosts :
  - common
  - mariadb

Quand on lance notre run avec notre playbook on a donc le resultat désiré : 
 ~/Documents/lab/ansible/provisionning  $  ansible-playbook -i hosts site.yml 

PLAY [deploy common and mariadb role] ******************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [common : install ntp] ****************************************************
changed: [centos]

TASK [common : configure ntp file] *********************************************
changed: [centos]

TASK [common : start the ntp service] ******************************************
changed: [centos]

RUNNING HANDLER [common : restart ntp] *****************************************
changed: [centos]

PLAY RECAP *********************************************************************
centos                     : ok=5    changed=4    unreachable=0    failed=0 

( ici on a commenté la partie mariadb : pour voir juste le run de l'install config ntp. ) 


== Dependance de roles  : ==

On peut biensur avoir des dependances entre différents roles .Pour les définir on devra passer par l'edition du fichier main.yml ( si on ne l'a pas renommé) de notre repertoire meta
en cherchant le keyword dependencies on tombe sur la section que l'on devra renseigner : 
dependencies: []
  # List your role dependencies here, one per line.
  # Be sure to remove the '[]' above if you add dependencies
  # to this list.
~
exemple ici on va vouloir mettre une dependance du role apache dans le role mariadb , on va donc editer le fichier role/mariadb/meta/main.yaml pour renseigner notre dépendances 

on va pouvoir renseigner directement notre role dans un cas basique : 
  - role: apache 

ou on peut également pouvoir saisir des paramètres ex : ici on utilise le role apache que pour les os redhat 
  - {role: apache, when : "ansible_os_family=='RedHat'"}

Les dependance de roles sont executées avant les roles qui dependent d'elles.

Les dependances de roles sont une best practice.

On va donc créer un role apache 

ansible-galaxy init apache 


 tree apache
apache
├── defaults
│   └── main.yml
├── files
├── handlers
│   └── main.yml
├── meta
│   └── main.yml
├── README.md
├── tasks
│   └── main.yml
├── templates
├── tests
│   ├── inventory
│   └── test.yml
└── vars
    └── main.yml
on va inclure des taches :

cat tasks/main.yml 
---
- name: install httpd and firewalld
  yum: name={{item}} state=present
  with_items:
  - httpd
  - firewalld

- name: start firewalld 
  service: name=firewalld state=started enabled=yes

- name: insert firewalld rule for httpd
  firewalld: port={{httpd_port}}/tcp permanent=true state=enabled immediate=yes

- name: start httpd
  service: name=httpd state=started enabled=yes

on defini le port du service http en variable :

cat vars/main.yml 
---
httpd_port: 80

et on joue notre playbook 

ansible-playbook -i hosts site.yml

on voit notre role common , avec le selinux, le role apache puis le role mariadb se jouer : apache avant mariadb car c'est une dépendance que l'on a fixé.

== Escalade de privilèges : ==

On peut setter les privileges à plusieurs niveaux comme c'est le cas avec beaucoup de param dans ansible.

dans le fichier de conf principal /etc/ansible/ansible.conf une section est réservée : 
elle est de base commentée car la granularité permet de definir finement les privileges dans les rep courant, le home etc...

[privilege_escalation]
#become=True
#become_method=sudo
#become_user=root
#become_ask_pass=False


on peut biensur egalement definir ces privileges dans le fichier d'inventaire ou meme passer le privilege sur la ligne de commande avec -b comme option.

on peut donc saisir comme on la vu nos params dans le playbook :
---
- name : deploy common and mariadb role 
  hosts: centos
  become: yes  <<< comme on l'a vu on devient root avec  ce param.
  become_user: pqsql  << exemple on execute le playbook en tant que user pgsql 
  become_user: michelle
  roles:
  - common

en lancant notre playbook avec l'option -vvvv on voit les  modifs de users s'executer : c'est bien pour le debug.
ansible-playbook -i hosts site.yml



=== delegation et actions locales  ==

on va pouvoir utiliser ce type de configuration typiquement pour permettre la désactivation d'un node du monitoring, une sortie ded loadbalancer .
on va donc pouvoir executer une tache qui se lancera sur un server particulier qui lui meme traitera une action sur un autre node.

on va donc deleguer notre action a effectuer depuis un autre node : ce ne sera pas le/les nodes de l'inventaires sur lequel la delagation est faite 

ex : on delegue a un load balancer la tache de sortir un node :

ex: ici on peut bancal de restart de machine et recup de facts apres le reboot de celle ci : 
on met un timeout et notre playbook reprend apres le reboot de la machine : 
 $  cat testdelegate.yml 
---
- name: test delegate
  hosts: centos
  become: yes

  tasks:
  - name: restart machine
    shell: sleep 2 && shutdown -r now "Ansible updates have happened" 
    async: 1
    poll: 0
    ignore_errors: True

  - name: waiting for server to come back
    wait_for: host={{inventory_hostname}} state=started delay=30 timeout=300
    become: no
    delegate_to: 127.0.0.1

- name: testing delegate facts
  hosts: centos
  become: yes
  
  tasks: 
  - name: gather local facts
    setup: 
    delegate_to: 127.0.0.1
    delegate_facts: true 

on lance le run et on a donc bien la reprise de notre playbook apres le reboot de la machine : 

 ~/Documents/lab/ansible/provisionning  $  ansible-playbook -i hosts delegate.yml 

PLAY [test delegate] ***********************************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [restart machine] *********************************************************
ok: [centos]

TASK [waiting for server to come back] *****************************************
ok: [centos -> 127.0.0.1]

PLAY [testing delegate facts] **************************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [gather local facts] ******************************************************
fatal: [centos -> 127.0.0.1]: FAILED! => {"changed": false, "failed": true, "module_stderr": "sudo: il est nécessaire de saisir un mot de passe\n", "module_stdout": "", "msg": "MODULE FAILURE"}
	to retry, use: --limit @/home/boogie/Documents/lab/ansible/provisionning/delegate.retry

PLAY RECAP *********************************************************************
centos                     : ok=4    changed=0    unreachable=0    failed=1   


=== gestion des erreurs : ==

on va pouvoir avoir a gérer des erreurs pendant nos run et pouvoir adapter le comportement de nos runs en fonction de ce qui est rencontré.


on va pouvoir decider de continuer a executer le run en cas d'erreur rencontrée par ex :

 ~/Documents/lab/ansible/provisionning  $  cat testErrors.yml 
---
- name: testing error handling
  hosts: centos
  become: yes

  tasks:
  - name: testing ignore errors
    user: name=boogie password={{uPassword}}
    ignore_errors: yes

  - name: next task even if there was a mistake :
    shell: echo hello world  

ici on decide de volontairement mettre une valeur non définie dans notre playbook et donc de générer une erreur ..le run va cependant continuer et passer a la tache suivante 

 ansible-playbook -i hosts testErrors.yml 

PLAY [testing error handling] **************************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [testing ignore errors] ***************************************************
fatal: [centos]: FAILED! => {"failed": true, "msg": "the field 'args' has an invalid value, which appears to include a variable that is undefined. The error was: 'uPassword' is undefined\n\nThe error appears to have been in '/home/boogie/Documents/lab/ansible/provisionning/testErrors.yml': line 7, column 5, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n  tasks:\n  - name: testing ignore errors\n    ^ here\n"}
...ignoring

TASK [next task ..even if there was a previous error ...] **********************
changed: [centos]

PLAY RECAP *********************************************************************
centos                     : ok=3    changed=1    unreachable=0    failed=0 


Il va être possible d'arrêter un run dans certaines conditions : ex : quand on trouve la presence d'un pattern précis ... :


cat testErrors.yml
---
- name: testing error handling
  hosts: centos
  become: yes

  tasks:
  - name: testing ignore errors
    user: name=boogie password={{uPassword}}
    ignore_errors: yes

  - name: next task ..even if there was a previous error ...
    shell: echo hello world  

  - name: put the word ko in clear ..in order to fail when found ....
    shell: echo "yopyop ko"
    register: result


  - debug: msg="Ok for sure we gonna stop the playbook ...as keyword is found."
    failed_when: result.stdout.find('ko')!=-1

  - name: just add another test ..just to see that the play stopped before ..
    shell: echo "yes ??"
    register: output

  - debug: msg="heres the word that shoulb be printed {{output.stdout}}"



on va tester et donc avoir notre run qui s'arrête a la detection du keyword trouvé : 
la derniere action afficher "yes??" ne s'executera pas puisque la tache précedante a arrêter le run .. 


PLAY [testing error handling] **************************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [testing ignore errors] ***************************************************
fatal: [centos]: FAILED! => {"failed": true, "msg": "the field 'args' has an invalid value, which appears to include a variable that is undefined. The error was: 'uPassword' is undefined\n\nThe error appears to have been in '/home/boogie/Documents/lab/ansible/provisionning/testErrors.yml': line 7, column 5, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n  tasks:\n  - name: testing ignore errors\n    ^ here\n"}
...ignoring

TASK [next task ..even if there was a previous error ...] **********************
changed: [centos]

TASK [put the word ko in clear ..in order to fail when found ....] *************
changed: [centos]

TASK [debug] *******************************************************************
fatal: [centos]: FAILED! => {
    "changed": false, 
    "failed": true, 
    "failed_when_result": true, 
    "msg": "Ok for sure we gonna stop the playbook ...as keyword is found."
}
	to retry, use: --limit @/home/boogie/Documents/lab/ansible/provisionning/testErrors.retry

PLAY RECAP *********************************************************************
centos                     : ok=4    changed=2    unreachable=0    failed=1  


=== test et debug : ===

on va pouvoir tester nos playbooks avec un mode dry run : "--check" pour voir le comportement de nos plays.

on va créer pour l'exemple un role php très simple qui ne va comporter qu'une task qui installe trois packets : 

cat  ../../roles/php/tasks/main.yml 
---
- name: install  php and git 
  yum: name={{item}} state=present
  with_items:
  - php
  - php-mysql
  - git
puis on inclu ce role dans notre site.yaml pour testet :

cat site.yml 
---
- name : deploy common and mariadb role 
  hosts: centos
  become: yes

  roles:
  - php

On lance ensuite notre dry run :

ansible-playbook -i hosts site.yml --check

PLAY [deploy common and mariadb role] ******************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [php : install  php and git] **********************************************
changed: [centos] => (item=[u'php', u'php-mysql', u'git'])

PLAY RECAP *********************************************************************
centos                     : ok=2    changed=1    unreachable=0    failed=0   

Ici on a la sortie de notre dry run ..on peut verifier que les packets n'ont pas été installé en checkant directement sur notre node la présence de php : 

 ansible centos  -i hosts -m yum -a "name=php state=absent" -b
centos | SUCCESS => {
    "changed": false, 
    "msg": "", 
    "rc": 0, 
    "results": [
        "php is not installed"
    ]
}


En erreur classique on peut avoir des erreurs de connections ssh (user  non defini pour la connexion ou autres ..) 

L'erreur peut être de type :
ansible centos  -i hosts -m setup
centos | UNREACHABLE! => {
    "changed": false, 
    "msg": "Failed to connect to the host via ssh: Permission denied (publickey,password).\r\n", 
    "unreachable": true


on peut s'aider au debug en passant plusieurs "v" (verbose) en param de notre test :

ansible centos  -i hosts -m setup -vvv
Using /home/boogie/Documents/lab/ansible/provisionning/ansible.cfg as config file
Using module file /usr/lib/python2.7/dist-packages/ansible/modules/core/system/setup.py
<127.0.0.1> ESTABLISH SSH CONNECTION FOR USER: root
<127.0.0.1> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o Port=22 -o 'IdentityFile="/home/boogie/.ssh/id_rsa"' -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=root -o ConnectTimeout=10 -o ControlPath=/home/boogie/.ansible/cp/ansible-ssh-%h-%p-%r 127.0.0.1 '/bin/sh -c '"'"'( umask 77 && mkdir -p "` echo ~/.ansible/tmp/ansible-tmp-1506586580.53-135159726548874 `" && echo ansible-tmp-1506586580.53-135159726548874="` echo ~/.ansible/tmp/ansible-tmp-1506586580.53-135159726548874 `" ) && sleep 0'"'"''
centos | UNREACHABLE! => {
    "changed": false, 
    "msg": "Failed to connect to the host via ssh: Permission denied (publickey,password).\r\n", 
    "unreachable": true
}
on a donc la plus de détails et on peut donc plus facilement débugguer.

On va aussi pouvoir avoir des erreurs de syntaxe a debugguer : 

> indentation 
> quoting mal fermé ( " manquant par exemple )
> variable, bloc  non définie : toujours vérifier la cohérence des noms et checker les eventuelles typo

Pour proceder a du debug on peut définir un point de depart a notre playbook : exemple commencer le test a partir d'une task définie : 

exemple notre playbook de tests comporte plusieurs tasks :

 ~/Documents/lab/ansible/provisionning  $  cat testErrors.yml
---
- name: testing error handling
  hosts: centos
  become: yes

  tasks:
  - name: testing ignore errors
    user: name=boogie password={{uPassword}}
    ignore_errors: yes

  - name: next task ..even if there was a previous error ...
    shell: echo hello world  

  - name: put the word ko in clear ..in order to fail when found ....
    shell: echo "yopyop ko"
    register: result


  - debug: msg="Ok for sure we gonna stop the playbook ...as keyword is found."
    failed_when: result.stdout.find('ko')!=-1
  
  - name: just add another test ..just to see that the play stopped before ..
    shell: echo "yes ??"
    register: output

  - debug: msg="heres the word that shoulb be printed {{output.stdout}}"

on va lancer notre play a partir d'une tache specifique : ce qui va skipper les tasks précédentes :

 ansible-playbook -i hosts testErrors.yml --start-at-task="put the word ko in clear ..in order to fail when found ...."

PLAY [testing error handling] **************************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [put the word ko in clear ..in order to fail when found ....] *************
changed: [centos]

TASK [debug] *******************************************************************
fatal: [centos]: FAILED! => {
    "changed": false, 
    "failed": true, 
    "failed_when_result": true, 
    "msg": "Ok for sure we gonna stop the playbook ...as keyword is found."
}
	to retry, use: --limit @/home/boogie/Dme: testing error handling
  hosts: centos
  become: yes

  tasks:
  - name: testing ignore errors
    user: name=boogie password={{uPassword}}
    ignore_errors: yes

  - name: next task ..even if there was a previous error ...
    shell: echo hello world  

  - name: put the word ko in clear ..in order to fail when found ....
    shell: echo "yopyop ko"
    register: result


  - debug: msg="Ok for sure we gonna stop the playbook ...as keyword is found."
    failed_when: result.stdout.find('ko')!=-1
  
  - name: just add another test ..just to see that the play stopped before ..
    shell: echo "yes ??"
    register: output

  - debug: msg="heres the word that shoulb be printed {{output.stdout}}"

ocuments/lab/ansible/provisionning/testErrors.retry

PLAY RECAP *********************************************************************
centos                     : ok=2    changed=1    unreachable=0    failed=1   


Il est aussi tout a fait possible de tester  un playbook sequentiellement avec un "--step" :

ce qui va noous permettre de tester toutes les sections de notre playbook une par une : un prompt nous demandant  de continuer ou non apparait : 
 ansible-playbook -i hosts testErrors.yml --step

PLAY [testing error handling] **************************************************
Perform task: TASK: setup (N)o/(y)es/(c)ontinue: y

Perform task: TASK: setup (N)o/(y)es/(c)ontinue: *******************************

TASK [setup] *******************************************************************
ok: [centos]
Perform task: TASK: testing ignore errors (N)o/(y)es/(c)ontinue: y

Perform task: TASK: testing ignore errors (N)o/(y)es/(c)ontinue: ***************

TASK [testing ignore errors] ***************************************************
fatal: [centos]: FAILED! => {"failed": true, "msg": "the field 'args' has an invalid value, which appears to include a variable that is undefined. The error was: 'uPassword' is undefined\n\nThe error appears to have been in '/home/boogie/Documents/lab/ansible/provisionning/testErrors.yml': line 7, column 5, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n  tasks:\n  - name: testing ignore errors\n    ^ here\n"}
...ignoring
Perform task: TASK: next task ..even if there was a previous error ... (N)o/(y)es/(c)ontinue: y

Perform task: TASK: next task ..even if there was a previous error ... (N)o/(y)es/(c)ontinue: ***

TASK [next task ..even if there was a previous error ...] **********************
changed: [centos]
Perform task: TASK: put the word ko in clear ..in order to fail when found .... (N)o/(y)es/(c)ontinue: N


Ne jamais hésiter sinon a utiliser le module debug ...


==== ansible + windows : ===

 cat ../ressources/Introduction_to_ansible_working_files/chapter6/windows.yml 
---
- name: testing windows module
  hosts: windows
  
  vars:
    uPassword: w!ndowS

  tasks: 
  - name: run ipconfig
    raw: ipconfig
    register: ipconfig

  - debug: var=ipconfig

  - name: test stat module on file
    win_stat: path="C:/Windows/win.ini"
    register: stat_file

  - debug: var=stat_file

  - name: add a local group
    win_group: name=TestWindowsGroup description="My Windows group" state=present

  - name: add a local user
    win_user: name=TestUser password={{uPassword}} groups=TestWindowsGroup group_action=add

  - name: give my local user some permissions
    win_acl: 
      path: "C:\\Users\\Public"
      user: TestUser
      rights: FullControl
      type: allow 

exemple de playbook :


cat ../ressources/Introduction_to_ansible_working_files/chapter6/playbook.yml 
---
- name: launching an ec2 instance
  hosts: localhost
  connection: local
  gather_facts: false 

  vars_files:
   - keypair.yml 

  tasks: 
  - name: search for the latest rhel 7 ami
    ec2_ami_find:
      region: us-east-1
      owner: "309956199498"
      name: "RHEL-7.2*"
    register: find_results

  - debug: var=find_results

  - name: find a subnet id
    ec2_vpc_subnet_facts:
      aws_access_key: "{{ec2_access_key}}"
      aws_secret_key: "{{ec2_secret_key}}"
      region: us-east-1
    register: subnet_ids

  - debug: var=subnet_ids

  - name: launch an ec2 instance
    ec2:
      aws_access_key: "{{ec2_access_key}}"
      aws_secret_key: "{{ec2_secret_key}}"
      instance_type: t2.micro
      region: us-east-1
      image: "{{find_results.results[0].ami_id}}"
      instance_tags:
         Name: mperz
      wait: yes
      vpc_subnet_id: "{{subnet_ids.subnets[0].id}}"
      assign_public_ip: yes 

il faut  examiner ansible galaxy car il n'y a pas beaucoup de modules ....


=== dynamic inventory : ===
specific au travail sur les instances amazon


== vault : ==

module permettant d'encrypter les données sensibles d'un fichier ..pour eviter de les passer en clair dans les runs.

- crypter un fichier existant :

pour encrypter un fichier sensible existant :

ansible-vault encrypt fichier_sensible.yaml

on creer un fichier test avec des entrées pour test :

vi test_passwd.yml

t file with fake word to test encryption with 
vault :

blabla1
testo44

on encrypt avec vault : un password nous est demandé :

 ~/Documents/lab/ansible/provisionning  $  ansible-vault encrypt test_passwd.yml 
Vault password: 
Encryption successful

on a maintenant un fichier crypté :
 ~/Documents/lab/ansible/provisionning  $  cat test_passwd.yml 
$ANSIBLE_VAULT;1.1;AES256
63316563353532366463323962363430303933383137336435356663303861306534383363336335
3134356635326466616131623065373532396665636231610a336538646166333738356162363563
38323861343762343866363135656264303862383264313131333334323462353139653537663932
6633353763316362330a326235636634663564353664653436303766393636386539636563336230
64336661353138333263656633313939303830333532646231653934333830323135623232373531
37376136656362396431616332663139613464343835313230623632363134343739386462646563
32343563633833343231363931636262343965366130303465653763333163656666613339663833
62373030393537666639

on va pouvoir si on veut éditer le fichier : 
ansible-vault edit test_passwd.yml 
Vault password: 
# test file with fake word to test encryption with 
vault :

blabla1
testo44
anotherblabla


on va pouvoir jouer un playbook utilisant un fichier crypté en passant en argument la commande nous demandant le password vault setté pour notre fichier sensible :



 cat testVault.yml 
---
- name: testing error handling
  hosts: centos
  become: yes

  tasks:
  
  - name: copy encrypt file
    copy: src=test_passwd.yml dest=/tmp/
  
  - name:  test to read encrypted file with vault 
    shell: cat /tmp/test_passwd.yml
    register: output

  - debug: msg="heres the word that shoulb be printed {{output.stdout}}"


 ansible-playbook -i hosts testVault.yml --ask-vault-pass 


 ansible-playbook -i hosts testVault.yml --ask-vault-pass
Vault password: 

PLAY [testing error handling] **************************************************

TASK [setup] *******************************************************************
ok: [centos]

TASK [copy encrypt file] *******************************************************
changed: [centos]

TASK [test to read encrypted file with vault] **********************************
changed: [centos]

TASK [debug] *******************************************************************
ok: [centos] => {
    "msg": "heres the word that shoulb be printed # test file with fake word to test encryption with \nvault :\n\nblabla1\ntesto44\nanotherblabla"
}

PLAY RECAP *********************************************************************
centos                     : ok=4    changed=2    unreachable=0    failed=0   


=== notes prometheus (ud) ===


prometheus est fait en go
prom collecte les metrics des points de monitoring sur les endpoints via http. On dis que prometheus "scrap" les data.

== Set up :

cf files pour baremetal

une fois les applis ( prometheus / node-exporter et grafana installés ) ont peut y acceder :

- prometheus    http://localhost:9090
- node-exporter http://localhost:9100
- grafana       http://localhost:3000


- prometheus :

on peut browser les differentes métriques , executer la requete ( recupérer les valeurs ) ou ajouter un graph : qui va nous montrer physiquement le resulat de notre requete sous forme de graph

ex: on selectionne via le menu deroulant la metrique :
scrape_samples_scraped

on peut avoir le graph 

- grafana :
une fois installé on va definir une datasource : on va donc ajouter prometheus ( localhost:9090) 

on ajoute un dashboard > on ajoute une metrique : ex on recupere scrape_samples_scraped celle qu'on a tester dans prometheus , on ajoute un graph et on a la meme chose que dans prometheus mais avec un visuel plus clair.


== concepts :

toutes les donnees sont stockée en format time series.
chaque time-series est identifiée par un nom de métrique (metric name ) et un set de paires clés valeurs apellées "labels"

exemple : 
metric: go_memstats_alloc_bytes

et on aura plusieurs labels de cette métrique : 
    ✔
    go_memstats_alloc_bytes{endpoint="web",instance="172.17.0.4:9090",job="monitor-prometheus-operato-prometheus",namespace="prom",pod="prometheus-monitor-prometheus-operato-prometheus-0",service="monitor-prometheus-operato-prometheus"}
    ✔
    go_memstats_alloc_bytes{endpoint="service",instance="172.17.0.2:3000",job="monitor-grafana",namespace="prom",pod="monitor-grafana-857c4b68b4-sqwkm",service="monitor-grafana"}
    ✔
    go_memstats_alloc_bytes{endpoint="metrics",instance="192.168.39.114:9100",job="node-exporter",namespace="prom",pod="monitor-prometheus-node-exporter-mb87t",service="monitor-prometheus-node-exporter"}
    ✔

 On aura donc la même métrique mais dont les résultats seront issus de plusieurs soft dfférents .

les data time-series sont la representation des data actuelles et elles peuvent prendre un format  float64 ou un timestamp de précision milliseconds
la notation des time-series suit un standart du type :
<metric_name>{<label_name=label_value..}
ex: 
node_boot_time_seconds{endpoint="metrics",instance="192.168.39.114:9100",job="node-exporter",namespace="prom",pod="monitor-prometheus-node-exporter-mb87t",service="monitor-prometheus-node-exporter"}


== configuration prometheus : =

La conf de prometheus est stockée dans un format yaml et on peut modifier cette config sans avoir a redémarrer prometheus.
un reload peut être fait avec un sig hup linux kill -SIGHUP <pid>

on peut aussi démarrer prom avec des params en lui passant des arguments de type : --config.file
si on veut modifier un param on doit par contre redémarrer prom.

on peut dans un deployment helm du chart prometheus-operator examiner la conf prometheus en nous loggant en shell dans le container prometheus-config-reloader de notre pod  prometheus-monitor-prometheus-operato-prometheus-0
on peut voir en gros les params de base en début de fichier dans la section global 
puis on va definir les targets : les endpoints qui permettront a prometheus de recup la data.
Le label job_name est rajouté sur toutes les lignes d'ou on veut scrap la data 

/etc/prometheus/config_out $ cat prometheus.env.yaml
global:
  evaluation_interval: 30s  <<<<  delai d'evaluation des rules définies dans notre conf 
  scrape_interval: 30s  <<< delai de recup de métrique.
  external_labels:
    prometheus: prom/monitor-prometheus-operato-prometheus
    prometheus_replica: prometheus-monitor-prometheus-operato-prometheus-0
rule_files:
- /etc/prometheus/rules/prometheus-monitor-prometheus-operato-prometheus-rulefiles-0/*.yaml    <<< fichier de rules 
scrape_configs:   <<<<< c'est dans cette section qu'on va définir les endpoints dont prometheus viendra scrapper les data.
- job_name: prom/monitor-prometheus-operato-alertmanager/0   <<<< on commence toujours notre label par job_name suivi de notre target            
  honor_labels: false
  kubernetes_sd_configs:
  - role: endpoints
    namespaces:
      names:
      - prometheus
  metrics_path: /metrics    <<< on defini ici que notre points d'entrée pour recup les metrics sera /metrics
  ...

on peut examiner nos targets en selectionnant status > target dans l'interface de prometheus 
on voit notre target definie dans la conf :


prom/monitor-prometheus-operato-alertmanager/0 (1/1 up)
Endpoint 	State 	Labels 	Last Scrape 	Scrape Duration 	Error
http://172.17.0.3:9093/metrics
	up 	endpoint="web" instance="172.17.0.3:9093" job="monitor-prometheus-operato-alertmanager" namespace="prom" pod="alertmanager-monitor-prometheus-operato-alertmanager-0" service="monitor-prometheus-operato-alertmanager" 	18.967s ago 	8.43ms

= Monitoring des nodes :

pour monitorer un node on doit ajouter un node exporter : qui va exposer les métrics des serveurs linux/ unix ( cpu, mem ..)
on peut dans une install classique installer le binaire de la maniere suivante :

#!/bin/bash
NODE_EXPORTER_VERSION="0.16.0"
wget https://github.com/prometheus/node_exporter/releases/download/v${NODE_EXPORTER_VERSION}/node_exporter-${NODE_EXPORTER_VERSION}.linux-amd64.tar.gz
tar -xzvf node_exporter-${NODE_EXPORTER_VERSION}.linux-amd64.tar.gz
cd node_exporter-${NODE_EXPORTER_VERSION}.linux-amd64
cp node_exporter /usr/local/bin

# create user
useradd --no-create-home --shell /bin/false node_exporter

chown node_exporter:node_exporter /usr/local/bin/node_exporter

# on créee un fichier de service : 
echo '[Unit]
Description=Node Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=multi-user.target' > /etc/systemd/system/node_exporter.service

# enable node_exporter in systemctl
systemctl daemon-reload
systemctl start node_exporter
systemctl enable node_exporter


On va ensuite alimenter notre fichier de conf prometheus pour qu'il prenne en compte notre exporter :
...
  - job_name: 'node_exporter'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:9100']

Dans une conf kube déployée avec le prometheus operator on aura classiquement un dameonset installé sur chaque noeud du cluster et on pourra retrouver cette conf par ex :

- job_name: monitoring/monitor-prometheus-operato-node-exporter/0
  honor_timestamps: true
  scrape_interval: 30s
  scrape_timeout: 10s
  metrics_path: /metrics
  scheme: http
  kubernetes_sd_configs:
  - role: endpoints
    namespaces:
      names:
      - monitoring
  relabel_configs:
  ....
  .......

on pourra examiner les métrics exposées par les membres de notre cluster en faisant un curl sur l'ip d'un node suivi du port 9100 (port de l'exporter) et de la route "/metrics" :
ex: 
curl http://192.168.0.81:9100/metrics |head
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 1.1048e-05
go_gc_duration_seconds{quantile="0.25"} 4.9248e-05
go_gc_duration_seconds{quantile="0.5"} 9.0784e-05
go_gc_duration_seconds{quantile="0.75"} 0.00013272
go_gc_duration_seconds{quantile="1"} 0.008615343
go_gc_duration_seconds_sum 6.307212057
go_gc_duration_seconds_count 22903
# HELP go_goroutines Number of goroutines that currently exist.
100 14997    0 14997    0     0   523k      0 --:--:-- --:--:-- --:--:--  542k

== Monitoring : ==

= Clients librairies :

avant de monitorer notre code on doit instrumenter notre code.
on a à notre disposition des librairies pretes en python, scala, go ....Lua pour nginx, ruby, rust ...
On peut si on en a besoin créer les notres : il faut juste respecter le format d'exposition.

- exposition format :
le format a respecter est de forme : "simple text format" : un nom de metric , un label clé valeur et la metrique récupérée.
ex: 
node_uname_info{domainname="(none)",machine="x86_64",nodename="k-node01",release="4.9.0-12-amd64",sysname="Linux",version="#1 SMP Debian 4.9.210-1+deb9u1 (2020-06-07)"} 1
process_cpu_seconds_total 1477.77
process_resident_memory_bytes 2.0381696e+07

- metrics : 4 types :

-> counter : nombre toujours croissant ( ex: nbr de visit de sites internet.)
-> gauge : valeur qui monte et descend ( charge cpu, temperature ..)
-> histogram : echantillon d'observation (temps de requete,taille des reponses) et ces observations sont assemblées ( somme, count ..) . le but est de calculer les quantiles (quantiles sont les valeurs qui divisent un jeu de données en intervalles contenant le même nombre de données )
-> summary: comme les histogram les summary. Fourni les total des observations et une somme des valeurs observées. Calcul des quantiles configurables en fenetre de temps glissante.
ex pour calculer une latence on doit avoir :
-> le nombre de requettes
-> le total de latence des ces requettes (sum) 
le taux de latence des requettes / diviser => on a la latence moyenne ( ??????? ) 

- prometheus instrumentation python :
client_python supporté chez prometheus
pip install prometheus_client

- prometheus instrumentation go :
client_go : tres simple à mettre en place 
un simple import de module est à faire.

= pushing metrics dans prometheus :

de base prometheus prefere le "pull base metric": prometheus recupere les metriques de nos systeme à un certain interval et va les stocker dans sa bdd.
dans certains cas ce n'est pas possibles. Les datas de notre app devront être poussées dans une pushgateway : qui est un composant de prometheus qui peut etre installé séparément.
prometheus vient ensuite récupérer ces data dans la pushgateway
ex: batchs jobs, serveurs derriere des firewall 

attention ces systemes ont des défaults : 
souvent il n'y a qu'une seule gateway ( spof) 
les healtchchecks auto de prometheus ne peuvent pas etre mis en place 
les data ne sont jamais purgées automatiquement des pushgateway : il faut donc le faire via une api:
ex : curl -X DELETE http://localost:9091/metrics/job/prom_course/instance/localhost

Il n'y a qu'un seul usecase valid pour une pushgateway 
> service level batch job non specific a une machine.
si on a des serveurs bloqués par un fw .il faut mettrer prometheus dans le même reseau .( ?) 

= Prometheus Query : =

prometheus fourni un language pour requetter : PromQL
(fourni des fonctions)
PromQL est en readonly : pas d'insert de data.

-> type :
- instant vector : un set de time serie qui contient un seul echantillon de chaque time series ayant le même timestamp :
ex: 
node_cpu_seconds_total

- range vector : un set de time serie qui contient un intervalle de chaque time series ayant le même timestamp :
node_cpu_seconds_total[5m]

- scalar : chiffre simple à virgule 
ex: -3.14

- strings : ex foobar (non utilisé)

-> operations :

- arithmetic ( + - / * ² ..)
- comparaison : = != > < >= <=
- logical set : and(intersection)  or(union) unless(complement)
- aggregations : sum ,min, max, count, quantile ......

exemples :
dans prometheus graph on saisi lesdifferentes requettes suivantes :

prometheus_http_requests_total :

prometheus_http_requests_total{code="200",endpoint="web",handler="/-/healthy",instance="10.124.71.82:9090",job="monitor-prometheus-operato-prometheus",namespace="monitoring",pod="prometheus-monitor-prometheus-operato-prometheus-0",service="monitor-prometheus-operato-prometheus"}	67337
prometheus_http_requests_total{code="200",endpoint="web",handler="/-/ready",instance="10.124.71.82:9090",job="monitor-prometheus-operato-prometheus",namespace="monitoring",pod="prometheus-monitor-prometheus-operato-prometheus-0",service="monitor-prometheus-operato-prometheus"}	67335
prometheus_http_requests_total{code="200",endpoint="web",handler="/-/reload",instance="10.124.71.82:9090",job="monitor-prometheus-operato-prometheus",namespace="monitoring",pod="prometheus-monitor-prometheus-operato-prometheus-0",service="monitor-prometheus-operato-prometheus"}	1
prometheus_http_requests_total{code="503",endpoint="web",handler="/-/ready",instance="10.124.71.82:9090",job="monitor-prometheus-operato-prometheus",namespace="monitoring",pod="prometheus-monitor-prometheus-operato-prometheus-0",service="monitor-prometheus-operato-prometheus"}
...

- tous les données de la sonde up comportant le endpoint metrics :
up{endpoint="metrics"}

up{endpoint="metrics",instance="192.168.0.80:9100",job="node-exporter",namespace="monitoring",pod="monitor-prometheus-node-exporter-p92t8",service="monitor-prometheus-node-exporter"}	1
up{endpoint="metrics",instance="192.168.0.82:9100",job="node-exporter",namespace="monitoring",pod="monitor-prometheus-node-exporter-qdkx9",service="monitor-prometheus-node-exporter"}	1
up{endpoint="metrics",instance="192.168.0.81:9100",job="node-exporter",namespace="monitoring",pod="monitor-prometheus-node-exporter-8jj8d",service="monitor-prometheus-node-exporter"}

- utilisation de regexp :

ex: on cherche les données https_request_total comprenant un label handler contenant ".*ags" :
http_request_total{handler=~".*ags"}

http_request_total{endpoint="service",handler="/tags",instance="10.124.104.166:3000",job="monitor-grafana",method="get",namespace="monitoring",pod="monitor-grafana-68bdf6dc46-29k2w",service="monitor-grafana",statuscode="200"}

ex: recherche des codes http de requettes différentes de 200 :
http_request_total{statuscode!="200"}

 	Value
http_request_total{endpoint="service",handler="/*",instance="10.124.104.166:3000",job="monitor-grafana",method="get",namespace="monitoring",pod="monitor-grafana-68bdf6dc46-29k2w",service="monitor-grafana",statuscode="302"}

ex: on peut rechercher  des codes retour différents de 3XX ( 301, 302 ..) 

http_request_total{statuscode!~"3.."}

http_request_total{endpoint="service",handler="/*",instance="10.124.104.166:3000",job="monitor-grafana",method="get",namespace="monitoring",pod="monitor-grafana-68bdf6dc46-29k2w",service="monitor-grafana",statuscode="200"}	2
http_request_total{endpoint="service",handler="/avatar/:hash",instance="10.124.104.166:3000",job="monitor-grafana",method="get",namespace="monitoring",pod="monitor-grafana-68bdf6dc46-29k2w",service="monitor-grafana",statuscode="200"}

- ajout de fonction :

calcul du taux de requette sur une mnt :

rate(http_request_total[1m])

{endpoint="service",handler="/*",instance="10.124.104.166:3000",job="monitor-grafana",method="get",namespace="monitoring",pod="monitor-grafana-68bdf6dc46-29k2w",service="monitor-grafana",statuscode="200"}	0
{endpoint="service",handler="/*",instance="10.124.104.166:3000",job="monitor-grafana",method="get",namespace="monitoring",pod="monitor-grafana-68bdf6dc46-29k2w",service="monitor-grafana",statuscode="302"}	0
{endpoint="service",handler="/avatar/:hash",instance="10.124.104.166:3000",job="monitor-grafana",method="get",namespace="monitoring",pod="monitor-grafana-68bdf6dc46-29k2w",service="monitor-grafana",statuscode="200"}

on peut cumuler les fonctions :

sommes des taux de requettes sur une mnt pour les job 
sum(rate(http_request_total[1m])) by (job)

{job="monitor-grafana"}	0

si on requette un peu grafana le chiffre change :
{job="monitor-grafana"}	0.8999999999999999

- on peut faire des opérations arithmetiques :

nombre total de memoire - nombre de memoire libre convertie en megabytes :
( node_memory_MemTotal_bytes -  node_memory_MemAvailable_bytes) / 1024 /1024

{endpoint="metrics",instance="192.168.0.80:9100",job="node-exporter",namespace="monitoring",pod="monitor-prometheus-node-exporter-p92t8",service="monitor-prometheus-node-exporter"}	986.4375
{endpoint="metrics",instance="192.168.0.82:9100",job="node-exporter",namespace="monitoring",pod="monitor-prometheus-node-exporter-qdkx9",service="monitor-prometheus-node-exporter"}	928.046875
{endpoint="metrics",instance="192.168.0.81:9100",job="node-exporter",namespace="monitoring",pod="monitor-prometheus-node-exporter-8jj8d",service="monitor-prometheus-node-exporter"}	683.7578125


== service discovery : ==

remplir manuellement le fichier de conf prometheus peut vite devenir fastidieux et source d'erreur.
On va pouvoir utiliser le service discovery de prometheus pour assurer le job 

un service discovery est la detection automatique de devices et services offertes par des éléments.

plusieurs methodes existent :
cloud provider amazon ( specificité des providers.)
kubernetes ( conf alimentée via kube )
dns ( on peut faire des entrées dns sur des services par exemple et prometheus alimentera sa conf via les records dns qu'il aura détecté.) 
file ( on peut alimenter dans un fichier via ansible nos ressources , prometheus viendra examiner périodiquement ce fichier et mettra a jour la conf ) 


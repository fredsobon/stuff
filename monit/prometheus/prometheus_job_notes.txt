== notes : ===

= concepts prometheus : 

activation d'une metrique du node exporter desactivé par défaut

ajout d'une machine a monitorer :

on ajoute dans le fichier de conf prometheus.yml

- job_name: node_exporter
  static_configs:
  - targets:
    - 10.121.253.78:9100

de base prometheus ecrit ses data en local : il ne pousse pas dans s3 par exemple.
Il y a des projets tierce pour stocker ( ex thanos) .

La base est une resolution de 15s avec un max de 15jours de retention.

-service discovery :

prometheus va inerroger quelque chose pour recup la listes des objects qu'il va scraper .

dns_sd_configs

on va modifier nsd : serveur dns  local autoritaire ( pas de fwd) :w


cat /etc/resolv.conf
; Created by cloud-init on instance boot automatically, do not edit.
;
; generated by /usr/sbin/dhclient-script
search lapin.io
nameserver 127.0.0.1
#nameserver 10.101.1.227
#nameserver 10.101.101.49
#nameserver 10.120.1.227


on ajoute a la fin dans le fichier nsd.conf notre domaine de test :

zone:
  name: lab.lapin.io
  zonefile: lab.lapin.io.conf


on va ajouter le fichier de zone :

[root@poc-prometheus-server-fso prometheus-2.18.1.linux-amd64]# cat /etc/nsd/lab.lapin.io.conf
;## NSD authoritative only DNS

$ORIGIN lab.lapin.io.    ; default zone domain
$TTL 86400           ; default time to live

@ IN SOA ns1 admin@lab.lapin.io (
           2012082703  ; serial number
           28800       ; Refresh
           14400        ; Retry
           864000      ; Expire
           86400       ; Min TTL
           )

           NS      ns1.lab.lapin.io.
           NS      ns2.lab.lapin.io.

virt02u   	   IN     A    10.121.253.1
virt04u   	   IN     A    10.121.253.2
virt06u   	   IN     A    10.121.235.3


FSO-node       IN     A    10.121.253.78

_pve._tcp      SRV    0 0 8006 FSO-node

_node._tcp     SRV   0 0 9100 FSO-node
_pve._tcp      SRV    0 0 8006 virt02u.lab.lapin.io.
_pve._tcp      SRV    0 0 8006 virt04u.lab.lapin.io.
_pve._tcp      SRV    0 0 8006 virt06u.lab.lapin.io.

;## NSD authoritative only DNS

maintenant on modifie la conf prometheus en ajoutant le service discovery :

scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'prometheus'

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
    - targets: ['localhost:9090']

  - job_name: 'node_exporter'
    dns_sd_configs:
    - names :
      - _node._tcp.lab.lapin.io

>> on aura maintenant une decouverte des servers service entrés dans la conf dns :

_node._tcp     SRV   0 0 9100 bob-node

- prometheus evalue les rules : les seuils une fois que le seuil est dépassé il envoit a alerte manager qui lui va envoyé a slack etc ... : alerte manager ne sert juste qu'a l'aiguillage

- prometheus devient un standart : les applis exposent les metriques au format prometheus.
on peut creer un fichier text a plat qui recupére ls datas au format prometheus
text_file

les fichiers doivent finir par .prom



- notes helm chart maison : 

umbrella : notre module maison est composé de 2 charts : 
prometheus-operator
secret : chart maison pour deployer les secrets

ex : dans notre cas on utilsie etcd en composant externe à kube.
on va devoir recupérer les metrics etcd en donnant les infos secrets (certif tls etcd) pour que prometheus-operator puisse recupérerer les valeurs.


on va avoir en plus les servicesmonitor pour exposer les metriques des composants externes : 

calico

> on va créer des objets services
> pod ( on doit avoir la liste des pod ) calico
> service monitor : fait un match sur les endpoints ( pod ) pour recupérer les data à monitorer 

idem pour typha
..
....


prometheus-operator peut donner des infos à grafana
> ex : on met les dashboard dans un volume 
mais pb en cas de scaling ( il faut pousser et repliquer les données ..)

on va plutot fournir des configmaps : 
on va lui donner les dashboards en configmap : c'est du json ..il faut faire du json dans du yaml .
il faut faire attention car les data doivent etre protégées ( les instructions go peuvent être mal interprétées ..) 
ex {{`{{ instance }}`}} 


stockage volume en local 
hostpath
pv  : physical volume
pvc : demande de stockage 

stockage local peut etre fait sur les nodes.
on stick l'appli sur les nodes dédiés.


= deploiment chart : ==


chart.yaml
( en v3 contient les dependancies) 

on verif les dépendancies :
helm dep list                                                                  [☸ |kubernetes-admin@sandbox:kube-system]
NAME               	VERSION	REPOSITORY                                                         	STATUS
prometheus-operator	8.9.2  	https://artifact.meetic.ilius.net/artifactory/helm-remote-stable/  	ok
secrets            	0.1.4  	https://artifact.meetic.ilius.net/artifactory/helm-ilius-incubator/	ok

si pas ok  :on meet a jour 

helm dependencies update 

> dans le rep charts on recup les charts gzip : on ne gzip pas  : pas nécéssaire.

helm_vars : on gere nos environments 

tree helm_vars                                                                 [☸ |kubernetes-admin@sandbox:kube-system]
helm_vars
├── ci
│   ├── secrets.ci.yaml
│   └── values.yaml
├── infra
│   ├── secrets.srs.yaml
│   └── values.yaml
├── recette
│   ├── secrets.srs.yaml
│   └── values.yaml
├── sandbox
│   └── values.yaml
└── siops
    ├── secrets.srs.yaml
    └── values.yaml

5 directories, 9 files


tree templates                                                                 [☸ |kubernetes-admin@sandbox:kube-system]
templates
├── calico
│   ├── endpoints.yaml
│   ├── servicemonitor.yaml
│   ├── service.yaml
│   ├── typha-endpoints.yaml
│   ├── typha-servicemonitor.yaml
│   └── typha-service.yaml
├── grafana
│   └── dashboards
│       ├── calico-felix.yaml
│       ├── calico-typha.yaml
│       └── prometheus-alertmanager.yaml
├── _helpers.tpl
├── ssl
│   └── grafana-certmanager.yaml
└── storage
    └── prometheus
        └── persistentvolume.yaml

6 directories, 12 files



helm template monitoring artifact/prometheus-operator < on recupere sur une registry

si on déploit en local on met .

              nomde notre deploiement . (endroit) -f valeurs --namespace voulu  ( si le namespace n'existe pas il le crée )
helm template monitoring . -f helm_vars/sandbox/values.yaml --namespace monitoring
                

la commande template va juste générer la conf mais n'applique rien 


on install 
helm install  monitoring . -f helm_vars/sandbox/values.yaml --namespace monitoring --create-namespace
manifest_sorter.go:192: info: skipping unknown hook: "crd-install"
manifest_sorter.go:192: info: skipping unknown hook: "crd-install"
manifest_sorter.go:192: info: skipping unknown hook: "crd-install"
manifest_sorter.go:192: info: skipping unknown hook: "crd-install"




le deployement fonctionne sauf qu'on a une erreur sur le pod prometheus
on voit dans les erreurs :


il nous manque le repertoire defini dans le volume :
/data/prometheus-database

on crée les repo et on change les droits au bon user chown -R 1000:1000 

on bute le pod et on c'est bon.


on peut voir toutes les rules crées par prometheus, services monitor, dahboard ..

l'inegralité de promethzus est piloté par kube


on va examiner prometheus en forwardant le port du pod 


kubectl port-forward prometheus-monitoring-prometheus-oper-prometheus-0 9090:9090
Forwarding from 127.0.0.1:9090 -> 9090
Forwarding from [::1]:9090 -> 9090
Handling connection for 9090
Handling connection for 9090
Handling connection for 9090
Handling connection for 9090
Handling connection for 9090
Handling connection for 9090


on voit que les metrics des composants kube et autre calico des service monitor sont récupérées : le protocole de service discovery interne de kube est alimenté.

des labels sont automatiquement fixé par kube pour les composants 


on peut créer les rules ,les dashboars



grafana : on examine grafana 

kubectl port-forward monitoring-grafana-584cb44f78-kv7vh 3000:3000


on peut voir les configmaps si on a comme label  grafana-dashboard -> on a un dashboard grafana 

ex :
monitoring/monitoring-prometheus-oper-k8s-resources-cluster


datasource :

on a la datasource prometheus mais aussi la datasource alartmanager 
http://127.0.0.1:9090/targets

==== notes kafka beguinner udemy ====

= generalités= 

systeme pour permette la gestion de flux de données.
exemples d'utilisation :
- message bus
- tracking d'activité
- recupération de métrique de differentes sources
- consolidation de logs
- stream processing ( avec spark par ex )
archi distribuée, resilients ,scalable , très rapide (très peu de latence) 

= topics / partitions / offsets : =

- topics : est la base de kafka.
-> c'est une flux de données ( similaire a une table en bdd )
-> il est identifié par son nom 
-> nombre de topic illimité
les topics sont  découpés en partitions 

- partitions :
conteneur pour topics
ex : 
1 topic est découpé en trois partitions 
Chaque  partition est ordonnées
chaque  message arrivant dans une partition ont un id incrementés : c'est un offset
Chaque offset n'a un sens que pour sa propre partition.

on defini le nombre de partition d'un topic à sa creation

L'ordre est garanti uniquement au sein de la même partition.

Les données dans kafka on une durée de vie limitée ( de base un message est conservé 1 semaine )
Dès qu'une data est ecrite on ne peut pas la modifier : c'est l'immutabilité.
Les données sont assignées aléatoirement à une partition. Sauf si une clé est fournie.


= brokers =

-> les brokers sont les endroits ou sont stockées les partitions des topics. On a régulièrement des clusters de brokers
-> Chaque broker est identifié par un id : obigatoirement un nombre.
-> chaque broker contient uniquement certaines partitions de topics.

Une fois connecté à un broker : on est automatiquement connecté à tous les noeuds du cluster.

= topic replication facter : =

-> les topics doivent avoir un facter de replication (ex : 2 , 3 ..) : si un broker est HS on continu à disposer de la data
ex : un topic est reparti sur deux partitions avec un facter de repliaction de 2 : le tout hebergé sur un cluster de 3 broker :
b : broker
p: partition
rp : replication de topic ( donc de partition )

  b1    b2    b3

  p0    p1    rp1
        rp0

on a donc  2 boker qui heberge les deux partitions du topics
on a aussi deux partition répliquées des deux partitions initiale de topic

Un facter de replication de 2 : signifie donc qu'on a deux copies de données.
Si on perd b2 : on a peut toujours servir les données depuis b1 et b3

= Leader de partition : =

un broker et un seul peut être à tout moment le leader d'une partition donnée.
Seulement le leader de la parition peut recevoir et distribuer les données pour une partition.
Les autres brokers vont juste synchroniser  les data.
Donc chaque partition a un leader et plusieurs ISR : in-sync-replica

pour reprendre notre exemple 
pour un topic de deux paritions avec un facter de replication de 2 :
broker 1 est Leader de partition p0
la replication de cette partition est faite sur broker2 qui est donc ISR de partition 0
idem broker2 est leader pour la partition 1 qui est répliquee sur broker 3 qui est donc isr de celle ci 

  b1    b2    b3

 L p0    L p1    I rp1
         I rp0

Si on pert un broker leader d'une partition alors une élection à lieu et ici si on perd b1 alors b2 deviendra leader sur p0

les synchros, elections sont faites par zookeeper.


=  producers : =

les producers ecrivent des messages dans des topics ( constitués de partitions.)
ils savent automatiquement vers quels brokers et partitions ils doivent ecrire.
En cas de panne d'un broker , le producer sait contourner le souci

                         b1 partition1 :  012345...
producer --> messages    b2 partition2 :  012345678...
                         b3 partition0 :  012...
la charge est repartie sur tous les brokers  grace au partitionnement 

Le producer peut choisir de recevoir le ack de la transaction d'ecriture ou pas 

acks=0 : pas de ack attendu avant d'emmettre un nouveau messages. Des données peuvent être perdues.
acks=1 : on va attendre le ack d'un leader.
acks=all : on attend de recevoir le ack de tout le monde : leader et isr ( pas de perte de données.)

les producers peuvent choisir d'emettre une clé avec leur messages ( string, number ...)

si la clé est nulle alors les messages sont envoyés en round robin sur les brokers
key=null : round robin
si une clé est définie : tous les messages pour cette clé iront sur la même partition : c'est capital dans le cas ou l'ordre des messages est important pour un champ particulier par exemple.

= consumers / consumers group =

les consumers lisent les data des topics
les consumers savent depuis quel broker lire
En cas de crash d'un broker : comme les produceurs , les consumers savent retablir la situation.
Les data sont lues dans l'ordre au sein d'UNE même partition.

   broker 1                le message est lu dans l'ordre         
   topic A - partition0  : message 012345...      ---------->     consumer

Un consumer peut lire plusieurs partitions mais c'est toujours ordonné pour chaque partition.
Les lectures de chaque partition se font en parrallèle.

consumers groups :
afin de pouvoir lire les data de plusieurs partitions, les consumers lisent les data dans des consumer groups.
chaque consumer dans un group de consumer lit des data exclusives à une partition donnée.
si on a plus de consumers que de partitions alors certains consumers seront inactifs.
              
                topic A - p0   topic A - p1   topic A - p2   
                  
                                                   
      consumer1  consumer2            consumer1  consumer2            consumer1  consumer2
      consumer group application 1    consumer group application 2    consumer group application 3    

les 3 partitions peuvent envoyer chacune les data vers chacun des group de consumers qui seront par exemple dédié chacun a une application (dashboard, logs ...)
au sein de ces groups , chaque consumer va pouvoir recevoir les data de 1 ou plusieurs partitions.
Les consumers vont automatiquement utiliser des GroupCoordinator et des ConsumerCordinator pour affecter un consumer à une partition.

Si on a 3 partitions a consommer et qu'on a un consumer-group de 4 consumers : un sera donc inactif. Cela peut être voulu au cas ou on perd un consumer le 4eme inactif sert de spare.

= consumer offset =

kafka enregistre les offset que chaque consumer-group est en train de lire.
Quand un consumer a recu les data de kafka , il commit les infos et les renvoient à kafka en ecrivant dans un topic special : __consumer_offset
Si un consumer crash, a son retour en prod, il pourra reprendre la lecture à l'endroit ou il s'est arrêté.

= Delivery semantic for consumer =

Les consumers vont pouvoir  choisir quand ils décident de commiter les offsets dans kafka.
Il y a trois possibilités :

-> at most once : commit de l'offset des que le message est reçu.
si le process a un probleme : le message est perdu.
-> al least once : ( methode préférable et plus utilisée) : commit de l'offset des que le message a été traité.
si le process a un probleme : le message sera de nouveau lu.
Attention : dans ce cas on peut avoir un traitement de message en double : il faut s'assurer que cela ne pose pas de probleme à nos applis.
-> Exactly once : cette methode est gérée par kafka en interne avec l'utilisation de kafka streams api.
pour kafka et les systems externe il faut s'asurer de l'utilisation d'un consumer indempotent : afin de ne pas avoir de duplicate en bdd par exemple.

= kafka broker discovery =
chaque broker d'un cluster est appellé un boostrap server : des qu'on etabli la connexion a un broker , on est connecté au cluster.
chaque broker connait tous les brokers du clusters, les topics et les partitions. Les infos sont stockées en metadata.

kafka client    1-> connexion + metadata request                                                broker12  broker1   broker2
                                                       2 <-  envoie de la liste des brokers     broker3   broker4
                                             
                3 -> connexion possible aux brokers nécéssaires (ex :broker4 )                             

= zookeeper =

zookeeper gére les broker ( leur liste). 
Il aide à l'election de leader pour les partitions
zookeeper envoi des notifs à kafka en cas de modification ( ajout de topics, suppression de topics, ajout de broker, crash de brokers ...) 

Kafka ne peut pas travailler sans zookeeper
zookeeper par defaut travaille avec un nombre impair de servers.
zookeeper a un leader ( qui gere les ecritures ) et des followers qui s'occupent des lectures.

Les producers et consumers n'ecrivent pas dans zookeeper. Ils ecrivent dans zookeeper.
kafka gere les metadata dans zookeeper.

A partir de la version 0.10 de kafka, zookeeper ne stocke pas les offsets de consumers.
Il est maintenant complétement isolé des producers et consumers.

 z: zookeeper
 f: followers
 l: leader
 b: broker
          z1 f   <--->  z2 l <---> z3 f
           |\           | \        \

          b1 b2        b3   b4        b5 

on peut avoir des nodes zookeepers séparés des brokers .
Il est possible de faire cohabiter les deux applis sur la même machine ( c'est cependant déconseillé.)

= kafka garantee =

-> les messages sont ajoutés dans les topics dans l'ordre de leur envoi.
-> les consumers lisent les messages stockés dans les topics dans l'ordre.
-> on peut avoir une tolerance de panne grace au facteur de réplication.
avec un facter de repli de 3 : on peut donc avoir un noeud en maintenance et un node qui crash 
-> tant que le nombre de partition pour un topic ne change pas , la même clé sera envoyée à la même partition.











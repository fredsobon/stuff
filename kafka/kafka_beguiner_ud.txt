==== notes kafka beguinner udemy ====

==  Theorie / generalités== 

systeme pour permette la gestion de flux de données.
exemples d'utilisation :
- message bus
- tracking d'activité
- recupération de métrique de differentes sources
- consolidation de logs
- stream processing ( avec spark par ex )
archi distribuée, resilients ,scalable , très rapide (très peu de latence) 

= topics / partitions / offsets : =

les messages stockés sont sous forme clé / valeur et de base ne dépassent pas 1Mb.
Chaque message est enregistré dans un topic kafka : 

- topics : est la base de kafka.
-> c'est une flux de données ( similaire a une table en bdd )
-> il est identifié par son nom 
-> nombre de topic illimité
les topics sont  découpés en partitions 

chaque topic est multisubscriber : 0 à n producer et 0 à n consumer

- partitions :
conteneur pour topics.
chaque partition va contenir des records spécifiques.

ex : 
partition   message 
p1          a  b c 
p2          d  e f g
p3          i

Les nouveaux messages arrivent toujours à la fin de la partition 
Les messages sont immuables et ordonnés.

ex : 
1 topic est découpé en trois partitions 
Chaque  partition est ordonnées
chaque  message arrivant dans une partition ont un id incrementés : c'est un offset. Id unique 

Chaque offset n'a un sens que pour sa propre partition.
on defini le nombre de partition d'un topic à sa creation
L'ordre est garanti uniquement au sein de la même partition.

Les données dans kafka on une durée de vie limitée ( de base un message est conservé 1 semaine ) : durée de retention. On peut aussi définir en fonction de la taille du topic. > delete les messages les plus anciens quand on arrive à la limite de la taille définie.

Dès qu'une data est ecrite on ne peut pas la modifier : c'est l'immutabilité.
Les données sont assignées aléatoirement à une partition. Sauf si une clé est fournie.

exemple d'un topic spilté en 3 partitions 

/opt/kafka/bin/kafka-topics.sh  --describe  --zookeeper localhost:2181 |more
...
Topic:email_validation	PartitionCount:3	ReplicationFactor:2	Configs:
	Topic: email_validation	Partition: 0	Leader: 3	Replicas: 3,4	Isr: 3,4
	Topic: email_validation	Partition: 1	Leader: 3	Replicas: 3,4	Isr: 3,4
	Topic: email_validation	Partition: 2	Leader: 3	Replicas: 3,4	Isr: 3,4
...

on va pouvoir paralléliser les  traitements grâce aux partitions 
ex : on a un producer qui ecrit dans trois partitions différentes.
on pourra donc mettre 3 producers et chaqun pourra ecrire dans une partition : on ira trois fois plus vite.




= brokers =

-> les brokers sont les endroits ou sont stockées les partitions des topics. On a régulièrement des clusters de brokers
-> Chaque broker est identifié par un id : obigatoirement un nombre.
-> chaque broker contient uniquement certaines partitions de topics.

Une fois connecté à un broker : on est automatiquement connecté à tous les noeuds du cluster.

= topic replication facter : =

Le facteur de réplication va être utiliser pour la tolérance aux pannes : defini le nombre de fois ou la donnée est répliquée dans un cluster kafka.
le facteur de replication va de 1 à N ( N = nombre de broker du cluster. )


Tolérance de (replication facter -1 ) broker en panne.

si on a 4 brokers avec un facteur de réplication à 3 on pourra donc avoir 3 -1 = 2 brokers en panne. Les données du cluster seront toujours dispo dans cette configuration car chaque partition encore dispo.

Biensur les performances seront amoindries.

On estime par défaut que le replication facter défini à 3 est recommandé.

-> les topics doivent avoir un facter de replication (ex : 2 , 3 ..) : si un broker est HS on continu à disposer de la data
ex : un topic est reparti sur deux partitions avec un facter de repliaction de 2 : le tout hebergé sur un cluster de 3 broker :
b : broker
p: partition
rp : replication de topic ( donc de partition )

  b1    b2    b3

  p0    p1    rp1
        rp0

on a donc  2 brokers qui hebergent les deux partitions du topics
on a aussi deux partition répliquées des deux partitions initiale de topic

Un facter de replication de 2 : signifie donc qu'on a deux copies de données.
Si on perd b2 : on a peut toujours servir les données depuis b1 et b3



= Leader de partition : =


On va définir pour chaque partition un broker leader 

un broker et un seul peut être à tout moment le leader d'une partition donnée.
Seulement le leader de la partition peut recevoir et distribuer les données pour une partition.
Les producers ne peuvent écrirent que dans partitions Leaders des brokers.
Les consumers ne peuvent récupérer les données que depuis les partitions Leader des brokers.

Les autres brokers vont juste synchroniser les data. On les appelle aussi des followers.
La partitions leaders va répliquer les données sur les followers.
Donc chaque partition a un leader et plusieurs ISR : in-sync-replica : qui est le status de réplication des followers.

Si on a un fail d'une partition Leader sur un broker : une nouvelle election de Leader est faite pour avoir un broker désigné comme Leader sur la partition.
On peut avoir un broker biensur Leader de plusieurs partitions.

Si notre machinne en panne revient up : nous n'avons pas de nouvelle élection. Le/les broker qui ont pris le lead sur une ou des partitions de la machinne tombée en panne,  restent Leader et la machinne maintenant réparée devient follower des partitions sur lesquelles etaient leader avant la panne. 

pour reprendre notre exemple 
pour un topic de deux paritions avec un facter de replication de 2 :
broker 1 est Leader de partition p0
la replication de cette partition est faite sur broker2 qui est donc ISR de partition 0
idem broker2 est leader pour la partition 1 qui est répliquee sur broker 3 qui est donc isr de celle ci 

  b1    b2    b3

 L p0    L p1    I rp1
         I rp0

Si on pert un broker leader d'une partition alors une élection à lieu et ici si on perd b1 alors b2 deviendra leader sur p0

les synchros, elections sont faites par zookeeper.


exemple de topic réparti en trois partition avec un facteur de réplication de 2 dans un cluster de 2 brokers 
Chaque partition est répliquée 1 fois. On voit que chaque partition est sur un leader différent et que nous avons bien 2 réplicats. Les partitions sont bien répliquées et synchro : isr

/opt/kafka/bin/kafka-topics.sh  --describe  --zookeeper localhost:2181 |more
...
Topic:email_validation  PartitionCount:3        ReplicationFactor:2     Configs:
        Topic: email_validation Partition: 0    Leader: 1       Replicas: 1,2   Isr: 1,2
        Topic: email_validation Partition: 1    Leader: 2       Replicas: 1,2   Isr: 1,2
        Topic: email_validation Partition: 2    Leader: 1       Replicas: 1,2   Isr: 1,2
...



=  producers : =

les producers ecrivent des messages dans des topics ( constitués de partitions.)
ils savent automatiquement vers quels brokers et partitions ils doivent ecrire.

Les messages sont envoyés par batchs.

En cas de panne d'un broker , le producer sait contourner le souci

                         b1 partition1 :  012345...
producer --> messages    b2 partition2 :  012345678...
                         b3 partition0 :  012...
la charge est repartie sur tous les brokers  grace au partitionnement 

Le producer peut choisir de recevoir le ack de la transaction d'ecriture ou pas 

acks=0 : pas de ack attendu avant d'emmettre un nouveau messages. Des données peuvent être perdues.
acks=1 : on va attendre le ack d'un leader.
acks=all : on attend de recevoir le ack de tout le monde : leader et isr ( pas de perte de données.)

les producers peuvent choisir d'emettre une clé avec leur messages ( string, number ...)

si la clé est nulle alors les messages sont envoyés en round robin sur les brokers
key=null : round robin
si une clé est définie : tous les messages pour cette clé iront sur la même partition : c'est capital dans le cas ou l'ordre des messages est important pour un champ particulier par exemple.

on peut donc avoir différentes stratégies de partitionnement pour l'envoie de messages 

> round roubin :
chaque message est envoyé à chacune des partitions 
0 puis 1 puis 2 puis 0 puis 1 puis 2 ...

> hash de clé :
au moment ou on envoi la données on hash la clé du message puis on envoie a une partition donnée 
idem pour la clé suivante ..
On enverra donc toujours les messages correspondants au hash de la clé à la même partition.

 producer                 partition
 message 

clé  valeur 

A     bob    (hash de A envoi sur partition 1)   >>>                1    
B     bib    (hash de B envoi sur partition 2 )  >>>                2
A     bab  >>> envoie sur partition 1 car hash de A 

...

Il est important de bien choisir son mode partitionnement 

Si on a un hash de clé tres présent on aura une partition qui sera tres remplie et donc un consummer qui aura beaucoup de données à dépiler : ce qui peut engendrer du lag ( données qui s'empilent dans le topic. )

= consumers / consumers group =


les consumers lisent les data des topics. un consumer s'abonne a un ou plusiers topics.
Ce sont les consumers qui demandent aux brokers de recupérer les données en faisant des fetchs requests.
L'ordre de lecture est bien garanti par partition : le consumer va lire les données les plus anciennes en premier.


les consumers savent depuis quel broker lire
En cas de crash d'un broker : comme les produceurs , les consumers savent retablir la situation.
Les data sont lues dans l'ordre au sein d'UNE même partition.

   broker 1                le message est lu dans l'ordre         
   topic A - partition0  : message 012345...      ---------->     consumer

Un consumer peut lire plusieurs partitions mais c'est toujours ordonné pour chaque partition.
Les lectures de chaque partition se font en parrallèle.

consumers groups :

un consumer group est un ensemble de consumer.
chaque consumer va lire une ou des partitions mais uniquement les partitions non lues par d'autre consumers du consumers group.
On a une repartition des partitions des consumers au sein d'un consumer group.

si on un un consumer group de 3 consumer et un topic reparti en trois partitions , chaque consumer pourra lire les données d'une partitions précise.
Si un consumer tombe en panne on a donc une partition qui n'est plus consommée. Dans ce cas on aura une élection de consummer qui lira cette partition.

On défini un group id pour les consumer group 

afin de pouvoir lire les data de plusieurs partitions, les consumers lisent les data dans des consumer groups.
chaque consumer dans un group de consumer lit des data exclusives à une partition donnée.
si on a plus de consumers que de partitions alors certains consumers seront inactifs.
              
                topic A - p0   topic A - p1   topic A - p2   
                  
                                                   
      consumer1  consumer2            consumer1  consumer2            consumer1  consumer2
      consumer group application 1    consumer group application 2    consumer group application 3    

les 3 partitions peuvent envoyer chacune les data vers chacun des group de consumers qui seront par exemple dédié chacun a une application (dashboard, logs ...)
au sein de ces groups , chaque consumer va pouvoir recevoir les data de 1 ou plusieurs partitions.
Les consumers vont automatiquement utiliser des GroupCoordinator et des ConsumerCordinator pour affecter un consumer à une partition.

Si on a 3 partitions a consommer et qu'on a un consumer-group de 4 consumers : un sera donc inactif. Cela peut être voulu au cas ou on perd un consumer le 4eme inactif sert de spare.

= consumer offset =

On a vu que l'offset identifie de manière unique un message dans une partition
On peut utiliser aussi un tripet : 

offset + consumer group + num de partition pour identifier précisement l'endroit ou se trouve notre application / notre consumer dans la lecture de données dans le topic.
Les consumers d'un consumer group partagent le même offset.
La mise à jour de l'offset set fait en continue au fur et à mesure de la lecture.

Un consumer peut relire un topic depuis un ancien offset ou passer des records et lire depuis maintenant.


on va choisir à la creation du consumer group, la politique de lecture des données.
exemple si on a un nouveau consumer group qui va lire un topic jamais consummé par ce consumer group on va utiliser le param : auto.offset.reset 

> earliest ( plus vieux offset)
> latest ( le plus recent)
> non ( exeption si pas d'offset dans le consumer group.) 


kafka enregistre les offset que chaque consumer-group est en train de lire.
Quand un consumer a recu les data de kafka , il commit les infos et les renvoient à kafka en ecrivant dans un topic special : __consumer_offset
Si un consumer crash, a son retour en prod, il pourra reprendre la lecture à l'endroit ou il s'est arrêté.


La strategie par defaut des offset dans kafka est l'autocommit : les offset sont enregistrés automatiquement
On peut commiter en manuel. 


= Delivery semantic for consumer =

Les consumers vont pouvoir  choisir quand ils décident de commiter les offsets dans kafka.
Il y a trois possibilités :

-> at most once : commit de l'offset des que le message est reçu.
si le process a un probleme : le message est perdu.
-> al least once : ( methode préférable et plus utilisée) : commit de l'offset des que le message a été traité.
si le process a un probleme : le message sera de nouveau lu.
Attention : dans ce cas on peut avoir un traitement de message en double : il faut s'assurer que cela ne pose pas de probleme à nos applis.
-> Exactly once : cette methode est gérée par kafka en interne avec l'utilisation de kafka streams api.
pour kafka et les systems externe il faut s'asurer de l'utilisation d'un consumer indempotent : afin de ne pas avoir de duplicate en bdd par exemple.

= kafka broker discovery =
chaque broker d'un cluster est appellé un boostrap server : des qu'on etabli la connexion a un broker , on est connecté au cluster.
chaque broker connait tous les brokers du clusters, les topics et les partitions. Les infos sont stockées en metadata.

kafka client    1-> connexion + metadata request                                                broker12  broker1   broker2
                                                       2 <-  envoie de la liste des brokers     broker3   broker4
                                             
                3 -> connexion possible aux brokers nécéssaires (ex :broker4 )                             

= zookeeper =


zookeeper va servir à l'admin de notre cluster. 
composé de znodes qui vont servir à l'élection de leader

zookeeper gére les broker ( leur liste). 
Il aide à l'election de leader pour les partitions
zookeeper envoi des notifs à kafka en cas de modification ( ajout de topics, suppression de topics, ajout de broker, crash de brokers ...) 

Kafka ne peut pas travailler sans zookeeper
zookeeper par defaut travaille avec un nombre impair de servers.
zookeeper a un leader ( qui gere les ecritures ) et des followers qui s'occupent des lectures.

Les producers et consumers n'ecrivent pas dans zookeeper. Ils ecrivent dans zookeeper.
kafka gere les metadata dans zookeeper.

A partir de la version 0.10 de kafka, zookeeper ne stocke pas les offsets de consumers.
Il est maintenant complétement isolé des producers et consumers.

 z: zookeeper
 f: followers
 l: leader
 b: broker
          z1 f   <--->  z2 l <---> z3 f
           |\           | \        \

          b1 b2        b3   b4        b5 

on peut avoir des nodes zookeepers séparés des brokers .
Il est possible de faire cohabiter les deux applis sur la même machine ( c'est cependant déconseillé.)

= kafka garantee =

-> les messages sont ajoutés dans les topics dans l'ordre de leur envoi.
-> les consumers lisent les messages stockés dans les topics dans l'ordre.
-> on peut avoir une tolerance de panne grace au facteur de réplication.
avec un facter de repli de 3 : on peut donc avoir un noeud en maintenance et un node qui crash 
-> tant que le nombre de partition pour un topic ne change pas , la même clé sera envoyée à la même partition.


=  quick resume =



source system   ->      producers        ->          kafka cluster                                          ->            consumers               ->  target system   
                     - round robin                     broker1     - topics                                                 - consumers offsets
                     - key base ordering               broker2     - partitions                                             - consumers group
                     - ack strategy                    brokern     - replication                                            - at least once
                                                                   - partition leader / isr (in sync replicat)              - at most once 
                                                                   - offset topics
                                                          |
                                                          |
                                                         
                                                         zookeeper 
                                                          - leader / follower
                                                          - broker management 


=== schema registry : ==

developpé par confluent.
schema versionné avec un id unique.
permet de garder une cohérence dans la structure des records de nos topics
Les schemats définissent la structure des records. (ex : topic user : on veut un nom en string etc ....) 
On va avoir un schema de défini par topic.
Les produceurs vont être obligés de respecter ce schemat pour produire les données dans ce topic.
Les producers vont créer un record avec un id de schema registry associé.

Les consumers vont pouvoir récupérer les schemats du topic et désérialiser directement les données du topic dans le bon format.



=== cluster set up ===                                                           

box vagrant pour lab : 
vagrant init ubuntu/xenial64
vagrant up
vagrant ssh 


- setup : 

.java8 est mandatory : 

root@ubuntu-xenial:~# apt install openjdk-8-jdk
check de version ok :
root@ubuntu-xenial:~# java -version
openjdk version "1.8.0_181"
OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-1ubuntu0.16.04.1-b13)
OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)

. download et install kafka : 

https://kafka.apache.org/downloads

on travaille ici sur une version 2.0
on recupere les binaires : version 2.12.2.0.0 : kafka_2.12-2.0.0.tgz
on place le tgz par exemple dans /opt

root@ubuntu-xenial:/opt/# tar -xzvf kafka_2.12-2.0.0.tgz 

t@ubuntu-xenial:/opt/# ls
kafka_2.12-2.0.0  kafka_2.12-2.0.0.tgz
root@ubuntu-xenial:/opt/kafka_2.12-2.0.0# ls
bin  config  libs  LICENSE  NOTICE  site-docs

on teste :

root@ubuntu-xenial:/opt/kafka_2.12-2.0.0/bin# ./kafka-topics.sh 
Create, delete, describe, or change a topic.
Option                                   Description                            
------                                   -----------                            
--alter                                  Alter the number of partitions,        
                                           replica assignment, and/or           
                                                                                      configuration for the topic.         

..on a bien une sortie console : l'installe de java et de kafka est ok.

on va créer un symlik kafka et ajouter le repertoire des bianaires dans notre profile :

root@ubuntu-xenial:/opt# ln -s kafka_2.12-2.0.0 kafka
root@ubuntu-xenial:/opt# /opt/kafka/bin/kafka-topics.sh 

root@ubuntu-xenial:~# cat ~/.profile 
# ~/.profile: executed by Bourne-compatible login shells.

if [ "$BASH" ]; then
  if [ -f ~/.bashrc ]; then
    . ~/.bashrc
      fi
      fi
mesg n || true
export PATH=$PATH:/opt/kafka/bin


on a maintenant la completion pour les commandes kafka : kafka- tab :
root@ubuntu-xenial:~# kafka-
kafka-acls.sh                        kafka-preferred-replica-election.sh
kafka-broker-api-versions.sh         kafka-producer-perf-test.sh
kafka-configs.sh                     kafka-reassign-partitions.sh
kafka-console-consumer.sh            kafka-replica-verification.sh
kafka-console-producer.sh            kafka-run-class.sh
kafka-consumer-groups.sh             kafka-server-start.sh
kafka-consumer-perf-test.sh          kafka-server-stop.sh
kafka-delegation-tokens.sh           kafka-streams-application-reset.sh
kafka-delete-records.sh              kafka-topics.sh
kafka-dump-log.sh                    kafka-verifiable-consumer.sh
kafka-log-dirs.sh                    kafka-verifiable-producer.sh
kafka-mirror-maker.sh                

- set up kafka :

on cree un rep data dans le kafka directory et un repertoire zookeeper dans data :
Setting up tree (1.7.0-3) ...
mkdir -p data/zookeeper
root@ubuntu-xenial:/opt/kafka# tree data/
data/
└── zookeeper

on va editer le fichier de conf zookeeper :
et ajouter le path de nos data :

root@ubuntu-xenial:/opt/kafka# cat config/zookeeper.properties 
# the directory where the snapshot is stored.
dataDir=/opt/kafka/data/zookeeper
# the port at which the clients will connect
clientPort=2181
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0


on va demarrer zookeeper :

root@ubuntu-xenial:/opt/kafka# zookeeper-server-start.sh config/zookeeper.properties 
[2018-11-04 18:01:42,680] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
...
[2018-11-04 18:01:42,749] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)

->> si on bind sur le port 2181 alors c'est cool.


on créer un folder kafka dans le repertoire data 
root@ubuntu-xenial:/opt/kafka/data# mkdir kafka
root@ubuntu-xenial:/opt/kafka/data# ls
kafka  zookeeper

et on edite le fichier server.properties :on rempli le path de notre repertoire kafka créee :

root@ubuntu-xenial:/opt/kafka# grep log.dirs config/server.properties 
log.dirs=/opt/kafka/data/kafka


on va demarrer kafka : Attention zookeeper doit toujours être démarré ! 

root@ubuntu-xenial:/opt/kafka# kafka-server-start.sh config/server.properties
...
[2018-11-04 18:13:55,087] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[2018-11-04 18:13:55,089] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)

on voit que tout est ok et que notre broker a un id de setté id=0
On peut remarquer également que des fichiers ont été crées dans le repertoire data/kafka :

root@ubuntu-xenial:/opt/kafka/data/kafka# ll
total 12
drwxr-xr-x 2 root root 4096 Nov  4 18:13 ./
drwxr-xr-x 4 root root 4096 Nov  4 18:04 ../
-rw-r--r-- 1 root root    0 Nov  4 18:13 cleaner-offset-checkpoint
-rw-r--r-- 1 root root    0 Nov  4 18:13 .lock
-rw-r--r-- 1 root root    0 Nov  4 18:13 log-start-offset-checkpoint
-rw-r--r-- 1 root root   54 Nov  4 18:13 meta.properties
-rw-r--r-- 1 root root    0 Nov  4 18:13 recovery-point-offset-checkpoint
-rw-r--r-- 1 root root    0 Nov  4 18:13 replication-offset-checkpoint
ex : 
root@ubuntu-xenial:/opt/kafka/data/kafka# cat meta.properties 
#
#Sun Nov 04 18:13:54 UTC 2018
version=0
broker.id=0


== kafka cli ==


on va pouvoir utiliser des commandes directement embarquées lors de l'install de kafka, cela permet de gérer facilement l'administration des topics( replication, partition, modif de retention, offset etc ..) et de pouvoir debuguer facilement en utilisant un produceur et un consumer en cli sans avoir besoin d'être developper.

/!\ les examples qui vont suivre seront issus de l'utilisation des commandes via un cluster kafka monté dans kube : on a aura des différences d'options sur un cluster standalone : notamment pour l'utilisation des arg bootstrap-server ( à la place de zookeeper  et <le nom du service zookeeper:2181> dans kube > mais qui devrait être obsolete à terme )  

on va se connecter en cli dans le pod kafka-client et executer les binaires kafka presents dans le path /usr/bin 

kctl exec -it kafka-client -- /bin/bash                                                                                            [☸ |minikube:default]
root@kafka-client:/# ls
bin  boot  dev	etc  home  lib	lib64  media  mnt  opt	proc  root  run  sbin  srv  sys  tmp  usr  var


root@kafka-client:/# ls /usr/bin/ka*
/usr/bin/kadmin			    /usr/bin/kafka-console-producer    /usr/bin/kafka-dump-log			  /usr/bin/kafka-producer-perf-test    /usr/bin/kafka-server-stop
/usr/bin/kafka-acls		    /usr/bin/kafka-consumer-groups     /usr/bin/kafka-leader-election		  /usr/bin/kafka-reassign-partitions   /usr/bin/kafka-streams-application-reset
/usr/bin/kafka-broker-api-versions  /usr/bin/kafka-consumer-perf-test  /usr/bin/kafka-log-dirs			  /usr/bin/kafka-replica-verification  /usr/bin/kafka-topics
/usr/bin/kafka-configs		    /usr/bin/kafka-delegation-tokens   /usr/bin/kafka-mirror-maker		  /usr/bin/kafka-run-class	       /usr/bin/kafka-verifiable-consumer
/usr/bin/kafka-console-consumer     /usr/bin/kafka-delete-records      /usr/bin/kafka-preferred-replica-election  /usr/bin/kafka-server-start	       /usr/bin/kafka-verifiable-producer
root@kafka-client:/# 

= topics : =

- lister les topics :

-> on peut lister les topics avec l'argument --zookeeper et l'adresse du svc zookeeper:port mais cette methode  devient deprecated 

root@kafka-client:/usr/bin# /usr/bin/kafka-topics --zookeeper kafka-cp-zookeeper:2181 --list
__confluent.support.metrics
__consumer_offsets
_confluent-command
_confluent-controlcenter-5-5-0-1-AlertHistoryStore-changelog
_confluent-controlcenter-5-5-0-1-Group-ONE_MINUTE-changelog
....
......
_confluent-ksql-kafka_command_topic
_confluent-metrics
_confluent-monitoring
_schemas
boogie-topic
kafka-cp-kafka-connect-config
kafka-cp-kafka-connect-offset
kafka-cp-kafka-connect-status
my-confluent-topic


- on peut lister les topics en donnant l'adresse du service d'un broker kafka (quand on dans kube) ou la liste des brokers individuels :

^Croot@kafka-client:/# kafka-topics --list --bootstrap-server kafka-cp-kafka:9092
__confluent.support.metrics
__consumer_offsets
_confluent-command
..
.....
_confluent-metrics
_confluent-monitoring
_schemas
boogie-topic
kafka-cp-kafka-connect-config
kafka-cp-kafka-connect-offset
kafka-cp-kafka-connect-status
my-confluent-topic


- description d'un topic :

root@kafka-client:/# kafka-topics --describe --zookeeper kafka-cp-zookeeper:2181 --topic boogie-topic
Topic: boogie-topic	PartitionCount: 3	ReplicationFactor: 3	Configs: 
	Topic: boogie-topic	Partition: 0	Leader: 0	Replicas: 0,2,1	Isr: 2,1,0
	Topic: boogie-topic	Partition: 1	Leader: 1	Replicas: 1,0,2	Isr: 2,1,0
	Topic: boogie-topic	Partition: 2	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0


root@kafka-client:/# kafka-topics --describe --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic
Topic: boogie-topic	PartitionCount: 3	ReplicationFactor: 3	Configs: 
	Topic: boogie-topic	Partition: 0	Leader: 0	Replicas: 0,2,1	Isr: 1,0,2
	Topic: boogie-topic	Partition: 1	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2
	Topic: boogie-topic	Partition: 2	Leader: 2	Replicas: 2,1,0	Isr: 1,0,2


- creation topic :

on examinera le path de nos binaires kafka (l'emplacement peut changer en fonction du set up : standalone, kubernetes ...) 


root@kafka-client:/# kafka-topics --create --bootstrap-server kafka-cp-kafka:9092 --replication-factor 2 --topic boogie-topic-replication
Created topic boogie-topic-replication.

root@kafka-client:/# kafka-topics --describe --bootstrap-server kafka-cp-kafka:9092  --topic boogie-topic-replication
Topic: boogie-topic-replication	PartitionCount: 1	ReplicationFactor: 2	Configs:
	Topic: boogie-topic-replication	Partition: 0	Leader: 1	Replicas: 1,2	Isr: 1,2


on peut créer un topic avec le nombre de partition désiré 
ex :
root@kafka-client:/# kafka-topics --create --bootstrap-server kafka-cp-kafka:9092 --partitions 2 --replication-factor 2 --topic boogie-topic-partition
Created topic boogie-topic-partition.

root@kafka-client:/# kafka-topics --describe --bootstrap-server kafka-cp-kafka:9092  --topic boogie-topic-partition
Topic: boogie-topic-partition	PartitionCount: 2	ReplicationFactor: 2	Configs:
	Topic: boogie-topic-partition	Partition: 0	Leader: 2	Replicas: 2,1	Isr: 2,1
	Topic: boogie-topic-partition	Partition: 1	Leader: 1	Replicas: 1,0	Isr: 1,0

on voit qu'on a deux partitions dans ce topic et que chacune est répliquée deux fois.
on voit que chacune des partition est sur un broker différent.


=  producer : = 

on va pouvoir utiliser pour du debug la console producer qui permet d'injecter directement dans un topic donné :

root@kafka-client:/# kafka-console-producer  --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic
>test injection dans topic via console producer
>ok

on peut ecrire un message de type clé valeur si on veut. on va pour cela definir dans notre ligne de commande la fonction permettant de définir la clé et un séparateur permettant de savoir ce qui sépare la clé de la valeur dans notre message :

root@kafka-client:/# kafka-console-producer  --bootstrap-server kafka-cp-kafka:9092 --property parse.key=true --property key.separator=: --topic boogie-topic
>user:bob
>age:77

= consumer : =

on va pouvoir vérifier qu'on peut bien consommer les données crées précédemment par notre producer :

on lance dans une console notre consumer avec le topic qu'on veut consommer en argument et dans une seconde console notre produceur dans lequel on envoi les datas 


root@kafka-client:/# kafka-console-producer  --bootstrap-server kafka-cp-kafka:9092 --property parse.key=true --property key.separator=: --topic boogie-topic
>name:boogie
>age:778


root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic
boogie
778

on voit que de base notre consumer ne présente que les valeurs du topic (pas les clés) et que seuls les nouveaux messages du producer sont consommés.


- consommation de tout le topic depuis le debut des entrées : 
on peut consommer tous les messages du topic avec l'option from-beguinning :

root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic --from-beginning
ok
77
778
boogie
test injection dans topic via console producer
bob


- affichage des clés 
on va pouvoir afficher les clés du topic.
les messages qui n'ont pas de clé auront la partie clé affichée à null.

root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic --from-beginning --property print.key=true
null	ok
age	77
age	778
name	boogie
null	test injection dans topic via console producer
user	bob

- affichage des timestamps : 

on va pouvoir afficher les timestamps de nos messages 

root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic --from-beginning --property print.key=true --property print.timestamp=true
CreateTime:1610873470522	null	ok
CreateTime:1610873704150	age	77
CreateTime:1610873952206	age	778
CreateTime:1610873941418	name	boogie
CreateTime:1610873454068	null	test injection dans topic via console producer
CreateTime:1610873698022	user	bob

- lecture d'une partition en particulier :

il est possible de lire les données d'un topic sur une partition particulière :

ici on va lire sur la partition 0 : 
root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic --from-beginning --property print.key=true --partition 0
null	ok
age	77
age	778

- affichage d'une entrée d'un offset particulier :

on est obligé de préciser la partition du num d'offset que l'on veut lire 
dans le cas de selection d'un offset on va lire tous les messages a partir de cet offset et skipper tous les messages précédents : 


root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic  --property print.key=true --partition 0 --offset 2
null	ok
age	77
age	778
^CProcessed a total of 3 messages
root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic  --property print.key=true --partition 0 --offset 3
age	77
age	778
^CProcessed a total of 2 messages
root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic  --property print.key=true --partition 0 --offset 4
age	778
^CProcessed a total of 1 messages

- precision de serialiazer et deserializer a utiliser :

de base le serializer est de type string . si on injecte des données avec le serializer avro la lecture du topic avec le serializer de base string sera illisible .

on peut avoir tout type de serializer et deserializer ( text, avro ...) : on va donc pouvoir préciser le type qu'on veut utiliser :

on va pouvoir le faire pour nos clés et nos valeurs :

root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic --from-beginning --property print.key=true --partition 0 --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
null	ok
age	77
age	778


idem pour nos valeurs :

root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic --from-beginning --property print.key=true --partition 0 --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
null	ok
age	77
age	778

ici on a pas de changement vu que l'injection des données est en mode string ..la lecture en mode string par default ne montre donc pas de différence.


=  consumer-group : =

visualiser, manipuler les consumers groups et leur offset , reset des offset au debut à la fin ou a une position donnée pour le topic.

sur la console consumer on va préciser un consumer group en argument ce qui va le créer : 
root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic --group boogiegroup --from-beginning
ok
77
778
boogie
test injection dans topic via console producer
bob


- on va lister nos consummers group :

root@kafka-client:/# kafka-consumer-groups --bootstrap-server kafka-cp-kafka:9092 --list
boogiegroup

- description de consummer group :

root@kafka-client:/# kafka-consumer-groups --bootstrap-server kafka-cp-kafka:9092 --describe --group boogiegroup

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                                 HOST            CLIENT-ID
boogiegroup     boogie-topic    0          5               5               0               consumer-boogiegroup-1-55445fc7-d080-4f02-9498-4a43361a1b50 /172.17.0.12    consumer-boogiegroup-1
boogiegroup     boogie-topic    1          2               2               0               consumer-boogiegroup-1-55445fc7-d080-4f02-9498-4a43361a1b50 /172.17.0.12    consumer-boogiegroup-1
boogiegroup     boogie-topic    2          4               4               0               consumer-boogiegroup-1-55445fc7-d080-4f02-9498-4a43361a1b50 /172.17.0.12    consumer-boogiegroup-1

on voit ici l'etat de notre consumer group sur le topic . 
on voit que sur chacune des partition , l'offset courant ou est rendu notre consumer group , le total de message ecrit dans chacune des partitions pour le topic ..et donc on peut en déduire le lag eventuel de consommation de topic.

on va evoyer des messages dans notre topic avec la console producer :

root@kafka-client:/# kafka-console-producer  --bootstrap-server kafka-cp-kafka:9092 --property parse.key=true --property key.separator=: --topic boogie-topic
>testconsumergroup: offset 1
>testconsumergroup: offset 2

sur une autre console on voit la consomation du topic par notre group de consumer : 

root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic --group boogiegroup --from-beginning
ok
77
778
boogie
test injection dans topic via console producer
bob
 offset 1
 offset 2

on examine le consumer group et on voit qu'on a donc deux nouveaux messages d'ecrits :

root@kafka-client:/# kafka-consumer-groups --bootstrap-server kafka-cp-kafka:9092 --describe --group boogiegroup

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                                 HOST            CLIENT-ID
boogiegroup     boogie-topic    0          7               7               0               consumer-boogiegroup-1-55445fc7-d080-4f02-9498-4a43361a1b50 /172.17.0.12    consumer-boogiegroup-1
boogiegroup     boogie-topic    1          2               2               0               consumer-boogiegroup-1-55445fc7-d080-4f02-9498-4a43361a1b50 /172.17.0.12    consumer-boogiegroup-1
boogiegroup     boogie-topic    2          4               4               0               consumer-boogiegroup-1-55445fc7-d080-4f02-9498-4a43361a1b50 /172.17.0.12    consumer-boogiegroup-1


si on continu a publier des nouveaux messages dans le topic et qu'on stoppe la lecture ( on ferme la console consumer ) on va voir du lag apparaitre :
root@kafka-client:/# kafka-console-producer  --bootstrap-server kafka-cp-kafka:9092 --property parse.key=true --property key.separator=: --topic boogie-topic
>name:boogie
>age:778
>testconsumergroup: offset 1
>testconsumergroup: offset 2
>bla1: test
>bla2: test1
>bla3: test2           


root@kafka-client:/# kafka-consumer-groups --bootstrap-server kafka-cp-kafka:9092 --describe --group boogiegroup

Consumer group 'boogiegroup' has no active members.

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
boogiegroup     boogie-topic    0          7               9               2               -               -               -
boogiegroup     boogie-topic    1          2               3               1               -               -               -
boogiegroup     boogie-topic    2          4               4               0               -               -               -
root@kafka-client:/#


on voit donc qu'on a publié 3 nouveaux messages et qu'il y a du lag ( plus de consumer en ecoute suite à la fermeture de console.) 

on voit le message "Consumer group 'boogiegroup' has no active members."


- manipulation d'offset :

on va pouvoir reset un offset de consumer group ( il ne faut pas que le consumer group soit actif ) 

root@kafka-client:/# kafka-consumer-groups --bootstrap-server kafka-cp-kafka:9092 --reset-offsets --group boogie-group --to-earliest --topic boogie-topic --dry-run

GROUP                          TOPIC                          PARTITION  NEW-OFFSET
boogie-group                   boogie-topic                   0          2
boogie-group                   boogie-topic                   1          1
boogie-group                   boogie-topic                   2          2

root@kafka-client:/# kafka-consumer-groups --bootstrap-server kafka-cp-kafka:9092 --reset-offsets --group boogie-group --to-earliest --topic boogie-topic --execute

GROUP                          TOPIC                          PARTITION  NEW-OFFSET
boogie-group                   boogie-topic                   0          2
boogie-group                   boogie-topic                   1          1
boogie-group                   boogie-topic                   2          2


root@kafka-client:/# kafka-consumer-groups --bootstrap-server kafka-cp-kafka:9092 --reset-offsets --group boogie-group --to-latest --topic boogie-topic --execute

GROUP                          TOPIC                          PARTITION  NEW-OFFSET
boogie-group                   boogie-topic                   0          15
boogie-group                   boogie-topic                   1          3
boogie-group                   boogie-topic                   2          5
root@kafka-client:/# kafka-consumer-groups --bootstrap-server kafka-cp-kafka:9092 --describe --group boogiegroup



on peut relire les offset depuis un certain numéro d'offset :

root@kafka-client:/# kafka-consumer-groups --bootstrap-server kafka-cp-kafka:9092 --reset-offsets --group boogie-group --to-offset 3 --topic boogie-topic  --execute

GROUP                          TOPIC                          PARTITION  NEW-OFFSET
boogie-group                   boogie-topic                   0          3
boogie-group                   boogie-topic                   1          3
boogie-group                   boogie-topic                   2          3
root@kafka-client:/#



on peut reset les offset  partir d'une certaine date ( relire les offsets du jour par exemple ): 
on precise la date en milisecondes :

root@kafka-client:/# kafka-consumer-groups --bootstrap-server kafka-cp-kafka:9092 --reset-offsets --group boogie-group --to-datetime 2021-01-17T10:45:00.000 --topic boogie-topic  --execute

GROUP                          TOPIC                          PARTITION  NEW-OFFSET
boogie-group                   boogie-topic                   0          15
boogie-group                   boogie-topic                   1          3
boogie-group                   boogie-topic                   2          5



= kafka-connect : =

kafka-connect va permettre de faire le lien entre l'exterrieur ( bdd , fichiers ..) et kafka.
On va connecter kafka avec les systemes externes.
On a tout un systeme de plugin qui permettent de se brancher a une bdd ,systeme de fichiers ... (s3, hdfs, elasticsearch ...) 

opensource
confluent : hub > ensemble de connecters :
- gratuits faits par la communauté
- payants commercialisés par confluent.


Les connecteur sont divisés en deux catégories :
-> source
-> sink

- Les sources :

prennent les infos du monde exterrieur et les envoient dans un topic kafka ( ex kafka-connect jdbc ..)

- Les sinks 
prennent les infos du topic et envoient vers l'exterrieur ( hdfs , .... )


on a un framwork commun pour tous les connecteurs.

kakfa-connect 

> 1 fichier commun a tous les connecter
> la conf du connecteur en lui même ( ex : mongodb =! s3 )

distribué et scalable

mode standalone > on fait tourner notre connecter en local pour du dev local
mode distributed > on va demarrer une api rest et on va publier les connecter
les kafka connect seront en cluster et auront la conf communes.


on a une extention commune qui permet les transformations simples de messages  
smt : simple message transform  ( une 20 de filtres )
pour transformer des messagess à la volée : ex anonymiser un champ  d'un topic


- creation de 2 kakfa-connect :
1 source file stream 
1 sink file stream


ex : 
source : on va avoir un fichier texte : on va recup le contenu pour l'envoyer dans un topic kafka
sink : on va lire un topic et ecrire dans un fichier texte


on crée un rep de conf kafka-connect dans lequel on va stocker nos fichiers de properties.
un fichier de properties par connecter.

on recupere les noms de services connect , kafka dans notre cluster kube et on ajoute la conf dans un rep du pod kafka-client :

on crée un fichier de conf générale et un fichier de conf par connecter 
ex : conf générale : 
cat connect-standalone.properties
bootstrap.servers=kafka-cp-kafka:9092
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.storage.StringConverter
offset.storage.file.filename=/root/kafka-connect/offset-storage.offset              <<< fichier ou sauvegarder les offsets
offset.flush.internal.flush=10000       <<<<<<< frequence de flush des offsets en ms
plugin.path=/root/plugins                           <<<<<< path des jars dans lequel on stocke nos connecter si en en fait manuellement 


on peut tester l'appel a l'api rest de connect : 
#  curl http://kafka-cp-kafka-connect:8083
{"version":"5.5.0-ccs","commit":"606822a624024828","kafka_cluster_id":"zrPa3b7YRn2SXFGcEdwxxQ"}root@kafka-client:~/kafka-connect#


maintenant on crée un fichier de conf pour notre connecter :

cat connect-file-source.properties
name=local-file-source      <<<<<<< nom de notre connecteur 
connector.class=FileStreamSource   <<<<< quelle classe de connector il utilise (text, s3 ...)
task.max=1              <<<< nombre de tache en //
topic=boogie-test       <<<< topic destination dans lequel le texte sera envoyé
file=/root/kafka-connect/test-source.txt   <<<<< fichier source depuis lequel kafka-connect prendra les données.



on va maintenant lancer le binaire connect qui se trouve dans le path de notre pod kafka-client :
avec le fichier de conf global et celui du connecteur source :

root@kafka-client:~/kafka-connect# /usr/bin/connect-standalone /root/kafka-connect/connect-standalone.properties /root/kafka-connect/connect-file-source.properties

maintenant on va lancer une console consumer pour voir le résulat dans notre topic :
on voit que les messages que l'on injecte dans le fichier source sont récupérés par le connector source et ecrits dans le topic que l'on a choisi


on ecrit du text dans notre fichier plat 
root@kafka-client:~/kafka-connect# echo "inject from txt in topic" >> /root/kafka-connect/test-source.txt^C
root@kafka-client:~/kafka-connect# echo "inject 2 from txt in topic" >> /root/kafka-connect/test-source.txt

notre connecteur le recupére bien et ecrit dans le topic : 
root@kafka-client:/# kafka-console-consumer --bootstrap-server kafka-cp-kafka:9092 --topic boogie-topic
inject from txt in topic
inject 2 from txt in topic




Wget -crawler

On va ici crawler un site pour par exemple permettre de mettre en cache les objets recupérés

 -r recursif 
 -l nombre de lien a suivre pour la profondeur du crawl
 - e robot=off ne pas se faire passer par un robot : dans ce cas le site ne renvoie que la home et robot.txt : pas possible de parcourir le site en profondeur
 --delete-after : on ne download pas les fichiers

wget -nd -r -l 10 -e robots=off --delete-after www.blabla.fr

On peut fake un useragent ( certains site n'autorisent pas le crawl en cli :

wget  -r -p -U Mozilla www.blabla.fr

Crawler d'images via proxy :

On PROD we are looping through URIs with images and calling wget llike this:

export http_proxy='proxy01:3128'
wget -T 5 -t 3 -v --connect-timeout 2 --header='X-User: pixplace' --header='X-User: crfplace' "$url" -O ${dest_dir}/$dir_id/${output_file}.${refc}.jpg 2>>"$dest_dir/$dir_id/report_wget_error.txt" 

    http://xahlee.info/linux/wget_curl_tutorial.html


===== notes etcd : ===

- etcd stocke les fichiers de configurations du cluster, comme le répertoire /etc sur Linux. Et comme pour les fichiers de configuration dans /etc, une bonne pratique consiste à avoir un backup des fichiers gérés par etcd. Ceci permettra de remettre en route plus rapidement un cluster après incident majeur.

sudo apt install etcd

- le serveur ecoute principalement sur deux ports : 
-2379 : pour les clients qui se connecteront sur etcd.
-2380 : pour les communications intra cluster etcd ( peers )  

= commandes : 

- lister les clés ( en apiv2 ) : 
ecdctl_client_ca.crt -cert-file etcdctl_client.crt --key-file etcdctl_client.key ls /coreos.com/network/config
/coreos.com/network/config

- recupérer la valeur d'une clé en apiv2 :  
etcdctl --endpoints https://etcd01:2379 -ca-file etcdctl_client_ca.crt -cert-file etcdctl_client.crt --key-file etcdctl_client.key get /coreos.com/network/config
{"Network": "10.10.0.0/16","Backend": {"Type": "vxlan"}}

- definir une clé : 
etcdctl --endpoints https://etcd01:2379 -ca-file etcdctl_client_ca.crt -cert-file etcdctl_client.crt --key-file etcdctl_client.key set '/coreos.com/network/config' '{"Network": "10.10.0.0/16","Backend": {"Type": "vxlan"}}'


- set up cluster : 

on va pouvoir demarrer avec un nombre limité de server etcd ( attention a respecter les qorum : voir matrices sur le site coreos ) 
en bonne pratique on demarre à 3 noeuds

pour ce faire on va devoir  s'assurer que certaines variables dans le fichier de conf ou service systemd comporte des valeurs specifiques :

- on va definir un token pour notre cluster : 

# Initial cluster token for the etcd cluster during bootstrap.
initial-cluster-token: "etcd-cluster-paas-kube"

- le premier demarrage du cluster ( bootstrap ) doit s'assurer qu'on a la variable
initial-cluster-state: "new" 
avec la liste des serveurs membres renseignée dans l'entree :

# Initial cluster configuration for bootstrapping.
initial-cluster: "etcdserver=https://etcdserver.boogie.net:2380"

Si on veut ajouter un nouveau membre , il faudra changer la valeur :
initial-cluster-state: "existing" 
puis alimenter la liste des peer urls


- tester en ssl :

# etcdctl --endpoints "https://etcdserver.io:2379" --ca-file /etc/kube-tls/etcd_ca.crt  --cert-file /etc/kube-tls/etcd_client.crt --key-file /etc/kube-tls/etcd_client.key  set bla blou

# etcdctl --endpoints "https://etcdserver.io:2379" --ca-file /etc/kube-tls/etcd_ca.crt  --cert-file /etc/kube-tls/etcd_client.crt --key-file /etc/kube-tls/etcd_client.key    get bla
blou

avec en conf : 


= exemples de conf : 

cat etcd/etcd.yml
# Source URL: https://raw.githubusercontent.com/coreos/etcd/master/etcd.conf.yml.sample
# This is the configuration file for the etcd server.

# Human-readable name for this member.
name: "etcdserver"

# Path to the data directory.
data-dir: "/var/lib/etcd/default.etcd"

# Number of committed transactions to trigger a snapshot to disk.
snapshot-count: 10000

# Time (in milliseconds) of a heartbeat interval.
heartbeat-interval: 100

# Time (in milliseconds) for an election to timeout.
election-timeout: 1000

# Raise alarms when backend size exceeds the given quota. 0 means use the
# default quota.
quota-backend-bytes: 0

# List of comma separated URLs to listen on for peer traffic.
listen-peer-urls: "https://0.0.0.0:2380"

# List of comma separated URLs to listen on for client traffic.
listen-client-urls: "https://0.0.0.0:2379"

# Maximum number of snapshot files to retain (0 is unlimited).
max-snapshots: 5

# Maximum number of wal files to retain (0 is unlimited).
max-wals: 5

# List of this member's client URLs to advertise to the public.
# The URLs needed to be a comma-separated list.
advertise-client-urls: "https://etcdserver.boogie.net:2379"

# List of this member's peer URLs to advertise to the rest of the cluster.
# The URLs needed to be a comma-separated list.
initial-advertise-peer-urls: "https://etcdserver.boogie.net:2380"

# Valid values include 'exit', 'proxy'
discovery-fallback: "proxy"

# Initial cluster configuration for bootstrapping.
initial-cluster: "etcdserver=https://etcdserver.boogie.net:2380"

# Initial cluster token for the etcd cluster during bootstrap.
initial-cluster-token: "etcd-cluster-paas-kubeinfra"

# Initial cluster state ('new' or 'existing').
initial-cluster-state: "new"

# Reject reconfiguration requests that would cause quorum loss.
strict-reconfig-check: false

# Auto compaction retention for mvcc key value store in hour. 0 means disable it.
auto-compaction-retention:

# Force to create a new one member cluster.
force-new-cluster: false

# Accept etcd V2 client requests
enable-v2: true

# Valid values include 'on', 'readonly', 'off'
proxy: "off"

# Time (in milliseconds) an endpoint will be held in a failed state.
proxy-failure-wait: 5000

# Time (in milliseconds) of the endpoints refresh interval.
proxy-refresh-interval: 30000

# Time (in milliseconds) for a dial to timeout.
proxy-dial-timeout: 1000

# Time (in milliseconds) for a write to timeout.
proxy-write-timeout: 5000

# Time (in milliseconds) for a read to timeout.
proxy-read-timeout: 0

client-transport-security:
  # Path to the client server TLS cert file.
  cert-file: "/etc/kube-tls/etcd_server.crt"

  # Path to the client server TLS key file.
  key-file: "/etc/kube-tls/etcd_server.key"

  # Enable client cert authentication.
  client-cert-auth: true

  # Path to the client server TLS trusted CA key file.
  trusted-ca-file: "/etc/kube-tls/etcd_server.crt"

peer-transport-security:
  # Path to the peer server TLS cert file.
  cert-file: "/etc/kube-tls/etcd_peer.crt"

  # Path to the peer server TLS key file.
  key-file: "/etc/kube-tls/etcd_peer.key"

  # Enable peer client cert authentication.
  client-cert-auth: true

  # Path to the peer server TLS trusted CA key file.
  trusted-ca-file: "/etc/kube-tls/etcd_peer.crt"

# Enable debug-level logging for etcd.
debug: false

= 
cat etcd/etcd.conf
# Managed by Puppet

#[member]
ETCD_NAME="etcdserver"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_SNAPSHOT_COUNT=10000
ETCD_HEARTBEAT_INTERVAL=100
ETCD_ELECTION_TIMEOUT=1000
ETCD_QUOTA_BACKEND_BYTES=0
ETCD_LISTEN_CLIENT_URLS="https://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="https://etcdserver.boogie.net:2379"
ETCD_MAX_SNAPSHOTS=5
ETCD_MAX_WALS=5
ETCD_ENABLE_V2=true
#
#[proxy]
ETCD_PROXY="off"
ETCD_PROXY_FAILURE_WAIT=5000
ETCD_PROXY_REFRESH_INTERVAL=30000
ETCD_PROXY_DIAL_TIMEOUT=1000
ETCD_PROXY_WRITE_TIMEOUT=5000
ETCD_PROXY_READ_TIMEOUT=0
#

#[cluster]
ETCD_LISTEN_PEER_URLS="https://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://etcdserver.boogie.net:2380"
ETCD_INITIAL_CLUSTER="etcdserver=https://etcdserver.boogie.net:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-paas-kubeinfra"
ETCD_DISCOVERY_FALLBACK="proxy"
ETCD_STRICT_RECONFIG_CHECK=false
ETCD_FORCE_NEW_CLUSTER=true
#

#[security]
ETCD_CERT_FILE="/etc/kube-tls/etcd_server.crt"
ETCD_KEY_FILE="/etc/kube-tls/etcd_server.key"
ETCD_CLIENT_CERT_AUTH=true
ETCD_TRUSTED_CA_FILE="/etc/kube-tls/etcd_ca.crt"
ETCD_PEER_CERT_FILE="/etc/kube-tls/etcd_peer.crt"
ETCD_PEER_KEY_FILE="/etc/kube-tls/etcd_peer.key"
ETCD_PEER_CLIENT_CERT_AUTH=true
ETCD_PEER_TRUSTED_CA_FILE="/etc/kube-tls/etcd_ca.crt"
#
#[logging]
ETCD_DEBUG=true
# 

=== kube backup : ===
= Backup and restore =

que doit on backuper ?

les configurations de nos ressources

on peut stocker tous nos fichiers de définition d'objects dans un repertoire dédié.
il nous faut donc une copie de sauvegarde de ces fichiers. --> on va habituellement les stocker dans un cvs : git / gitlab /github.
on peut pour s'assurer que toutes les ressources sont bien backupées intérroger l'apiserver et stocker l'intégralité de nos ressources dans un fichier de backup.

kubectl get all --all-namespaces -o yaml > all-deployed-services.yaml

certains outils dédiés existent.

Etcd stocke les états du cluster : les infos du cluster y sont stockées.

On va pouvoir backuper les data etcd

>> on défini le datadir de etcd : l'endroit ou sont stockées les data.

Un outil de snap est fourni avec etcd :

Attention il faut préfixer avec l'api V3 de etcd
ETCDCTL_API=3 etcdctl snapshot save snapshot.db
>>> ici on backup dans le repertoire courant nos data dans un fichier appellé snapshot.db
on peut biensur backupé ou on veut en précisant le path :

ETCDCTL_API=3 etcdctl snapshot save /home/boogie/backup/etcd_bck.db

On peut voir l'état de notre snap :

ETCDCTL_API=3 etcdctl snapshot status snapshot.db

Pour restaurer un backup :

1/ stopper le kubeapiserver : car on va devroir arreter le service etcd

service kube-apiserver stop

2/ restauration du snap sur l'intégralité des nodes du cluster etcd :
ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --data-dir /var/lib/etcd-for_backup \
  --initial-cluster etcd01=https://etcd01.boogie.net:2380,etcd02=https://etcd02.boogie.net:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls https://${internal_ip}:2380

!! /!\ Etcd restore les data depuis le backup et instancie une NOUVELLE configuration de cluster
et configure les membres comme des nouveaux membres d'un nouveau cluster !!
C'est fait par prévention pour empecher un nouveau membre de s'integrer dans un cluster déja existant.
a ce moment la un nouveau datadir est créer dans notre exemple : /var/lib/etcd-from-backup

On va maintenant configurer notre service etcd pour qu'il prenne en compte notre nouveau data dir domain et notre nouveau token

une fois notre service modifier on reload le service daemon et etcd :

systemctl daemon-reload
service etcd restart

3/ on démarre le kubeapiserver service :

service kube-apiserver start

Attention pour le tls on doit indiquer a etcd les endpoint; ca; crt et key



== disaster recovery : ==

= short version http only : =

1 /backup des data sur un node : 

$ ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshot.db


2/ arrêt du service etcd sur tous les nodes du cluster 
systemctl etcd stop


3/ restore du backup savaugarder sur chacun des noeuds :

$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --name m1 \
  --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls http://host1:2380
$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --name m2 \
  --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls http://host2:2380
$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --name m3 \
  --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls http://host3:2380

4/ démarrage etcd en background les nodes vont se synchroniser 

$ etcd \
  --name m1 \
  --listen-client-urls http://host1:2379 \
  --advertise-client-urls http://host1:2379 \
  --listen-peer-urls http://host1:2380 &
$ etcd \
  --name m2 \
  --listen-client-urls http://host2:2379 \
  --advertise-client-urls http://host2:2379 \
  --listen-peer-urls http://host2:2380 &
$ etcd \
  --name m3 \
  --listen-client-urls http://host3:2379 \
  --advertise-client-urls http://host3:2379 \
  --listen-peer-urls http://host3:2380 &


= short version tls =

1/ backup : 
ETCDCTL_API=3 /usr/local/bin/etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/client.key --cert /etc/kubernetes/pki/etcd/client.crt  --endpoints "https://etcd01bv:2379,https://etcd02uv:2379,https://etcd03bv:2379"

2/ arrêt etcd 
systemctl stop etcd

3/ suppression du rep de data à faire sur TOUS les noeuds du cluster 
rm -rf /var/lib/etcd/

4/ restoration du backup avec une configuration fraiche du cluster ( les data sont contenues dans le snap ) à faire sur TOUS les noeuds du cluster : 

ETCDCTL_API=3 /usr/local/bin/etcdctl snapshot restore snapshot.db --name $(hostname) --initial-cluster etcd01bv=https://etcd01bv:2380,etcd02uv=https://etcd02uv:2380,etcd03bv=https://etcd03bv:2380 --initial-cluster-token etcd-cluster-1 --initial-advertise-peer-urls https://$(hostname):2380 --data-dir=/var/lib/etcd

cat /etc/systemd/system/etcd.service

5/ demmarage en cli d'etcd et en background  à faire sur TOUS les noeuds du cluster :

Les noeuds du cluster vont se synchroniser 

/usr/local/bin/etcd --name $(hostname) --listen-client-urls https://$(ip a | grep 10.121 | awk '{print $2}' | cut -d "/" -f1):2379  --advertise-client-urls https://$(ip a | grep 10.121 | awk '{print $2}' | cut -d "/" -f1):2379 --listen-peer-urls https://$(ip a | grep 10.121 | awk '{print $2}' | cut -d "/" -f1):2380 --cert-file=/etc/kubernetes/pki/etcd/server.crt --key-file=/etc/kubernetes/pki/etcd/server.key --client-cert-auth --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-client-cert-auth --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --initial-cluster etcd01bv=https://etcd01bv:2380,etcd02uv=https://etcd02uv:2380,etcd03bv=https://etcd03bv:2380 --data-dir /var/lib/etcd/ &

6/ check des membres du cluster : 

ps fax | grep etcd

watch /usr/local/bin/etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --key-file /etc/kubernetes/pki/etcd/peer.key --cert-file /etc/kubernetes/pki/etcd/peer.crt --endpoints "https://etcd01bv:2379,https://etcd02uv:2379,https://etcd03bv:2379" member list







= principe : =

https://github.com/etcd-io/etcd/tree/master/Documentation/op-guide

etcd is designed to withstand machine failures. An etcd cluster automatically recovers from temporary failures (e.g., machine reboots) and tolerates up to (N-1)/2 permanent failures for a cluster of N members. When a member permanently fails, whether due to hardware failure or disk corruption, it loses access to the cluster. If the cluster permanently loses more than (N-1)/2 members then it disastrously fails, irrevocably losing quorum. Once quorum is lost, the cluster cannot reach consensus and therefore cannot continue accepting updates.

To recover from disastrous failure, etcd v3 provides snapshot and restore facilities to recreate the cluster without v3 key data loss. To recover v2 keys, refer to the v2 admin guide.

Snapshotting the keyspace
Recovering a cluster first needs a snapshot of the keyspace from an etcd member. A snapshot may either be taken from a live member with the etcdctl snapshot save command or by copying the member/snap/db file from an etcd data directory. For example, the following command snapshots the keyspace served by $ENDPOINT to the file snapshot.db:

$ ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshot.db
Restoring a cluster
To restore a cluster, all that is needed is a single snapshot "db" file. A cluster restore with etcdctl snapshot restore creates new etcd data directories; all members should restore using the same snapshot. Restoring overwrites some snapshot metadata (specifically, the member ID and cluster ID); the member loses its former identity. This metadata overwrite prevents the new member from inadvertently joining an existing cluster. Therefore in order to start a cluster from a snapshot, the restore must start a new logical cluster.

Snapshot integrity may be optionally verified at restore time. If the snapshot is taken with etcdctl snapshot save, it will have an integrity hash that is checked by etcdctl snapshot restore. If the snapshot is copied from the data directory, there is no integrity hash and it will only restore by using --skip-hash-check.

A restore initializes a new member of a new cluster, with a fresh cluster configuration using etcd's cluster configuration flags, but preserves the contents of the etcd keyspace. Continuing from the previous example, the following creates new etcd data directories (m1.etcd, m2.etcd, m3.etcd) for a three member cluster:

$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --name m1 \
  --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls http://host1:2380
$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --name m2 \
  --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls http://host2:2380
$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --name m3 \
  --initial-cluster m1=http://host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls http://host3:2380
Next, start etcd with the new data directories:

$ etcd \
  --name m1 \
  --listen-client-urls http://host1:2379 \
  --advertise-client-urls http://host1:2379 \
  --listen-peer-urls http://host1:2380 &
$ etcd \
  --name m2 \
  --listen-client-urls http://host2:2379 \
  --advertise-client-urls http://host2:2379 \
  --listen-peer-urls http://host2:2380 &
$ etcd \
  --name m3 \
  --listen-client-urls http://host3:2379 \
  --advertise-client-urls http://host3:2379 \
  --listen-peer-urls http://host3:2380 &
Now the restored etcd cluster should be available and serving the keyspace given by the snapshot.

Restoring a cluster from membership mis-reconfiguration with wrong URLs
Previously, etcd panics on membership mis-reconfiguration with wrong URLs (v3.2.15 or later returns error early in client-side before etcd server panic).

Recommended way is restore from snapshot. --force-new-cluster can be used to overwrite cluster membership while keeping existing application data, but is strongly discouraged because it will panic if other members from previous cluster are still alive. Make sure to save snapshot periodically.




= volumes : 

- concepts :
on va d'abord se focus sur la notion de volumes dans docker : de base les données ne sont pas persistantes dans docker : une suppression de container implique naturellement une suppression de data : qui sont donc de base volatiles.
pour assurer de la persistance à nos data on va attacher un volume que l'on aura créer au prealable à notre container : même si notre container est détruit pas de souci les data sont présentes et un nouveau container peut les exploiter.

Le  principe est le même dans kubernetes : les pods par nature sont éphémère et perdent donc les data quand ils sont détruits.
On va donc attacher un volume dans  un pod : si celui ci est détruit pas de souci un nouveau pod pourrra accéder au data du volume en le montant.


L’utilisation d’un volume persistant se fait au sein de la déclaration d’un pod. Une première solution pourrait être de passer par un service externe comme avec un point de montage NFS.
Cette déclaration de persistance de données se définira en deux parties : 
-> un référencement au niveau du pod dans le champ volumes ,
-> une indication de l’emplacement du montage au sein du container.
Le référencement d’un volume NFS au niveau d’un pod se présentera sous la forme suivante :
spec:
  volumes:
    - name: nfs
      nfs:
      # URL for the NFS server
        server: 192.168.0.1
        path: /
Le montage au niveau du container se présentera ainsi :
spec:
  containers:
   - name: mailhog
     image: mailhog/mailhog
     # Mount the NFS volume in the container
     volumeMounts:
       - name: nfs
         mountPath: /maildir

- Implémentation :

ex: on va créer un pod qui va écrire dans le fichier d'un repertoire un nombre aléatoire :
on va définir un volume qui va utiliser une méthode de stockage ( drivers nécéssaire à l'utilisation du stockage)
ici on va monter un volume local à notre host et specifier que le stockage sera dans un repertoire local à notre systeme.
On va monter ce volume dans notre container en définissant un point de montage et le volume dédié crée sur notre host.

apiVersion: v1
kind: Pod
metadata:
  name : random-number
spec:

  containers:
  - name: alpine
    image: alpine
    command: ["/bin/sh", "-c"]
    args: ["shuff -i 0-100 n -1 >> /opt/numb.out";]
    volumeMounts:                   <<<<<   definition du montage au sein de notre container
      mountPath: /opt               <<<<<   point de montage sur lequel on stockera / accedera au data depuis le container
      name: data-volume             <<<<    nom du volume utilisé
  volume:                           <<<<< def de notre volume
  - name: data-volume
    hostPath:                       <<<< definition d'un volume local à notre host
      path: /data                   <<<< point de montage de notre host
      type: Directory               <<<< type de montage : ici c'est un repoertoire simple.



quand on aura ecrit un nombre aléatoire via la commande de notre pod : le fichier généré sera accessible au sein du pod dans /opt ..et forcement en local sur le host dans /data ..la suppression du pod n'affectera pas la persistance  de données.
- Options de volumes :

Si le montage d'un volume local est interessant pour un pod sur un node ceci est absolument déconseillé sur un cluster de plusieurs nodes : les pods de chacun des nodes auront un volumes /opt propre a repoertoire /data de chaque node  : ils seront donc potentiellement différents : ce qui génere de l'inconsistance  de données ...

NB: On utilisera pas l'option :"hostPath" pour un systeme de production.

Dans le cas d'un cluster de plusieurs nodes ont va devoir utiliser des solutions de stockage repliquées ( de nombreuses solutions existent : glusterfs, ceph,
nfs, google ...)
Pour renseigner un volume avec un systeme externe on va simplement renseigner le type dans notre déclaration.
ex avec un volume amazon :

apiVersion: v1
kind: Pod
metadata:
  name : random-number
spec:

  containers:
  - name: alpine
    image: alpine
    command: ["/bin/sh", "-c"]

   args: ["shuff -i 0-100 n -1 >> /opt/numb.out";]
    volumeMounts:
      mountPath: /opt
      name: data-volume
  volume:
  - name: data-volume
    awsElasticBlocStore:
      volumeID: < volume-id >
      fsType: ext4




= persistent volume : =


on a vu que la configuration d'un volume implique une définition dans chaque pod. Si nous avons plusieurs users chacun devra configurer le volume dans sa definition de pod ..et la modifier en cas de changement .

On va vouloir pouvoir gérer le volume de maniere plus centralisée. l'admin va créer un pool de storage qui seront a dispo pour les users qui donc pourront selectionner un volume pour le deploiment de leur appli .
cest ce qu'on appelle un "persistant volume claims" : PVC
ex :

pvc-definition.yaml

apiVersion: v1
type: PersistantVolume
metadata:
  - name : pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi
  awsElasticBlocStore:
    volumeID: < volume-id >
    fsType: ext4

kubectl create -f pvc-definition.yaml

pour examiner nos pvc :

kubectl get persistentvolume

= persistant volume claims : =

on a vu comment declarer un pvc maintenant on va voir comment mettre a dispo se volume claim pour un node.

nous avons deux type d'objects : un pv : persistant volume que l'admin met a dispo et un pvc que le user crée pour utiliser le pv
une fois que les objects sont crées kubernetes va binder les objects pv et pvc entre eux en fonction des properties definies dans les volumes.

Chaque pvc est bindé sur un pv . pendant l'operation de montage kubernetes va chercher un pv ayant la volumétrie, les modes d'access, le type de storage définis dans le pvc.
Si plusieurs volumes sont candidats : on pourra toujours sélectionner un pv particulier en utilisant les labels et selectors.

On pourra donc avoir un gros volume de monté sur un petit claim s'il n'y a pas d'autre meilleure option
Il y a une relation 1/1 entre les claims et les volumes.
Si aucun volume n'est dispo alors le volume claim restera en etat pending jusqu'a ce qu'un nouveau volume soit dispo : dans ce cas le montage sera automatique.


- declaration et creation de notre object :
pvc-definition.yaml

apiVersion: v1
type: PersistantVolumeClaim
metadata:
  - name: my_claim
spec:
  accessModes:
    - ReadWriteOnce
  ressources:
    requests:
      storage: 500Mi

kubectl create pvc-definition.yaml

on va pouvoir examiner nos pvc avec :

kubectl get persistantvolumeclaim
Quand on va examiner le pvc et le volume precedement crée on va  voir que les options matchent :

pvc-definition.yaml

apiVersion: v1
type: PersistantVolume
metadata:
  - name : pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi
  awsElasticBlocStore:
    volumeID: < volume-id >
    fsType: ext4

on va maintenant pouvoir voir que le pvc a bien ete bindé sur notre pv

kubectl get persistantvolumeclaim

NAME    STATUS  VOLUME  CAPACITY ACCESS MODES STORAGECLASS AGE
myclaim Bound   pv-vol1  1GBi    RWO                       3mnts

- pour supprimer un pvc :

kubectl delete persistantvolumeclaim myclaim

Nous avons plusieurs options pour gérer le volume.
De maniere native le volume est en mode :
persistantVolumeReclaimPolicy:  Retain

--> le volume sera conservé jusqu'à ce qu'il soit détruit manuellement par l'admin.
Le volume ne sera pas réutilisable pour d'autres claims.

On peut sinon decider de supprimer le volume :
persistantVolumeReclaimPolicy:  Delete
dans ce cas des que le claim est supprimé le volume est egalement supprimé.

On peut egalement "wipper" les data a la suppression du claim :

persistantVolumeReclaimPolicy:  Recycle

ex : pratique : 

on va utiliser les volumes pour stocker des données en dehors des containers : puisqu'un container qui s'arrête perd ses données.
Les volumes persistents de kube vont permettre d'attacher un volume a un container même si celui-ci s'arrête dans ce cas le volume contenant les data pourra être rattaché à un nouveau container.
Les volumes peuvent être attachés via des volumes plugins (local, aws, gcp, ceph, nfs ...)
En utilisant des volumes on peut deployer des applications statefull : ces applis doivent pouvoir lire et ecrire sur le filesystem local qui doit être persistant dans le temps.
On peut donc faire tourner un mysql en utilisant un stockage persistent . Attention la gestion des volumes est encore récente dans kube.

1/ 
On va d'abord devoir créer un volume (on choisi en fonction du plugin manager que l'on veut utiliser) 
ex : 
## Create Volume in AWS

```
aws ec2 create-volume --size 10 --region your-region --availability-zone your-zone --volume-type gp2 --tag-specifications 'ResourceType=volume, Tags=[{Key= KubernetesCluster, Value=kubernetes.domain.tld}]'

2/ on va créer notre déploiment en précisant le point de montage du volume, on précise le volume , le plugin utilsié et l'id du volume (dans notre cas c'est de l'aws) :

boogie$ cat volumes/helloworld-with-volume.yml                                                  [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
        volumeMounts:
        - mountPath: /myvol
          name: myvolume
      volumes:
      - name: myvolume
        awsElasticBlockStore:
          volumeID: # insert AWS EBS volumeID here


- volume provisionning 
on va pouvoir créer nos volumes en fonction de nos volumes plugins avant de les allouer aux pods concernés.

on va declarer un volume :
ex: 

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  zone: us-east-1


on va maintenant associer ce volume a la physical volume claim : requete / demande de volume physique nécéssaire à notre pod :

# pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
  namespace: test
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10

      
cat first-app/helloworld.yml                                                            [☸ minikube:default]
---
apiVersion: v1
kind: Pod
metadata:
  name: nodehelloworld.example.com
  labels:
    app: myapp
spec:
  containers:
  - name: k8s-demo
    image: wardviaene/k8s-demo
    ports:
    - name: nodejs-port
      containerPort: 3000
    volumeMount:
    - mountPath: "/var/www/html"
      name: mypvc
  volumes:
  - name: mypvc
    persistentVolumeClaim:
      claimName: myclaim   <<<<< on reference ici le nom du volume claim qu'on a déclarer dans le PersistentVolumeClaim
  nodeSelector:
    environment: lab
      


      

== rook notes : ==


https://itnext.io/deploy-a-ceph-cluster-on-kubernetes-with-rook-d75a20c3f5b1
https://github.com/rook/rook
https://medium.com/devopsturkiye/rook-a-storage-orchestrator-to-run-stateful-workloads-on-kubernetes-with-ceph-500882ecf005
https://medium.com/cloudops/the-ultimate-rook-and-ceph-survival-guide-eff198a5764a



git clone https://github.com/rook/rook.git
# creation de crds, services accounts, clusterroles ..
kubectl create -f rook/cluster/examples/kubernetes/ceph/common.yaml
# creation du rook-ceph operator et configmap :
kubectl create -f rook/cluster/examples/kubernetes/ceph/operator.yaml
# check 
kubectl get all -n rook-ceph
# un daemonset rook-discover  examine en continu l'etat de chaque node et integre chaque nouveau disque et partition  au cluster ceph
# creation du cluster ceph . tous les device raw vont être ajoutés au cluster :
kubectl create -f rook/cluster/examples/kubernetes/ceph/cluster.yaml
# check 
kubectl get all -n rook-ceph
# tests - acces au dashboard ceph 
kubectl port-forward service/rook-ceph-mgr-dashboard 8443:8443 -n rook-ceph
accessible : 
https://localhost:8443
# login admin / mdp identifiable avec :
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o yaml | grep "password:" | awk '{print2}' | base64 --decode
# acces au cluser ceph en cli : un container contenant les outils de diag ceph peut être utilisé
kubectl create -f rook/cluster/examples/kubernetes/ceph/toolbox.yaml
# on se connecte au pod :
kubectl exec \
  -n rook-ceph \
  -it $(kubectl get po -n rook-ceph |egrep rook-ceph-tools |awk '{print $1}') \
  -- /bin/bash

[root@rook-ceph-tools-7f96779fb9-42rv8 /]# ceph status
  cluster:
    id:     0dccd01a-6f7e-401b-ba66-18eec5b5e26a
    health: HEALTH_WARN
            clock skew detected on mon.b, mon.c

  services:
    mon: 3 daemons, quorum a,b,c (age 38m)
    mgr: a(active, since 38m)
    osd: 5 osds: 5 up (since 38m), 5 in (since 38m)

  data:
    pools:   2 pools, 33 pgs
    objects: 258 objects, 805 MiB
    usage:   7.4 GiB used, 143 GiB / 150 GiB avail
    pgs:     33 active+clean

[root@rook-ceph-tools-7f96779fb9-42rv8 /]# ceph osd status
ID  HOST                         USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE
 0  node-02.lapin.io  1215M  9020M      0        0       0        0   exists,up
 1  node-03.lapin.io  1182M  9053M      0        0       0        0   exists,up
 2  node-04.lapin.io  1825M  8410M      0        0       0        0   exists,up
 3  node-02.lapin.io  1637M  58.3G      0        0       0        0   exists,up
 4  node-03.lapin.io  1670M  58.3G      0        0       0        0   exists,up

[root@rook-ceph-tools-7f96779fb9-8m76b /]# ceph mon dump
dumped monmap epoch 3
epoch 3
fsid 0dccd01a-6f7e-401b-ba66-18eec5b5e26a
last_changed 2020-06-24T08:17:55.904208+0000
created 2020-06-24T08:17:38.057070+0000
min_mon_release 15 (octopus)
0: [v2:10.88.169.104:3300/0,v1:10.88.169.104:6789/0] mon.a
1: [v2:10.81.64.225:3300/0,v1:10.81.64.225:6789/0] mon.b
2: [v2:10.87.229.123:3300/0,v1:10.87.229.123:6789/0] mon.c

 
 [root@rook-ceph-tools-7f96779fb9-8m76b /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                            STATUS  REWEIGHT  PRI-AFF
-1         0.14658  root default
-5         0.06839      host node-02-lapin-io
 0    hdd  0.00980          osd.0                          down         0  1.00000
 3    hdd  0.05859          osd.3                          down         0  1.00000
-7         0.06839      host node-03-lapin-io
 1    hdd  0.00980          osd.1                            up   1.00000  1.00000
 4    hdd  0.05859          osd.4                            up   1.00000  1.00000
-3         0.00980      host node-04-lapin-io
 2    hdd  0.00980          osd.2                            up   1.00000  1.00000


[root@rook-ceph-tools-7f96779fb9-8m76b /]# ceph osd lspools 
1 device_health_metrics
2 replicapool



# creation de la storage class et du replica pool ceph :
kubectl apply -f rook/cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml
# check :
 kubectl get storageclasses.storage.k8s.io                                            [☸ |kubernetes-admin@sandbox:rook-ceph]
NAME              PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
rook-ceph-block   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   24m
# provisionning automatique : on va maintenant pouvoir provisionner automatiquement nos volumes pour les applis statefull.ex :on deploit un wordpress et un mysql . On va remplacer pour nos besoins de test le service Loadbalancer en cluster ip : les ressources ne seront accessibles que depuis le cluster :
sed -i "s#  type: LoadBalancer#  type: ClusterIP#g" rook/cluster/examples/kubernetes/wordpress.yaml
kubectl create -f rook/cluster/examples/kubernetes/mysql.yaml
kubectl create -f rook/cluster/examples/kubernetes/wordpress.yaml
# check :
kubectl get pv,pvc                                                                   [☸ |kubernetes-admin@sandbox:rook-ceph]
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                      STORAGECLASS      REASON   AGE
persistentvolume/pvc-5e3beb06-7d33-4be0-bc25-1591d7bec875   20Gi       RWO            Delete           Bound    rook-ceph/mysql-pv-claim   rook-ceph-block            24m
persistentvolume/pvc-96580901-09c0-4108-a2f3-5b1b195928e6   20Gi       RWO            Delete           Bound    rook-ceph/wp-pv-claim      rook-ceph-block            24m

NAME                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
persistentvolumeclaim/mysql-pv-claim   Bound    pvc-5e3beb06-7d33-4be0-bc25-1591d7bec875   20Gi       RWO            rook-ceph-block   24m
persistentvolumeclaim/wp-pv-claim      Bound    pvc-96580901-09c0-4108-a2f3-5b1b195928e6   20Gi       RWO            rook-ceph-block   24m
# test et acces depuis un node du cluster ( ayant kubectl )  : on a donc bien acces à notre appli stateless qui a son storage provisionner en auto , et distribuer sur un cluster ceph orchestré par rook :

Last login: Wed Jun 24 10:41:28 2020 from 10.201.14.111
[root@fso-master-01 ~]# curl -L -s  http://$(kubectl get svc wordpress -n rook-ceph -o jsonpath='{.spec.clusterIP}')
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
	<meta name="viewport" content="width=device-width" />
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="robots" content="noindex,nofollow" />
	<title>WordPress &rsaquo; Installation</title>
	<link rel='stylesheet' id='buttons-css'  href='http://10.94.64.54/wp-includes/css/buttons.min.css?ver=4.6.1' type='text/css' media='all' />
<link rel='stylesheet' id='install-css'  href='http://10.94.64.54/wp-admin/css/install.min.css?ver=4.6.1' type='text/css' media='all' />
<link rel='stylesheet' id='dashicons-css'  href='http://10.94.64.54/wp-includes/css/dashicons.min.css?ver=4.6.1' type='text/css' media='all' />
</head>


=== config rook-ceph : ===

on va pouvoir definir les devices que l'on veur integrer dans notre cluster ceph 
ex : on a trois vms chacune avec 2 hdd : 1 hdd system 1 hdd data : on ne va pas vouloir integrer l'espace libre / restant du hdd system (s'il y en a ) dans notre cluster ceph.
pour cela dans notre conf de cluster on va pouvoir definir une regex :

storage:
  useAllNodes: true
  useAllDevices: false     <<< on defini ici le fait de ne pas intégrer tous les devices / espaces libre présent sur nos noeuds ( on preserve l'espace du hdd system )
  deviceFilter: sd[b-z]    <<< exemple de conf ou on va specifier de pouvoir recup les devices portant le nom sd[b ..z] 



== set up : 

- install de l'operateur rook via helm : 
helm repo add rook-stable https://charts.rook.io/stable

helm install --name rook --namespace rook-ceph rook-stable/rook-ceph

> ko a troubleshooter ( pb object kube deprecated en v1.18.4 )




conf de cluster ceph fonctionnelle : avec 3 serveurs ayant chacun un hdd dédié à ceph :


cat rook/cluster/examples/kubernetes/ceph/cluster.yaml                                                                 [☸ |kubernetes-admin@sandbox:monitoring]
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/ceph:v15.2.3
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    modules:
    - name: pg_autoscaler
      enabled: true
  dashboard:
    enabled: true
    ssl: true
  monitoring:
    enabled: false
    rulesNamespace: rook-ceph
  network:
  crashCollector:
    disable: false
  cleanupPolicy:
    confirmation: ""
  annotations:
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
#    mgr:
#      limits:
#        cpu: "500m"
#        memory: "1024Mi"
#      requests:
#        cpu: "500m"
#        memory: "1024Mi"
  removeOSDsIfOutAndSafeToRemove: false
  storage: # cluster level storage configuration and selection
    useAllNodes: true
    useAllDevices: false <<<<<<<<<<<<<< ici on set le param disant a ceph de ne pas utiliser tout l'espace dispo et visible sur les noeuds (on preserve le hdd du systeme par ex de l'integration à ceph)
    deviceFilter: sdb    <<<<<<<<<<<<<< on defini ici que seul le device sdb sera utilisé comme disk data ceph - osd
    config:
  disruptionManagement:
    managePodBudgets: false
    osdMaintenanceTimeout: 30
    manageMachineDisruptionBudgets: false
    machineDisruptionBudgetNamespace: openshift-machine-api


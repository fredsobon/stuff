== notes kustomized : ==

intro : 
https://blog.scottlowe.org/2019/09/13/an-introduction-to-kustomize/
https://github.com/kubernetes-sigs/kustomize/blob/master/docs/fields.md

base exemple : 
https://levelup.gitconnected.com/kubernetes-change-base-yaml-config-for-different-environments-prod-test-6224bfb6cdd6

complet + exemples :
https://blog.stack-labs.com/code/kustomize-101/

reference : 
https://github.com/kubernetes-sigs/kustomize


=  customisation de manifests pour utilisation dans plusieurs environnements avec Kustomize  ==

https://cloud.ibm.com/docs/containers?topic=containers-app&locale=fr

Dans le cadre d'une application vous souhaitez maintenir une parité développement et production en configurant un pipeline de développement et de distribution en continu qui utilise une source de codebase courante sous contrôle de version. 
Dans vos référentiels de codebase, vous stockez vos fichiers manifeste de configuration de ressource Kubernetes, souvent au format YAML. Vous pouvez utiliser le projet Kubernetes Kustomize pour standardiser et personnaliser vos déploiements sur plusieurs environnements.

Par exemple, vous pouvez configurer un fichier YAML kustomization de base pour déclarer des objets Kubernetes, tels que des déploiements et des PVC qui sont partagés dans vos environnements de développement, de test et de production. Ensuite, vous pouvez configurer des fichiers YAML kustomization distincts qui ont des configurations personnalisées pour chaque environnement, par exemple, davantage de réplicas en environnement de production qu'en environnement de test. 
Ces fichiers YAML personnalisés peuvent ensuite chevaucher ou être utilisés pour générer le fichier YAML de base partagé de manière à vous permettre de gérer des environnements qui sont pour la plupart identiques, à l'exception de quelques différences de configuration de chevauchement dont vous contrôlez la source. 
Pour plus d'informations sur Kustomize, par exemple, un glossaire ou une foire aux questions, voir la documentation Kustomize: 
https://github.com/kubernetes-sigs/kustomize/tree/master/docs

-> Avant de commencer :

Créez ou mettez à jour un cluster qui exécute Kubernetes version 1.14 ou ultérieure.
Assurez-vous que votre version kubectl correspond à la version de votre cluster.
Pour configurer des fichiers de configuration avec Kustomize :

Installez l'outil kustomize.
Pour macOS, vous pouvez utiliser le gestionnaire de package brew.
brew install kustomize

- Créez un répertoire pour votre application dans un système de contrôle de version, tel que Git.

git init ~/<my_app>
Créez la structure de vos référentiels pour vos répertoires kustomize base, overlay et les répertoires d'environnement de préproduction (staging) et de production (prod), par exemple. Dans les étapes suivantes, vous configurez ces référentiels pour une utilisation avec kustomize :


mkdir -p ~/<my_app>/base &&
mkdir -p ~/<my_app>/overlay &&
mkdir -p ~/<my_app>/overlay/staging &&
mkdir -p ~/<my_app>/overlay/prod

Exemple de structure de référentiels :

.
├── base
└── overlay
    ├── prod
    └── staging

-> Configurez le référentiel base.

Accédez au référentiel base.

cd ~/<my_app>/base

Créez un ensemble initial de fichiers YAML de configuration Kubernetes pour votre déploiement d'application. Vous pouvez utiliser l'exemple de fichier YAML wasliberty pour créer un déploiement, un service, un objet ConfigMap et une réservation de volume persistant.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: wasliberty
spec:
  replicas: 3
  selector:
    matchLabels:
      app: wasliberty
  template:
    metadata:
      labels:
        app: wasliberty
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - wasliberty
              topologyKey: kubernetes.io/hostname
      containers:
      - name: wasliberty
        image: icr.io/ibm/liberty:latest
        env:
          - name: VERSION
            valueFrom:
              configMapKeyRef:
                name: wasliberty
                key: VERSION
          - name: LANGUAGE
            valueFrom:
              configMapKeyRef:
                name: wasliberty
                key: LANGUAGE
          - name: username
            valueFrom:
              secretKeyRef:
                name: wasliberty
                key: username
          - name: password
            valueFrom:
              secretKeyRef:
                name: wasliberty
                key: password
        ports:
          - containerPort: 9080
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1024Mi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /
            port: 9080
          initialDelaySeconds: 300
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /
            port: 9080
          initialDelaySeconds: 45
          periodSeconds: 5
        volumeMounts:
        - name: pvmount
          mountPath: /test
      volumes:
      - name: pvmount
        persistentVolumeClaim:
          claimName: wasliberty
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: wasliberty
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: wasliberty
---
apiVersion: v1
kind: Service
metadata:
  name: wasliberty
  labels:
    app: wasliberty
spec:
  ports:
  - port: 9080
  selector:
    app: wasliberty
  type: NodePort
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: wasliberty
  labels:
    app: wasliberty
data:
  VERSION: "1.0"
  LANGUAGE: en
---
apiVersion: v1
kind: Secret
metadata:
  name: wasliberty
  labels:
    app: wasliberty
type: Opaque
data:
  username: dXNlcm5hbWU=
  password: cGFzc3dvcmQ=
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wasliberty
  annotations:
    volume.beta.kubernetes.io/storage-class: "ibmc-file-bronze"
  labels:
    billingType: "hourly"
    app: wasliberty
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 24Gi


Créez un fichier kustomization qui spécifie la configuration de base à appliquer aux différents environnements. 
Le fichier kustomization doit inclure la liste de fichiers YAML de configuration de ressource Kubernetes qui sont stockés dans le même référentiel base. 
Dans le fichier kustomization, vous pouvez également ajouter des configurations qui s'appliquent à tous les fichiers YAML de ressource dans le référentiel de base, par exemple, un préfixe ou un suffixe qui est ajouté à tous les noms de ressource, un libellé, l'espace de nom existant dans lequel les ressources sont créées, des secrets, des objets ConfigMap, etc.


apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: wasliberty
namePrefix: kustomtest-
nameSuffix: -v2
commonLabels:
  app: kustomized-wasliberty
resources:
- deployment.yaml
- service.yaml
- pvc.yaml
- configmap.yaml
- secret.yaml

Les noms des fichiers YAML resource doivent correspondre aux noms des autres fichiers dans le référentiel base. Vous pouvez inclure plusieurs configurations dans le même fichier, mais dans l'exemple, les configurations sont des fichiers distincts, par exemple, deployment.yaml, service.yaml et pvc.yaml.

Générez vos fichiers YAML resource avec les configurations que vous avez définies dans le fichier YAML de base kustomization. 
Les ressources sont générées en combinant les configurations dans les fichiers YAML kustomization et resource. 
Les fichiers YAML combinés sont renvoyés dans stdout dans la sortie de terminal. Utilisez cette même commande pour générer les modifications ultérieures que vous apportez au fichier YAML kustomization, telles que l'ajout d'un libellé.

kustomize build

-> configuration des environments specifiques : overlays : staging /prod :

Configurez votre référentiel overlay avec des fichiers YAML kustomization uniques pour chacun de vos environnements, par exemple, staging et prod.
Dans le référentiel staging, créez un fichier kustomization.yaml. Ajoutez des configurations uniques à staging, telles qu'un libellé, une balise image ou un fichier YAML pour un nouveau composant que vous souhaitez tester.

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namePrefix: staging-
commonLabels:
  env: staging
  owner: TeamA
bases:
- ../../base
patchesStrategicMerge:
- configmap.yaml
- new_staging_resource.yaml
resources:
- new_staging_resource.yaml

Description des composants du fichier YAML :

-namePrefix	Spécifiez un préfixe à associer au nom de chaque ressource que vous souhaitez créer avec votre fichier kustomization de préproduction, par exemple, staging-.

-commonLabels	Ajoutez des libellés qui sont uniques aux objets de préproduction, par exemple, l'environnement de préproduction et l'équipe responsable.
bases	Ajoutez un chemin relatif à un répertoire ou une URL vers un référentiel distant qui contient un fichier kustomization base. Dans cet exemple, le chemin relatif pointe vers le fichier kustomization base dans le référentiel base que vous avez créé précédemment. Cette zone est obligatoire pour un fichier kustomization overlay.

-patchesStrategicMerge	Répertoriez les fichiers YAML de configuration ressource que vous souhaitez fusionner dans le fichier kustomization base. Vous devez également ajouter ces fichiers au même référentiel que le fichier kustomization, par exemple, overlay/staging. Ces fichiers de configuration resource peuvent contenir de petites modifications qui sont fusionnées avec les fichiers de configuration base de même nom sous forme de correctif. La ressource récupère tous les composants qui se trouvent dans le fichier de configuration base, plus les composants supplémentaires que vous spécifiez dans le fichier de configuration overlay.
Si la configuration est un nouveau fichier qui ne se trouve pas dans la base, vous devez également ajouter le nom de fichier à la zone resources.

-resources Répertoriez les fichiers YAML de configuration resource qui sont uniques dans le référentiel staging et non inclus dans le référentiel base. Ajoutez ces fichiers également dans la zone patchesStrategicMerge et ajoutez-les au même référentiel que le fichier kustomization, par exemple, overlay/staging.

Autres configurations possibles	Pour connaître les autres configurations que vous pouvez ajouter à votre fichier :
https://github.com/kubernetes-sigs/kustomize#1-make-a-kustomization-file


Générez vos fichiers de configuration staging/overlay.

kustomize build overlay/staging

Répétez ces étapes pour créer votre fichier kustomization prod/overlay et d'autres fichiers YAML de configuration. Par exemple, vous pouvez augmenter le nombre de répliques dans votre fichier deployment.yaml de sorte que votre environnement de production puisse gérer davantage de demandes utilisateur.

Passez en revue votre structure de référentiel kustomize pour vous assurer qu'elle contient tous les fichiers de configuration YAML dont vous avez besoin. La structure peut se présenter comme suit :
├── base
│   ├── configmap.yaml
│   ├── deployment.yaml
│   ├── kustomization.yaml
│   ├── pvc.yaml
│   ├── secret.yaml
│   └── service.yaml
└── overlay
    ├── prod
    │   ├── deployment.yaml
    │   ├── kustomization.yaml
    │   └── new_prod_resource.yaml
    └── staging
        ├── configmap.yaml
        ├── kustomization.yaml
        └── new_staging_resource.yaml

Appliquez les ressources Kubernetes pour l'environnement que vous souhaitez déployer. L'exemple ci-après utilise le référentiel staging.
Accédez au répertoire overlay/staging. Si vous n'avez pas créé vos ressources lors de l'étape précédente, créez-les maintenant.

cd overlay/staging && kustomize build

Appliquez les ressources Kubernetes à votre cluster. Ajoutez l'indicateur -k et le répertoire dans lequel se trouve le fichier kustomization. Par exemple, si vous vous trouvez déjà dans le répertoire staging, ajoutez ../staging pour marquer le chemin vers le répertoire.

kubectl apply -k ../staging

Exemple de sortie :
configmap/staging-kustomtest-configmap-v2 created
secret/staging-kustomtest-secret-v2 created
service/staging-kustomtest-service-v2 created
deployment.apps/staging-kustomtest-deployment-v2 created
job.batch/staging-pi created
persistentvolumeclaim/staging-kustomtest-pvc-v2 created

Assurez-vous que les modifications uniques du répertoire staging sont appliquées. Par exemple, si vous avez ajouté un préfixe staging-, les pods et les autres ressources qui sont créés comportent ce préfixe dans leur nom.


kubectl get -k ../staging
Exemple de sortie :

NAME                                        DATA   AGE
configmap/staging-kustomtest-configmap-v2   2      90s

NAME                                  TYPE     DATA   AGE
secret/staging-kustomtest-secret-v2   Opaque   2      90s

NAME                                    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/staging-kustomtest-service-v2   NodePort   172.21.xxx.xxx   <none>        9080:30200/TCP   90s

NAME                                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/staging-kustomtest-deployment-v2   0/3     3            0           91s

NAME                   COMPLETIONS   DURATION   AGE
job.batch/staging-pi   1/1           41s        2m37s

NAME                                              STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS       AGE
persistentvolumeclaim/staging-kustomtest-pvc-v2   Pending                                      ibmc-file-bronze   90s

Répétez ces étapes pour chaque environnement que vous souhaitez créer.


- Facultatif : nettoyez votre environnement en retirant toutes les ressources que vous avez appliquées avec Kustomize.

kubectl delete -k <directory>
Exemple de sortie :
configmap "staging-kustomtest-configmap-v2" deleted
secret "staging-kustomtest-secret-v2" deleted
service "staging-kustomtest-service-v2" deleted
deployment.apps "staging-kustomtest-deployment-v2" deleted
job.batch "staging-pi" deleted
persistentvolumeclaim "staging-kustomtest-pvc-v2" deleted



= patch dans kustomized : =

JSON Patching
A kustomization file supports customizing resources via JSON patches.

on va pouvoir patcher des ressources ( modifier, ajouter des champs dans nos manifests de base ) : les différentes politiques de merge pourront être étudiées à part .


on va pour l'exemple definir un manifest qui sera ensuite patcher pour l'exemple : 

Make a place to work:

DEMO_HOME=$(mktemp -d)
We'll be editting an Ingress object:

cat <<EOF >$DEMO_HOME/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /
        backend:
          serviceName: homepage
          servicePort: 8888
      - path: /api
        backend:
          serviceName: my-api
          servicePort: 7701
      - path: /test
        backend:
          serviceName: hello
          servicePort: 7702
EOF

- patch :

on veut apporter les modifs suivantes à notre manifest :

-> changer le host par foo.bar.io
-> changer le port de '/' de 8888 à 80
-> ajouter une section de path /heatlhz 
insert an entirely new serving path /healthz at a particular point in the paths list, rather than at the end or the beginning.


Voila la patch que nous allons créer : 
Here's the patch file to do that:

cat <<EOF >$DEMO_HOME/ingress_patch.json
[
  {"op": "replace",
   "path": "/spec/rules/0/host",
   "value": "foo.bar.io"},

  {"op": "replace",
   "path": "/spec/rules/0/http/paths/0/backend/servicePort",
   "value": 80},

  {"op": "add",
   "path": "/spec/rules/0/http/paths/1",
   "value": { "path": "/healthz", "backend": {"servicePort":7700} }}
]
EOF


on va biensur ajouter le fichier ingress dans le fichier kustomization :

We'll of course need a kustomization file referring to the Ingress:

cat <<EOF >$DEMO_HOME/kustomization.yaml
resources:
- ingress.yaml
EOF

on va egalement ajouter la section patch qui va pointer sur notre fichier de patch défini auparavant : 

To this same kustomization file, add a patches field refering to the patch file we just made and target it to the Ingress object:

cat <<EOF >>$DEMO_HOME/kustomization.yaml
patches:
- path: ingress_patch.json
  target:
    group: networking.k8s.io
    version: v1beta1
    kind: Ingress
    name: my-ingress
EOF

On voit le fichier modifié désiré : 

Define the expected output:

cat <<EOF >$DEMO_HOME/out_expected.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: foo.bar.io
    http:
      paths:
      - backend:
          serviceName: homepage
          servicePort: 80
        path: /
      - backend:
          servicePort: 7700
        path: /healthz
      - backend:
          serviceName: my-api
          servicePort: 7701
        path: /api
      - backend:
          serviceName: hello
          servicePort: 7702
        path: /test
EOF


on build notre conf : 
Run the build:

kustomize build $DEMO_HOME >$DEMO_HOME/out_actual.yaml

Confirm they match:

diff $DEMO_HOME/out_actual.yaml $DEMO_HOME/out_expected.yaml


/!\ on peut créer notre fichier de patch en yaml ou en json /!\ 

If you prefer YAML to JSON, the patch can be expressed in YAML format (neverthless following JSON patch rules):

cat <<EOF >$DEMO_HOME/ingress_patch.yaml
- op: add
  path: /spec/rules/0/http/paths/-
  value:
    path: '/canada'
    backend:
      serviceName: hoser
      servicePort: 7703
EOF
Now add this to the list of patches in the kustomization file:

cat <<EOF >>$DEMO_HOME/kustomization.yaml
- path: ingress_patch.yaml
  target:
    group: networking.k8s.io
    version: v1beta1
    kind: Ingress
    name: my-ingress
EOF

We expect the following at the end of the output:

cat <<EOF >$DEMO_HOME/out_expected.yaml
      - backend:
          serviceName: hello
          servicePort: 7702
        path: /test
      - backend:
          serviceName: hoser
          servicePort: 7703
        path: /canada
EOF
Try it:

kustomize build $DEMO_HOME | tail -n 8 |\
    diff  $DEMO_HOME/out_expected.yaml -
To see how to apply one JSON patch to many resources, see the multi-patch demo.




==== fonction de patchs : ====

https://skryvets.com/blog/2019/05/15/kubernetes-kustomize-json-patches-6902/

Introduction

Kustomize is a supplement and very useful tool for kuberentes that is responsible for template management. One of the core functionalities is to create overriding rules on top of an existing template without changing the latter. While official documentation provides a great overview of the basic features such as adding namespace, prefixes, annotations, labels, it lacks an explanation of common scenarios where the need to add, remove or replace values of a base template. More precisely, I'll be focusing on JsonPatches6902 and how to create them in, native for kubernetes, yaml format.
What is JsonPatches6902?

Let's start with the number first. There is an RFC6902 standard defines how to apply JSON patches. The original purpose of this standard is to eliminate bandwidth and CPU waste while processing large resources for HTTP requests: instead of sending entire JSON object (resource) via HTTP PUT/POST method, it allows to send only the modified part (patch).

According to the standard, there are 6 types of operations that can be performed on an object: add, remove, replace, move, copy, test. We will take a look at the first three since they have a more applicable sense to our use case. From a syntactical point of view, each operation must have op member indicating one of the mentioned types, e.g. add:

{ "op": "add" }
Copy

In addition to that, operations must include a path key. The value of the path is a location within the JSON document need to be modified and standardized by RFC6901:

{ "op": "add", "path": "/a/b" }
Copy

The other key-value pairs of operation depend on the particular operation being performed.
Example with gitea

After we had a brief overview of the official standard, let's switch to kubernetes and see how this functionality can be used within kustomize.

Sometimes it's best to explain the concept by example and that's what we're going to do. Out test subject will be represented by gitea - self-hosted git service. This docker image is an excellent candidate for the scenario as it does support multiple environment variables and have a front-facing UI interface allows to see the changes applied through the patches.
Project structure

As it's in the case for most of my articles, the full project can be found on GitHub. The structure can be represented in the following diagram:

|-- base
    gitea-pod.yaml
    gitea-service.yaml
    kustomization.yaml
|-- overlays
    |-- add
        ...
        kustomization.yaml
    |-- remove
        ...
        kustomization.yaml
    |-- replace
        ...
        kustomization.yaml
Copy

At the simplest level, the codebase contains gitea pod and gitea service where both represent the base layer. There are two ways of how to deploy this configuration to, let's say, minikube environment:

    If kustomize package is installed: kustomize build ./base | kubectl apply -f -
    if kubernetes version is higher than 1.14: kubectl apply -k ./base

The base might not very useful in the beginning since the service is only reachable within kubernetes environment. In the next section, we will see how patches can help to expose the service so we can see a nice UI in the browser.

Going further through the overview, we have "overlays" that modify a common base. Each overlay is represented by a specific operation such as add, remove or replace which customizes and applies patches on top of the base while leaving the latter untouched. Later we will take a look into details of each operation.

To remove deployed configuration at any point of time run:

kubectl delete pod,svc --selector=app=gitea
Copy

Kustomization file overview

kustomize can work properly only when kustomization.yaml is present in the target folder. Here's an example from the codebase:

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
bases:
- ../../base
patchesJson6902:
- target:
    version: v1 # apiVersion
    kind: Pod
    name: gitea
  path: gitea-pod-patch-image-version.yaml
Copy

As you may notice, each key under the target uses exactly the same value from the resource it's being applied to.

However, for some type of kubernetes primitives with the prefix before the slash (which called a "group") in apiVersion field it's required to specify additional property called group. Here's an example of StatefulSet:

#...
patchesJson6902:
- target:
    group: "apps"
    version: v1 # apiVersion
    kind: StatefulSet
    name: example-statefulset
  path: my-statefulset-patch.yaml
Copy

The full list of such objects and groups can be found in the article: Which Kubernetes apiVersion Should I Use?
Operations
Add

To apply this configuration, run:

kubectl apply -k ./overlays/add
Copy

When kubernetes finishes to prepare the environment you should be able to see gitea homepage by navigating to http://<minikube-url>:30000 (usually, but not always, minikube IP is 192.168.99.100). Thanks to one of the applied patches which turned regular kuberetes service into a NodePort service, it became possible to reach the web-page from the browser. However, there are different types of add operations and let's look at them at a closer level.
Add a member to an object

The patch illustrating this functionality is gitea-pod-http-app-name-patch.yaml. It changes the default gitea website title to the value of APP_NAME environment variable:

- op: add
  path: "/spec/containers/0/env"
  value:
    - name: APP_NAME
      value: ABC Inc. Private Git Repository
Copy

The path field specifies a place in an object where to insert the required property:

#...
spec:
  containers:
  - name: gitea
    image: gitea/gitea:1.8
#...
Copy

Since containers is an array, we need to specify the item of the array needed to be updated. We only have one item and that's why the number is 0. Alternatively, we can indicate an exact index of an element or use - which represents the last element of the array.

Patches applied to a service, work in exactly the same way:

- op: add
  path: "/spec/type"
  value: "NodePort"
- op: add
  path: "/spec/ports/0/nodePort"
  value: 30000
Copy

By turning simple gitea-service.yaml:

kind: Service
apiVersion: v1
metadata:
  name: gitea-service
  labels:
    app: gitea
spec:
  selector:
    app: gitea
  ports:
  - name: ui-port
    port: 3000
Copy

into NodePort:

apiVersion: v1
kind: Service
metadata:
  labels:
    app: gitea
  name: gitea-service
spec:
  type: NodePort
  selector:
    app: gitea
  ports:
  - name: ui-port
    nodePort: 30000
    port: 3000
Copy

Append to a List

The gitea-pod-patch-sidecar-container.yaml patch appends sidecar container to the containers list in the pod:

- op: add
  path: "/spec/containers/-"
  value:
    name: sidecar
    image: busybox
    args:
    - sleep
    - "3600"
Copy

Which results in the following output:

#...
containers:
  - name: gitea
    image: gitea/gitea:1.8
    env:
    - name: APP_NAME
      value: ABC Inc. Private Git Repository
    ports:
    - containerPort: 3000
  - name: sidecar
    image: busybox
      args:
    - sleep
    - "3600"
#...
Copy

The sidecar container is not very useful in this particular example, but it demonstrates very well how to append an object to a list.
Replace

Replace allows changing a value on specified path. This particular patch changes the image version of the gitea. To apply the patch:

kubectl apply -k ./overlays/replace
Copy

gitea-service-patch-remove-port-name.yaml:

- op: replace
  path: "/spec/containers/0/image"
  value: "gitea/gitea:1.7"
Copy

Produced result:

#...
containers:
  - name: gitea
    image: gitea/gitea:1.7 # <-- Replaced version
Copy

Remove

As a name suggests, removes a key on a specified path. To apply the configuration, run

kubectl apply -k ./overlays/remove
Copy

gitea-service-patch-remove-port-name.yaml:

- op: remove
  path: "/spec/ports/0/name"
Copy

This example removes name key from the ports item within a service:

spec:
  selector:
    app: gitea
  ports:
  - port: 3000
      # name: ui-port <-- removed

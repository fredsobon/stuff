==== notes upgrade cluster kube : ====


kubeadm / kubelet / kubeconfig >> on set up le pkgs correspondants à la version kube que l'on veut installer sur notre cluster .


1/ on installe sur notre premier controller la version de kubeadm correspondant à la version kube cible 

apt / yum install kubeadmx.xx

2/ 
on drain le premier controller : 
kubectl drain controller01 --ignore-daemonsets

3/ on examine le plan de migration :
kubeadm upgrade plan


4/ on lance l'upgrade de la version :

kubeadm upgrade apply v1.xx.x

5/ on remet en prod le premier controller 

kubectl uncordon controller01

6/ on install kubeadm sur les autres controller

7/ on drain notre second controller : 

kubectl drain controller02 --ignore-daemonsets

8/ on upgrade le deuxieme controller :

kubeadm upgrade node

9/ on remet en prod le deuxieme controller 

kubectl uncordon  controller02


10 / on repete les opérations sur les autres controllers :

apt / yum install kubeadm
kubectl drain controllerxx --ignore-daemonsets
kubeadm upgrade node
kubectl uncordon  controllerxx

11/  on install kubelet et kubectl sur tous les controllers :

yum install -y kubelet-1.17.8-0 kubectl-1.17.8-0 --disableexcludes=kubernetes
on reload kubelet :
systemctl restart kubelet && systemctl daemon-reload


12/ upgrade workers :

on installe / upgrade kubeadm sur tous les workers : 
yum install -y kubeadm-1.17.8-0 --disableexcludes=kubernetes

13/ on drain un par un tous les workers :
kubectl drain worker01 --ignore-daemonsets

14/ on met à jours kubelet : 
yum install -y kubelet-1.17.8-0 kubectl-1.17.8-0 --disableexcludes=kubernetes
systemctl restart kubelet
systemctl daemon-reload

15/ on remet en prod notre worker 

kubectl uncordon worker01 

16/ on repette l'operation sur tous les workers.

==== help / debug :

- tips upgrade controller : 
en cas de souci pour l'upgrade on peut forcer en passsant le fichier de conf
# kubeadm upgrade apply v1.19.4 --config config.yaml

# 
kubeadm --kubeconfig /etc/kubernetes/admin.conf --config /etc/kubernetes/config.yaml   upgrade diff 

- sur les workers portant du storage : 

kubectl drain fso-worker02 --ignore-daemonsets --delete-local-data




== validation applications systeme : ==

                                         
NAME                  STATUS   AGE
authent               Active   300d
cert-manager          Active   300d
default               Active   312d
external-dns          Active   61d
ingress-controller    Active   301d
kube-public           Active   312d
kube-system           Active   312d
metallb-system        Active   307d
prometheus-operator   Active   306d


> set up helm
calico ok (values pod cidr)

prometheus-operator ok
proxmox-monitoring ok MAIS pb flux 10.121.253.0/24 9100 / 9200

cert-manager ok ( modif values pour proxy )   > branch FeatureUpgradek8s

external-dns ok 

traefik  ok  (ajout values-fso avec record pour externalIP) > branch FeatureUpgradek8s  
setting label sur node pour ingress :
kubectl label nodes fso-controller01 node-role.kubernetes.io/ingress=true

 helm ls --all-namespaces                                 [☸ |kubernetes-admin@fso_sandbox:ingress-controller]
NAME                            	NAMESPACE          	REVISION	UPDATED                                	STATUS  	CHART                           	APP VERSION
calico                          	tigera-operator    	1       	2020-11-09 13:42:34.166036512 +0100 CET	deployed	calico-0.0.3              	1.7.0      
cert-manager                    	cert-manager       	2       	2020-11-10 15:05:41.91617385 +0100 CET 	deployed	cert-manager-0.0.3        	           
external-dns                    	external-dns       	1       	2020-11-10 15:10:16.841578866 +0100 CET	deployed	external-dns-0.0.3        	0.7.3      
sandbox-proxmox-monitoring	        prometheus-operator	1       	2020-11-10 14:08:47.263970456 +0100 CET	deployed	proxmox-monitoring-0.1.0  	1.0.0      
prometheus-operator             	prometheus-operator	1       	2020-11-10 13:55:14.420673396 +0100 CET	deployed	prometheus-operator-0.0.18	9.3.1      
traefik                         	ingress-controller 	1       	2020-11-10 15:37:16.61327572 +0100 CET 	deployed	traefik-0.0.4             	           


cluster 3 controllers / 3 workers 

version de départ v1.16.3
version cible 1.19.3

Attention on ne pas peut upgrader de plusieurs versions en une fois ( 1.16 > 1.18 = impossible ) .on va donc upgrader unitairement version par version ( 1.16.3 > 1.17.x puis 1.17.x > 1.18.x puis 1.18.x > 1.19.3)


= upgrade step 1 des composants 

on va mettre a jour les composants suivants : kubeadm / kubectl / kubelet 


- On va identifier la version présente sur nos repos pour notre premier upgrade : 

yum -v list kubeadm --show-duplicates |grep 1.17
kubeadm.x86_64                  1.17.0-0                    kubernetes_el7
kubeadm.x86_64                  1.17.1-0                    kubernetes_el7
kubeadm.x86_64                  1.17.2-0                    kubernetes_el7
kubeadm.x86_64                  1.17.3-0                    kubernetes_el7
kubeadm.x86_64                  1.17.4-0                    kubernetes_el7
kubeadm.x86_64                  1.17.5-0                    kubernetes_el7
kubeadm.x86_64                  1.17.6-0                    kubernetes_el7
kubeadm.x86_64                  1.17.7-0                    kubernetes_el7
kubeadm.x86_64                  1.17.7-1                    kubernetes_el7
kubeadm.x86_64                  1.17.8-0                    kubernetes_el7

on va donc installer la version 1.17.8-0 




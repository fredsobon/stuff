===== notes learn helm packpub : =====


- chap2 : install chart :

on va set up un wordpress 
minikube 4gb de ram driver / vm-driver: kvm2

helm repo add bitnami https://charts.bitnami.com/bitnami

kubectl create namespace chapter3
namespace/chapter3 created

on va pouvoir overrider les values de base que l'on peut examiner avaec la commande suivante : 

 helm show values bitnami/wordpress --version 8.1.0
The result of this command should be a long list of possible values that you
...

on va créer un fichier contenant les valeurs que nous voulons personaliser ( user , mdp bdd, nom du blog ...
on va comme on travaille dans minikube modifier la conf reseau en modifiant la conf loadbalancer en nodeport 

cat wordpress-values.yaml
wordpressUsername: helm-user
wordpressPassword: my-pass
wordpressEmail: helm-user@example.com
wordpressFirstName: boogie
wordpressLastName: man
wordpressBlogName: Learn Helm!
service:
 type: NodePort

on lance l'install :

helm install [NAME] [CHART] [flags]

-> Name : le nom que l'on veut donner à notre release helm
-> Chart : est le nom du chart helm installer (on peut utiliser la forme  <repo name>/<chart name>
-> flags : params que l'on peut passer pour overrider des valeurs par exemple.

on lance l'install :

helm install wordpress bitnami/wordpress --values=wordpress-values.yaml --namespace chapter3 --version 8.1.0
NAME: wordpress
LAST DEPLOYED: Sun Oct  4 17:22:23 2020
NAMESPACE: chapter3
STATUS: deployed
REVISION: 1
NOTES:
1. Get the WordPress URL:

  export NODE_PORT=$(kubectl get --namespace chapter3 -o jsonpath="{.spec.ports[0].nodePort}" services wordpress)
  export NODE_IP=$(kubectl get nodes --namespace chapter3 -o jsonpath="{.items[0].status.addresses[0].address}")
  echo "WordPress URL: http://$NODE_IP:$NODE_PORT/"
  echo "WordPress Admin URL: http://$NODE_IP:$NODE_PORT/admin"

2. Login with the following credentials to see your blog

  echo Username: helm-user
  echo Password: $(kubectl get secret --namespace chapter3 wordpress -o jsonpath="{.data.wordpress-password}" | base64 --decode)


on voit le contenu du fichier note du chart  affiché 
on peut examiner le chart installé :

helm list --namespace chapter3
NAME     	NAMESPACE	REVISION	UPDATED                                 	STATUS  	CHART          	APP VERSION
wordpress	chapter3 	1       	2020-10-04 17:22:23.914776076 +0200 CEST	deployed	wordpress-8.1.0	5.3.2

The list subcommand provides the following information:
• The release name
• The release namespace
• The latest revision number of the release
• A timestamp of the latest revision
• The release status
• The chart name
• The application version

-  helm namespace variable d'environment :
on peut definir une variable d'env définissant le namespace dans lequel on travaille avec notre helm :

export HELM_NAMESPACE=chapter3-  helm namespace variable d'environment :
on peut definir une variable d'env définissant le namespace dans lequel on travaille avec notre helm :

export HELM_NAMESPACE=chapter3
verif :
helm env
HELM_BIN="helm"
HELM_DEBUG="false"
HELM_KUBEAPISERVER=""
HELM_KUBECONTEXT=""
HELM_KUBETOKEN=""
HELM_NAMESPACE="chapter3"
HELM_PLUGINS="/home/boogie/.local/share/helm/plugins"
HELM_REGISTRY_CONFIG="/home/boogie/.config/helm/registry.json"
HELM_REPOSITORY_CACHE="/home/boogie/.cache/helm/repository"
HELM_REPOSITORY_CONFIG="/home/boogie/.config/helm/repositories.yaml"


- choisir entre --set et --values :
on peut overrider des valeur en cli avec --set ,
une bonne pratique est d'utiliser un fichier de values que l'on pourra versionner par ex dans git.
Biensur on ne met pas les secret dans git et on verra plus tard comment passer des secrets dans nos charts.

= exam de notre wordpress :

export NODE_PORT=$(kubectl get --namespace chapter3 -o jsonpath="{.spec.ports[0].nodePort}" services wordpress)

export NODE_IP=$(kubectl get nodes --namespace chapter3 -o jsonpath="{.items[0].status.addresses[0].address}")

echo "WordPress URL: http://$NODE_IP:$NODE_PORT/"
WordPress URL: http://192.168.39.214:32575/


on se logge dans notre navigo :

echo "WordPress Admin URL: http://$NODE_IP:$NODE_PORT/admin"
WordPress Admin URL: http://192.168.39.214:32575/admin

on se loggue en recupérant les infos user et mdp defini dans le fichier values


echo Username: helm-user
on peut onterroger kube pour retrouver le mdpt stocké en secret : 
echo Password: $(kubectl get secret --namespace chapter3 wordpress -o jsonpath='{.data.wordpress-password}' | base64 --decode)
Password: my-pass


on va maintenant upgrader notre chart en ajoutant un replicat et definisssant des ressources pour notre appli : on modifie donc notre fichier de values :

cat wordpress-values.yaml
wordpressUsername: helm-user
wordpressPassword: my-pass
wordpressEmail: helm-user@example.com
wordpressFirstName: boogie
wordpressLastName: man
wordpressBlogName: Boogie Learn Helm!
service:
  type: NodePort
replicaCount: 2
resources:
  requests:
   memory: 256Mi
   cpu: 100m  


helm upgrade [RELEASE] [CHART] [flags]

helm upgrade wordpress bitnami/wordpress --values wordpress-values.yaml -n chapter3 --version 8.1.0
Release "wordpress" has been upgraded. Happy Helming!
NAME: wordpress
LAST DEPLOYED: Sun Oct  4 18:14:33 2020
NAMESPACE: chapter3
STATUS: deployed
REVISION: 4
NOTES:
1. Get the WordPress URL:

  export NODE_PORT=$(kubectl get --namespace chapter3 -o jsonpath="{.spec.ports[0].nodePort}" services wordpress)
  export NODE_IP=$(kubectl get nodes --namespace chapter3 -o jsonpath="{.items[0].status.addresses[0].address}")
  echo "WordPress URL: http://$NODE_IP:$NODE_PORT/"
  echo "WordPress Admin URL: http://$NODE_IP:$NODE_PORT/admin"

2. Login with the following credentials to see your blog

  echo Username: helm-user
  echo Password: $(kubectl get secret --namespace chapter3 wordpress -o jsonpath="{.data.wordpress-password}" | base64 --decode)


Reusing and resetting values during an upgrade
The helm upgrade command includes two additional flags that are used to manipulate
values that are not present in the helm install command.
Let's look at these flags now:
• --reuse-values: When upgrading, reuse the last release's values.
• --reset-values: When upgrading, reset the values to the chart defaults.


par default si on ne precise rien le reuse-values est utilisé.

si on fait un upgrade avec un --set seule la valeur modifiée est changée : le reste des valeurs definies dans notre fichier sont ignorées :
helm upgrade wordpress bitnami/wordpress --set replicaCount=1 -n chapter3 --version 8.1.0

helm get values wordpress -n chapter3
USER-SUPPLIED VALUES:
replicaCount: 1


on a donc un reset des values défnies au préalable.
Users can manually provide the --reset-values or --reuse-values flags to
explicitly determine the behavior of values during an upgrade. Use the --resetvalues flag if you would like the next upgrade to reset each value to its default before
overriding it from the command line. Provide the --reuse-values flag if you would
like to reuse each of the values from a previous revision while setting different values from
the command line. To help simplify the management of values during an upgrade, try to
keep your values in a file that can be used to declaratively set values for each upgrade

- History :

les revisions des releases sont conservées en secret dans kube ( par defautl on peut les definir en configmap ou en mémoire que l'on defini dans la var HELM_DRIVER 
ex: 

 kubectl get secrets -n chapter3
NAME                              TYPE                                  DATA   AGE
default-token-n8m5s               kubernetes.io/service-account-token   3      118m
sh.helm.release.v1.wordpress.v1   helm.sh/release.v1                    1      96m
sh.helm.release.v1.wordpress.v2   helm.sh/release.v1                    1      83m
sh.helm.release.v1.wordpress.v3   helm.sh/release.v1                    1      52m
sh.helm.release.v1.wordpress.v4   helm.sh/release.v1                    1      44m
sh.helm.release.v1.wordpress.v5   helm.sh/release.v1                    1      31m
sh.helm.release.v1.wordpress.v6   helm.sh/release.v1                    1      26m
wordpress                         Opaque                                1      96m
wordpress-mariadb                 Opaque                                2      96m

helm history wordpress -n chapter3
REVISION	UPDATED                 	STATUS    	CHART          	APP VERSION	DESCRIPTION
1       	Sun Oct  4 17:22:23 2020	superseded	wordpress-8.1.0	5.3.2      	Install complete
2       	Sun Oct  4 17:36:10 2020	superseded	wordpress-8.1.0	5.3.2      	Upgrade complete
3       	Sun Oct  4 18:06:46 2020	superseded	wordpress-8.1.0	5.3.2      	Upgrade complete
4       	Sun Oct  4 18:14:33 2020	superseded	wordpress-8.1.0	5.3.2      	Upgrade complete
5       	Sun Oct  4 18:27:41 2020	superseded	wordpress-8.1.0	5.3.2      	Upgrade complete
6       	Sun Oct  4 18:32:46 2020	deployed  	wordpress-8.1.0	5.3.2      	Upgrade complete


on peut examiner les valeurs d'une revision : 

helm get values wordpress --revision 3 -n chapter3
USER-SUPPLIED VALUES:
service:
  type: NodePort
wordpressBlogName: Learn Helm!
wordpressEmail: helm-user@example.com
wordpressFirstName: boogie
wordpressLastName: man
wordpressPassword: my-pass
wordpressUsername: helm-user


 helm get values wordpress --revision 6 -n chapter3
USER-SUPPLIED VALUES:
replicaCount: 1


- rollback :

helm rollback <RELEASE> [REVISION] [flags]

helm rollback wordpress 5 -n chapter3
Rollback was a success! Happy Helming!

on peut voir le rollback dans l'history :

helm history wordpress -n chapter3
REVISION	UPDATED                 	STATUS    	CHART          	APP VERSION	DESCRIPTION
1       	Sun Oct  4 17:22:23 2020	superseded	wordpress-8.1.0	5.3.2      	Install complete
2       	Sun Oct  4 17:36:10 2020	superseded	wordpress-8.1.0	5.3.2      	Upgrade complete
3       	Sun Oct  4 18:06:46 2020	superseded	wordpress-8.1.0	5.3.2      	Upgrade complete
4       	Sun Oct  4 18:14:33 2020	superseded	wordpress-8.1.0	5.3.2      	Upgrade complete
5       	Sun Oct  4 18:27:41 2020	superseded	wordpress-8.1.0	5.3.2      	Upgrade complete
6       	Sun Oct  4 18:32:46 2020	superseded	wordpress-8.1.0	5.3.2      	Upgrade complete
7       	Sun Oct  4 19:09:33 2020	deployed  	wordpress-8.1.0	5.3.2      	Rollback to 5

on retrouve bien les valeurs voulues dans notre version rollbackée :

helm get values wordpress -n chapter3
USER-SUPPLIED VALUES:
replicaCount: 2
resources:
  requests:
    cpu: 100m
    memory: 256Mi
service:
  type: NodePort
wordpressBlogName: Boogie Learn Helm!
wordpressEmail: helm-user@example.com
wordpressFirstName: boogie
wordpressLastName: man
wordpressPassword: my-pass
wordpressUsername: helm-user


- Desinstallation de chart :

helm uninstall RELEASE_NAME [...] [flags]

helm uninstall wordpress -n chapter3
release "wordpress" uninstalled

helm list -n chapter3
NAME	NAMESPACE	REVISION	UPDATED	STATUS	CHART	APP VERSION


Attention il va rester un persistantvolumeclaim defini dans le statefullset qui n'est pas supprimé lors de la désinstallation du chart.
le statefullset est deleted mais le pvc associé a ce statfullset ne l'est pas.

kctl get pvc -n chapter3
NAME                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-wordpress-mariadb-0   Bound    pvc-014b0631-6f4e-4dfb-9182-068750a2cff0   8Gi        RWO            standard       114m


on va donc le supprimer manuellement :

 kubectl delete pvc -l release=wordpress -n chapter3

-suppression de notre namespace et arret de minikube :
 kubectl delete namespace chapter3
  minikube stop

== understanding helm charts : ==

= yaml format :

format utilisé entre autre dans kube et helm.
"YAML files follow a key-value format to declare configuration." 

- definition d'une simple paire clé / valeur
ex: 
name: LearnHelm

clé = name
valeur = LearnHelm

on a un separateur ":" entre les deux ET un espace qui est obligatoire.
on peut imbriquer dans yaml plusieurs blocs :
ex :
resources:
 limits:
 cpu: 100m
 memory: 512Mi

cette ressource contient une map de 2 paires de clés / valeurs
Clé                       valeur
ressources.limits.cpu     100m
ressources.limits.memory  512Mi
Les clés sont déterminées par l'indentation yaml . Chaque indentation ajoute un "." separateur au nom de la clé.
La valeur de la clé est atteinte quand il ne reste plus d'indentation dans le yaml.
De base l'indentation yaml est de 2 espaces. Attention les tabulations ne sont pas admises et font ressortir une parse error.

- types de valeurs :

-> string : en principe on peut entourer le texte de ".

Les multi lignes peuvent être construites avec un |
ex: 
configuration: |
 server.port=8443
 logging.file.path=/var/log

-> integer :
ex: 
replicas: 1

Attention si on quote la valeur elle sera considérée comme une string:
replicas: "1"

-> boolean :
ex: 
ingress:
 enable: true

 prend 2 valeurs true ou false
mais aussi yes, no, on, off, y, n, Y, and N

-> list :

les éléments d'une liste sont présentées par "-" :
ex: 
servicePorts:
 - 8080
 - 8443

autre exemple :
deployment:
  env:
   - name: MY_VAR
     value: MY_VALUE
   - name: SERVICE_NAME
     value: MY_SERVICE

dans cet exemple le tableau contient une liste d'objects contenant des champs name et value 


= json :

" JSON is a string of key-value
pairs, similar to YAML. The key difference is that while YAML relies on spacing and
indentation to properly configure key-value pairs, JSON relies on braces and brackets."

exemple precedent en json :

{
 'deployment': {
 'env': [
   {
     'name': 'MY_VAR',
     'value': 'MY_VALUE'
   },
   {
     'name': 'SERVICE_NAME',
     'value': 'MY_SERVICE'
   }
  ]
 }

Toutes les clés sont entourées de simple quote "'" précédent un ":"

"Curly braces ({) denote a block in a similar way to how indentations denote a
block in YAML.
• Square brackets ([) denote a list in a similar way to how dashes denote a list
in YAML."

= helm chart structure : =

un chart est un packaging de ressources kube permettant de déployer des applis de maniere plus ou moins complexes.
Une structure est cependant  respecter :

mychart  > le repertoire top level domaine est le nom du chart par bonne pratique.

Chart.yaml -> fichier contenant les metadata du chart. mandatory
templates -> repertoire contenant les ressources kube sous format yaml  mandatory en principe
templates/NOTES.txt -> fichier d'instruction qui donnent des informations sur l'installation d'un chart . pas mandatory 
values.yaml -> fichier contenant les valeurs par défaut du chart. Ce fichier est une best practice de helm.
.helmignore -> fichier contenant une list de fichiers et repertoire à exclure pour le packaging de helm.
charts/   -> repertoire contenant les charts dont ce chart dépend. PAs mandatory car le process de helm dependancy va créer automatiquement ce rep.
Chart.lock -> fichier pour sauvegarder la version des precedentes dépendances appliquées.PAs mandatory car le process de helm dependancy va créer automatiquement ce rep.
crds/ -> repertoire contenant des crd en yaml devant être installées avant les fichiers dans le rep templates.
README.md -> fichier contenant les consignes d'install du chart. Best practice.
LICENCE -> fichier contenant la licence d'un chart . pas mandatory
values.schema.json -> fichier contenant les données du fichier values sous format json. pas mandatory


- Charts templates :

Le principal but d'un chart est de creer et manager les ressources kube pour faire une application.Ceci est fait grace aux templates dont les valeurs passées en parametres vont customiser ces templates.
le rep template contient des ressources kube en yaml 

ex: 
templates/
 configmap.yaml
 deployment.yaml
 service.yaml

la configmap pourra être comme ceci :
apiVersion: v1
kind: ConfigMap
metadata:
 name: {{ .Release.Name }}
data:
 configuration.txt: |-
 {{ .Values.configurationData }}

Les {{ et }} sont issues du templating go et ces blocs seront remplacés et alimentés par les valeurs des éléments trouvés dans le / les fichiers values et par les variables internes builin à helm.

- Go templating :

language go initié par google en 2009, utilisé par kube, helm et autres ...
un des éléments au coeur de go est son moteur de templating qui va permettre de générer des fichiers sous différents formats.
le templating go controle et process les blocs compris entre {{ et }}, lors de l'install ou l'upgrade d'un chart ce fait ces accolades sont supprimées.

- paramétrage de base et objects builtin :

les charts helm contiennent au moins un fichier values.yaml qui contient les valeurs par défault du chart qui sont reférencées par les templates go et procéssées par helm pour générer une ressource kube.
ex: values.yaml

## chapterNumber lists the current chapter number
chapterNumber: 4
## chapterName gives a description of the current chapter
chapterName: Understanding Helm Charts

les lignes commencant par un # sont des commentaires qui peuvent nous aider à comprendre les valeurs.
Les templates go commencant par .Values vont référencer les valeurs définies dans le fichier values.yaml ou passées en argument avec les flags --set ou --values pendant l'installation ou l'upgrade d'un chart
L'exemple suivant nous montre le template avant son traitement : 

env:
 - name: CHAPTER_NUMBER
 value: {{ .Values.chapterNumber }}
 - name: CHAPTER_NAME
 values: {{ .Values.chapterName }}

Apres le traitement le code de la ressource  concernée sera donc comme suivant : 
env:
 - name: CHAPTER_NUMBER
 value: 4
 - name: CHAPTER_NAME
 values: Understanding Helm Charts



- built in :

Le .Values est un objet builtin utilisé pour le paramétrage.
cf list des built in dans la  documentation (https://helm.sh/docs/chart_template_guide/builtin_objects/)

les élément suivants sont les principaux :

.Release.Name -> nom de la release utilisé pour l'installation
.Release.Namespace -> namespace utilisé pour le déployement du chart.
.Release.Revision -> num de revision d'install ou d'upgrade
.Values -> utilisé pour pour référencer les valeurs du ou des fichiers values.yaml ou defini en param sur la ligne de cimmande par le user.

.Values -> utilisé pour pour référencer les valeurs du ou des fichiers values.yaml ou defini en param sur la ligne de commande par le user.
.Chart.Name, .Chart.Version, .Chart.AppVersion ... -> utilisés pour référencer les champs dans le fichier Chart.yaml : on peut utiliser les différents champ de ce fichier par .Chart.$Field
.Files.Get -> utilisé pour recupérer un fichier dans un rep du chart.
.Files.AsSecrets -> retourne un fichier encodé en base64 pour créer un secret depuis un fichier dans un repertoire du chart.
.Files.AsConfig -> retourne les données d'un fichier sous format yaml pour créer les data d'une configmap avec les données d'un fichier dans un rep du chart.
.Capabilities.APIversions -> retourne une liste de l&pi kube du cluster
.Template.Name -> retourne le chemin relatif du fichier utilisé par l'object.

Le "." prefixant chaque objet représente le scope de l'object.
Un "." suivi d'un nom d'object limite le scope à cet object :
ex: .Release > limite au scope .Release , .Values au scope .Values 

Le "." représente le scope global : rendant tous les objects visibles.

- fichier values.schema.json 
optionnel, il va servir en renforcer les controlles de saisies dans le fichier values.yaml

ex :
{
 '$schema': 'https://json-schema.org/draft-07/schema#',
 'properties': {
   'replicas': {
     'description': 'number of application instances to deploy',
     'minimum': 0
     'type' 'integer'
   },
 . . .
 'title': 'values',
 'type': 'object'
}
 
on voit ici que le minimum de replica a definir est de 0 et que le type attendu est un entier.

- control de flux :

if/else > utilisé pour inclure ou exlure un bloc dans un fichier 

ex: ici si la valeur definie dans .Values.probeType.httpGet est vrai ou non nulle : on defini la sonde avec un httpGet port 8080 .. si aucune valeur n'est définie dans notre values.yaml alors on défini une sonde de type tcpSocket 


readinessProbe:
{{- if .Values.probeType.httpGet }}
  httpGet:
    path: /healthz
    port: 8080
    scheme: HTTP
{{- else }}
  tcpSocket:
    port: 8080
{{- end }}
 initialDelaySeconds: 30
 periodSeconds: 10

with > va être defini pour modifier le scope des valeurs référencées
pratique pour des cascades de valeurs définies 

ex : values.yaml 
application:
  resources:
    limits:
      cpu: 100m
       memory: 512Mi

sans l'utilisation de with dans nos templates on sera obliger de définir les champs comme suivants :

cpu: {{ .Values.application.resources.limits.cpu }}
memory: {{ .Values.application.resources.limits.memory }}

avec with on aura une syntaxe plus concise et lisible 

{{- with .Values.application.resources.limits }}
cpu: {{ .cpu }}
memory: {{ .memory }}
{{- end }}
       
- range > va permettre de boucler sur une liste de valeurs :

servicePorts:
  - name: http
  port: 8080
  - name: https
  port: 8443
  - name: jolokia
  port: 8778
       
spec:
  ports:
{{- range .Values.servicePorts }}
  - name: {{ - name }}
  port: {{ .port }}
{{- end }}

Attention : with et range limitent leur action dans le scope des objets définis (ex : .Values, .Release) ..si on veut agir au niveau du scope global : on doit préfixer les références avec "$" 

ex:  ici on boucle sur les valeurs de ports de l'objet .Values mais on va ajouter le nom de notre release issu de l'objet .Release dans un autre scope en préfixant avec "$" :
{{- range .Values.servicePorts }}
 - name: {{ $.Release.Name }}-{{ .name }}
 port: {{ .port }}
{{- end }}


- Variables de templates :

on peut créer des variables dans nos templates pour ajouter des options de processing 

definition de variable :

ex : 
{{ $myvar := 'Hello World!' }}

Les variables peuvent être assignées à des objects :
ex :
{{ $myvar := .Values.greeting }}

on pourra referencer la variable par ex comme ceci :

data:
 greeting.txt: |
   {{ $myvar }}


un exemple d'utilisation est la capture des index dans une boucle sur une liste :

data:
 greetings.txt: |
{{- range $index, $value := .Values.greetings }}
 Greeting {{ $index }}: {{ $value }}
{{- end }}
Les résulats pourront donner quelque chose comme : 
data:
 greetings.txt: |
 Greeting 0: Hello
 Greeting 1: Hola
 Greeting 2: Hallo

on peut biensur aussi boucler sur une map / hash / dictionnaire :

data:
  greetings.txt: |
{{- range $key, $val := .Values.greetings }}
  Greeting in {{ $key }}: {{ $val }}
{{- end }}

avec comme résultat par exemple :

data:
 greetings.txt: |
 Greeting in English: Hello
 Greeting in Spanish: Hola
 Greeting in German: Hallo

on peut aussi utiliser les variables pour référencer des objects en dehors de notre scope : 

ex: le template suivant va echouer car .Release.Name n'est pas sous le scope de .Values.application.configuration 

{{- with .Values.application.configuration }}
My application is called {{ .Release.Name }}
{{- end }}

scope of .Values.application.configuration. One way this can be remedied is
by setting a variable to .Release.Name above the with block:
une manière de remedier à ce pb est de définir une variable avant le bloc with :
{{ $appName := .Release.Name }}
{{- with .Values.application.configuration }}
My application is called {{ $appName }}
{{- end }}

Cependant la methode utilisant le $ pour référencer le scope global est meilleure car plus lisible et moins longue a écrire. 

- process complexes avec les pipes  / pipelines :

on va pouvoir chainer les commandes avec des fonctions comme dans linux :

on peut  par exemple utiliser la fonction indent qui va ajouter des espaces :

on peut ecrire et utiliser les fonctions ( en tout cas indent de plusieurs manieres) 
ex 
data:
 application-config: |-
{{ indent 4 .Values.config }}

ou alors avec un pipe :

data:
 application-config: |-
{{ .Values.config | indent 4 }}

l'avantage du pipe est qu'on peut chainer les commandes / fonctions :
ex :
data:
 application-config: |-
{{ .Values.config | indent 4 | quote }}

la liste des fonctions Go est importante :

 https://golang.org/pkg/text/template/#hdr-Functions 
 and in the Sprig template library at http://masterminds.github.io/sprig/. 

- Reutilisation de code avec les "named template" :

quand on cree des templates on peut etre confronter à la répétition de bloc qui peut être longue 
ex :

labels:
 'app.kubernetes.io/instance': {{ .Release.Name }}
 'app.kubernetes.io/managed-by': {{ .Release.Service }}
 ....
dans certains cas ces labels doivent être inclus dans toutes les ressources helm ...ce qui peut être long , fastidieux et source d'oubli et encore plus si le label doit être modifié.
helm permet de créer des named template pour pouvoir facilement réutiliser des blocs de code.
Ils doivent être sous le repertoire templates et finir par l'extension ".tpl"
Beaucoup de charts sont crées avec le fichier  _helpers.tpl mais ce n'est pas une obligation.

pour créer un named template on utilisera le mot define dans notre fichier .tpl 
ex : pour l'exemple de nos labels : 

{{- define 'mychart.labels' }}
labels:
 'app.kubernetes.io/instance': {{ .Release.Name }}
 'app.kubernetes.io/managed-by': {{ .Release.Service }}
{{- end }}

L'action define prend un nom de template en argument.Dans notre exemple mychart.labels
La convention de nommage est que le template est $CHART_NAME.$TEMPLATE_NAME dans lequel $CHART_NAME est le nom du chart et $TEMPLATE_NAME est un nom court décrivant le but du template
mychart.labels implique que le template est natif au chart helm mychart et qu'il va générer des labels pour les ressources du chart.

pour utiliser un named template on saisira la fonction include 

include [TEMPLATE_NAME] [SCOPE]

le param TEMPLATE_NAME  est le nom du named template qui va être appliqué 
le param SCOPE est le scope des valeurs et objects builtin qui doivent être traités 
dans la plupart du temps le params est "." pour signaler le top scope level
le $ peut être utiliser si le named scope références des valeurs en dehors du scope actuel 
ex d'utilisation : 

metadata:
 name: {{ .Release.Name }}
{{- include 'mychart.labels' . | indent 2 }}

on a ici l'exemple qui defini le nom de la ressource comme le nom de la release et ensuite la fonction include qui va process les labels et les indentés de 2 espaces comme déclarer dans le pipe
a la fin du traitement on peut avoir une ressource de type :  
metadata:
 name: template-demonstration
 labels:
 'app.kubernetes.io/instance': template-demonstration
 'app.kubernetes.io/managed-by': Helm

Helm also provides a template action that can also expand named templates. This
action has the same usage as include, but with one major limitation—it cannot be used
in a pipeline to provide additional formatting and processing. The template action is
used to simply display data inline. Because of this limitation, chart developers should use
the include function over the template action since include has feature parity with
template but also provides the additional benefit of pipeline processing



!!!!! p 114 to be completed !!!!!!



== Building your first chart : ==

on va utiliser une appli fourni par la communauté kube pour comprendre notre build helm : "https://kubernetes.io/docs/
tutorials/stateless-application/guestbook/"
code php  en front et une boite de dialogue pour saisir nos data et redis pour stocker les data 

on aura une replication redis : un master qui prendra les ecritures saisies lors de l'envoi de data et un node qui servira aux lectures
 https://github.com/PacktPublishing/-Learn-Helm -> repertoire the helm-charts/charts/guestbook 

 kubectl create namespace chapter5

pour avoir un chart fonctionnel on doit avoir des éléments mandatory :
• Chart.yaml: Used to define chart metadata
• values.yaml: Used to define default chart values
• templates/: Used to define chart templates and Kubernetes resources to be
created

la commande helm create va nous créer automatiquement tous les éléments nécéssaires et même plus : 

ogie@boogieland  ~/Documents/lab/helm/guestbook   helm create guestbook                                                                                                                [☸ |minikube:chapter5]
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
Creating guestbook
 boogie@boogieland  ~/Documents/lab/helm/guestbook  tree                                                                                                                                  [☸ |minikube:chapter5]
.
└── guestbook
    ├── charts
    ├── Chart.yaml
    ├── templates
    │   ├── deployment.yaml
    │   ├── _helpers.tpl
    │   ├── hpa.yaml
    │   ├── ingress.yaml
    │   ├── NOTES.txt
    │   ├── serviceaccount.yaml
    │   ├── service.yaml
    │   └── tests
    │       └── test-connection.yaml
    └── values.yaml

on va supprimer le rep tests que l'on verra plus tard pour les tests unitaires 
 rm -rf guestbook/templates/tests
 tree                                                                                                                                  [☸ |minikube:chapter5]
.
└── guestbook
    ├── charts
    ├── Chart.yaml
    ├── templates
    │   ├── deployment.yaml
    │   ├── _helpers.tpl
    │   ├── hpa.yaml
    │   ├── ingress.yaml
    │   ├── NOTES.txt
    │   ├── serviceaccount.yaml
    │   └── service.yaml
    └── values.yaml

examen des ressources 

-  Chart.yaml 
cat Chart.yaml                                                                                                              [☸ |minikube:chapter5]


apiVersion: v2   <<<<<< version d'api : en helm3 v2 est préférable 
name: guestbook  <<<<<<  nom de notre chart 
description: A Helm chart for Kubernetes  <<<<< description rapide de notre chart

# A chart can be either an 'application' or a 'library' chart.
#
# Application charts are a collection of templates that can be packaged into versioned archives
# to be deployed.
#
# Library charts provide useful utilities or functions for the chart developer. They're included as
# a dependency of application charts to inject those utilities and functions into the rendering
# pipeline. Library charts do not define any templates and therefore cannot be deployed.
type: application  <<<<<<< type de chart application ou library ( library vient avec des addons et helpers qui peuvent être utilisés dans d'autres charts pour éviter les répétitions.

# This is the chart version. This version number should be incremented each time you make changes
# to the chart and its templates, including the app version.
# Versions are expected to follow Semantic Versioning (https://semver.org/)
version: 0.1.0   <<<< numéro de version qui doit être conforme au Semantic Versioning specifications (SemVer).


# This is the version number of the application being deployed. This version number should be
# incremented each time you make changes to the application. Versions are not expected to
# follow Semantic Versioning. They should reflect the version the application is using.
appVersion: 1.16.0    <<<<< version de l'application déployée par le chart helm

Il n'y a pas ici pour l'instant de section :

dependencies: une list de charts dont ce Helm chart depend

- Ajout de la dépendance redis :
comme on l'a vu notre appli a besoin d'une db redis pour fonctionner

on va chercher le chart redis :

helm search hub redis                                                                                                      [☸ |minikube:chapter5]
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
URL                                               	CHART VERSION	APP VERSION  	DESCRIPTION
https://hub.helm.sh/charts/drycc-canary/redis     	1.0.0        	             	A Redis database for use inside a Kubernetes cl...
https://hub.helm.sh/charts/choerodon/redis        	0.2.5        	0.2.5        	redis for Choerodon
https://hub.helm.sh/charts/drycc/redis            	1.1.0        	             	A Redis database for use inside a Kubernetes cl...
https://hub.helm.sh/charts/wener/redis            	10.8.2       	6.0.8        	Open source, advanced key-value store. It is of...
https://hub.helm.sh/charts/bitnami/redis          	11.2.2       	6.0.8        	Open source, advanced key-value store. It is of...
https://hub.helm.sh/charts/hephy/redis            	2.4.2        	             	A Redis database for use inside a Kubernetes cl...
https://hub.helm.sh/charts/helm-incubator/redis...	0.5.0        	4.0.12-alpine	A pure in-memory redis cache, using statefulset...
https://hub.helm.sh/charts/inspur/redis-cluster   	0.0.2        	5.0.6        	Highly available Kubernetes implementation of R...
https://hub.helm.sh/charts/helm-stable/redis-ha   	4.4.4        	5.0.6        	Highly available Kubernetes implementation of R...
https://hub.helm.sh/charts/hkube/redis-ha         	3.6.1005     	5.0.5        	Highly available Kubernetes implementation of R...
https://hub.helm.sh/charts/dandydeveloper/redis-ha	4.10.2       	6.0.7        	Highly available Kubernetes implementation of R...
https://hub.helm.sh/charts/softonic/redis-sharded 	0.3.0        	6.0.6        	A Helm chart for sharded redis
https://hub.helm.sh/charts/bitnami/redis-cluster  	3.2.8        	6.0.8        	Open source, advanced key-value store. It is of...
https://hub.helm.sh/charts/prometheus-community...	3.6.0        	1.11.1       	Prometheus exporter for Redis metrics
https://hub.helm.sh/charts/hmdmph/redis-pod-lab...	1.0.2        	1.0.0        	Labelling redis pods as master/slave periodical...
https://hub.helm.sh/charts/wyrihaximusnet/redis...	1.0.1        	v1.0.0       	Redis Database Assignment Operator
https://hub.helm.sh/charts/pozetron/keydb         	0.5.1        	v5.3.3       	A Helm chart for multimaster KeyDB optionally w...
https://hub.helm.sh/charts/enapter/keydb          	0.16.2       	6.0.16       	A Helm chart for KeyDB multimaster setup
https://hub.helm.sh/charts/helm-stable/sensu      	0.2.3        	0.28         	Sensu monitoring framework backed by the Redis ...

on va si nous ne l'avons pas encore ajouter le repo bitnami :
helm repo add bitnami https://charts.bitnami.com

on va chercher un num de version de chart pas d'appli : le num de l'appli correspond a la version de redis pas celle du chart.

helm search repo redis --versions --max-col-width 100 |grep bit |head                                                       [☸ |minikube:chapter5]
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
bitnami/redis                                 	11.2.1       	6.0.8          	Open source, advanced key-value store. It is often referred to as a data structure server since k...
bitnami/redis                                 	11.2.0       	6.0.8          	Open source, advanced key-value store. It is often referred to as a data structure server since k...
bitnami/redis                                 	11.1.3       	6.0.8          	Open source, advanced key-value store. It is often referred to as a data structure server since k...
bitnami/redis                                 	11.1.1       	6.0.8          	Open source, advanced key-value store. It is often referred to as a data structure server since k...
bitnami/redis                                 	11.1.0       	6.0.8          	Open source, advanced key-value store. It is often referred to as a data structure server since k...
bitnami/redis                                 	11.0.7       	6.0.8          	Open source, advanced key-value store. It is often referred to as a data structure server since k...
bitnami/redis                                 	11.0.6       	6.0.8          	Open source, advanced key-value store. It is often referred to as a data structure server since k...
bitnami/redis                                 	11.0.5       	6.0.8          	Open source, advanced key-value store. It is often referred to as a data structure server since k...
bitnami/redis                                 	11.0.4       	6.0.8          	Open source, advanced key-value store. It is often referred to as a data structure server since k...
bitnami/redis                                 	11.0.3       	6.0.8          	Open source, advanced key-value store. It is often referred to as a data structure server since k..

Dependencies allow you to choose a particular chart version, or a wildcard such
as 10.5.x. Using a wildcard allows you to easily keep your chart updated with
the latest Redis version matching that wildcard (which, in this case, is version
10.5.14). In this example, we will use version 11.x.x

en épurant notre fichier Chart.yaml on a :

cat Chart.yaml                                                                                                              [☸ |minikube:chapter5]

apiVersion: v2
name: guestbook
description: A Helm chart for Kubernetes
type: application
version: 0.1.0
appVersion: 1.16.0
dependencies:
 - name: redis
 version: 11.x.x
 repository: https://charts.bitnami.com/bitnami

on va maintenant download notre dépendance :

ceci va dl le chart redis dans le rep charts et créer le fichier Chart.lock 
helm dependency update .

tree                       [☸ |minikube:chapter5]
.
└── guestbook
    ├── Chart.lock
    ├── charts
    │   └── redis-11.2.2.tgz
    ├── Chart.yaml
    ├── templates
    │   ├── deployment.yaml
    │   ├── _helpers.tpl
    │   ├── hpa.yaml
    │   ├── ingress.yaml
    │   ├── NOTES.txt
    │   ├── serviceaccount.yaml
    │   └── service.yaml
    └── values.yaml

- modification du fichier values.yaml 

en mode interractif on peut utliser les flags : --set or --values 
on va ajouter les valeurs qui nous interressent. Un chart doit être bien autodocumenté.

on va examiner les valeurs possible dans notre chart redis : 

helm show values charts/redis-11.2.2.tgz |head                                                                              [☸ |minikube:chapter5]
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry and imagePullSecrets
##
global:
  # imageRegistry: myRegistryName
  # imagePullSecrets:
  #   - myRegistryKeySecretName
  # storageClass: myStorageClass
  redis: {}
....
.......

La premiere valeur qu'on va modifier est fullnameOverride :

## String to partially override redis.fullname template (will maintain the release name)
##
# nameOverride:

## String to fully override redis.fullname template
##
# fullnameOverride:


les charts utilisent souvent cette valeur dans un named template appellé "$CHART_NAME.fullname"
quand fullnameOverride est définie le named template va definir a cette valeur sinon le resultat de ce template se basera sur l'object ".Release.Name object, ou le nom de la release utilisée pour l'install.
La dependance redis utilise redis.fullname  pour aider la conf redis avec son master et slave.

ex d'une utilisation : 
name: {{ template 'redis.fullname' . }}-master

The Guestbook application requires the Redis services to be named redismaster and redis-slave. As a result, the fullnameOverride value should
be set to redis.

on peut decompresser le chart redis et examiner le fichier _helpers.tpl qui nous montre comment est defini le named template 
....
{{- define "redis.fullname" -}}
{{- if .Values.fullnameOverride -}}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" -}}
{{- else -}}
{{- $name := default .Chart.Name .Values.nameOverride -}}
...
Le code source de l'appli guestbook est dispo :
https://github.com/kubernetes/examples/blob/master/guestbook/php-redis/guestbook.php

dans le fichier values on va donc définir  des valeurs mandatory pour faire fonctionner l'appli guestbook 

fullnameOverride: "redis"  <<<< c'est le nom attendu pour l'appli
usePassword: false  <<<< l'appli simple n'utilise pas d'autent
on va aussi desactiver une option d'ecriture de backup de redis dans un configmap ( nous utiliserons plus tard un snapshot) 
configmap: |-
 appendonly no


Avec ces éléments de configs déterminés on va ajouter un bloc dans notre fichier values.yaml qui sera dédié à notre dépendance redis c'est pourquoi une section redis est ajoutée avec les valeurs dédiées overridées.
redis:
 # Override the redis.fullname template
 fullnameOverride: redis
 # Enable unauthenticated access to Redis
 usePassword: false
 # Disable AOF persistence
 configmap: |-
   appendonly no

L'utilisation de helm create a auto provisionner des fichiers qui vont devoir être modifiés car ils servent d'exemple

ex : dans le fichier values.yaml on a une image nginx de définie , un num de version

on va modifier les valeurs :
..
image:
  repository: repository: gcr.io/google-samples/gb-frontend  <<< on remplace l'image nginx par celle de notre appli
...
service:
  type: NodePort  <<<<< on met NodePort à la place de ClusterIp
  port: 80

on va modifier le fichier Chart.yaml pour changer le appVersion :

appVersion: v4

on lance l'install de notre chart :

helm install my-guestbook . -n chapter5                                                                                    [☸ |minikube:chapter5]
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
Error: cannot load values.yaml: error converting YAML to JSON: yaml: line 8: mapping values are not allowed in this context
 ✘ boogie@boogieland  ~/Documents/lab/helm/guestbook/guestbook  vi values.yaml                                                                                                            [☸ |minikube:chapter5]
 boogie@boogieland  ~/Documents/lab/helm/guestbook/guestbook  helm install my-guestbook . -n chapter5                                                                                     [☸ |minikube:chapter5]
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
NAME: my-guestbook
LAST DEPLOYED: Sun Oct 25 18:42:11 2020
NAMESPACE: chapter5
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace chapter5 -o jsonpath="{.spec.ports[0].nodePort}" services my-guestbook)
  export NODE_IP=$(kubectl get nodes --namespace chapter5 -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT


on verifie notre install : 
kctl get pods                                                                                                               [☸ |minikube:chapter5]
NAME                            READY   STATUS    RESTARTS   AGE
my-guestbook-794d6fbfd7-kldj5   1/1     Running   0          79s
redis-master-0                  1/1     Running   0          79s
redis-slave-0                   1/1     Running   0          79s
redis-slave-1                   1/1     Running   0          27s

on va lancer les commandes fournies dans le NOTES de notre install pour acceder à l'appli :

  export NODE_PORT=$(kubectl get --namespace chapter5 -o jsonpath="{.spec.ports[0].nodePort}" services my-guestbook)
  export NODE_IP=$(kubectl get nodes --namespace chapter5 -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
http://192.168.39.214:32256

curl -I http://192.168.39.214:32256                                                                                         [☸ |minikube:chapter5]

HTTP/1.1 200 OK
Date: Sun, 25 Oct 2020 17:44:58 GMT
Server: Apache/2.4.10 (Debian) PHP/5.6.20
Last-Modified: Wed, 09 Sep 2015 18:35:04 GMT
ETag: "399-51f54bdb4a600"
Accept-Ranges: bytes
Content-Length: 921
Vary: Accept-Encoding
Content-Type: text/html


si on reussi a saisir un message dans le champ c'est que tout est ok et que c'est ecrit en db

on va desinstaller notre chart et detruire le pvc manuellement pour améliorer notre chart 

helm uninstall my-guestbook -n chapter5                                                                                    [☸ |minikube:chapter5]
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
release "my-guestbook" uninstalled
 boogie@boogieland  ~/Documents/lab/helm/guestbook/guestbook  kubectl delete pvc -l app=redis -n chapter5                                                                                 [☸ |minikube:chapter5]
persistentvolumeclaim "redis-data-redis-master-0" deleted
persistentvolumeclaim "redis-data-redis-slave-0" deleted
persistentvolumeclaim "redis-data-redis-slave-1" deleted

= Amelioration du chart guestbook : =

toutes les applis peuvent être améliorées ..dans notre cas on veut ameliorer deux éléments 
> ajouter des hooks de backup et restoration de db redis 
> ajouter un controle de saisi dans le formulaire web de notre app.

- creation de hooks pré-upgrade et pré-rollback 

> pré-upgrade : ce hook intervient des que la commande upgrade est executée mais avant la modif de ressources kube. On va créer un snapshot de db au cas ou l'upgrade se passe mal.
> pre-rollback : ce hook intervient des que la commande rollback est executée mais avant le revert des ressources kube : une restoration du snap de db va être fait pour s'assurer qu'on va revenir sur les data correspondante à notre version rollbackée


- pre-upgrade hook :
dans redis les data de snapshots sont stockées dans un fichier  dump.rdb 
on peut backuper ce fichier en creant un pvc , ce hook peut créer un job qui copie le fichier dans le nouveau pvc crée

on va donc créer nous même ce hook 

dans le rep templates on crée un rep backup :
mkdir backup

on va ensuite créer deux fichiers pour nos deux taches 

touch persistentvolumeclaim.yaml ; touch job.yaml


- pvc : 

cat  persistentvolumeclaim.yaml                                                                         
{{- if .Values.redis.master.persistence.enabled }}   <<<< ici tout le bloc du if est appliqué si la valeur redis.master.persistence.enabled  est définie à vrai : ce qui est le cas quand on examine les valeurs du chart de dependance redis : helm show values charts/redis-11.2.2.tgz |grep persistence
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: redis-data-{{ .Values.redis.fullnameOverride }}-master-0-backup-{{ sub .Release.Revision 1 }} <<< ici on defini le nom de notre pvc dédié au backup il contient le mot backup et comporte le num de version qui est celui de la version de l'upgrade -1 grace a la fonction sub ..1 : on a donc un snap avec nos data fraiches au cas ou l'upgrade se passe mal. 
  labels:
    {{- include "guestbook.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": pre-upgrade <<< on crée un annotation pre upgrade 
    "helm.sh/hook-weight": "0"   <<< on donne un poid a notre hook : la valeur 0 indique que ce hook est executé avant tous les autres .
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: {{ .Values.redis.master.persistence.size }}
{{- end }}



- job :

cat job.yaml                                                                                             
{{- if .Values.redis.master.persistence.enabled }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "guestbook.fullname" . }}-backup
  labels:
    {{- include "guestbook.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": pre-upgrade   <<<<<  on defini ici une annotation de pre-upgrade 
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded  <<< on pose ici une annotation qui va permettre de delete le hook ( de base les hook ne sont pas détruit . on ajoute la policy before-hook-creation,hook-succeeded : qui va donc détruire le job pendant les phases d'upgrade : pour relancer un bacth frais , de plus le job sera delete si le run c'est bien passé.
    "helm.sh/hook-weight": "1" <<<< la valeur ici est de 1 : ce hook passe donc apres notre hook de pvc 
spec:
  template:
    spec:
      containers:
        - name: backup
          image: redis:alpine3.11
          command: ["/bin/sh", "-c"]
          args: ["redis-cli -h {{ .Values.redis.fullnameOverride }}-master save && cp /data/dump.rdb /backup/dump.rdb"]  <<<<<< ici on defini la commande qui va va se connecter au master , faire le backup et le copier sur le pvc crée pour 
          volumeMounts:
            - name: redis-data
              mountPath: /data
            - name: backup
              mountPath: /backup
      restartPolicy: Never
      volumes:                     <<<<<<  on defini ici les pvc master et backup qui seront montés pour l'operation de backup 
        - name: redis-data
          persistentVolumeClaim:
            claimName: redis-data-{{ .Values.redis.fullnameOverride }}-master-0
        - name: backup
          persistentVolumeClaim:
            claimName: redis-data-{{ .Values.redis.fullnameOverride }}-master-0-backup-{{ sub .Release.Revision 1 }}
{{- end }}



- creation du hook de pre-rollback : 

sous le rep templates : mkdir restore

cat job.yaml                                                                                            

{{- if .Values.redis.master.persistence.enabled }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "guestbook.fullname" . }}-restore
  labels:
    {{- include "guestbook.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": pre-rollback   <<<< on defini un hook de pre rollback 
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  template:
    spec:
      containers:
        - name: restore
          image: redis:alpine3.11
          command: ["/bin/sh", "-c"]
          args: ["cp /backup/dump.rdb /data/dump.rdb &&  <<<< ici on copie depuis notre pvc backup le snap et on reinjecte sur le master db , on restart la base redis.
            redis-cli -h {{ .Values.redis.fullnameOverride }}-master debug restart || true"]  <<< on s'assure que le code retour du redemarrage de la base est n'est pas ok  : on a un retour "normal" de redis qui nous dis que la connexion a la db s'est terminée ,on a un code retour different de 0 .. on rajoute un || true qui va donc sortir si on a un code retour à 0 ..ce qui prouverai que le redémarrage n'a pas eu lieu. 
          volumeMounts:
            - name: redis-data
              mountPath: /data
            - name: backup
              mountPath: /backup
      restartPolicy: Never
      volumes:
        - name: redis-data
          persistentVolumeClaim:
            claimName: redis-data-{{ .Values.redis.fullnameOverride }}-master-0
        - name: backup
          persistentVolumeClaim:
            claimName: redis-data-{{ .Values.redis.fullnameOverride }}-master-0-backup-{{ .Release.Revision }}  <<< on defini le volume de backup voulu avec le num de revision.
{{- end }}


- reinstall de notre chart avec les hooks :

helm install  guestbook .

on va tester qu'on a acces a l'appli et qu'on peut ecrire un message 

quand c'est ok on lance notre upgrade :

helm upgrade  guestbook .

on voit que notre pvc de backup  est créee :

kctl get pvc                                                                                                                [☸ |minikube:chapter5]
NAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
redis-data-redis-master-0            Bound    pvc-fcfc75d3-c4b3-4ca7-9d47-284fe8ab06a9   8Gi        RWO            standard       2m43s
redis-data-redis-master-0-backup-1   Bound    pvc-d6369288-924a-4399-b56b-547feffc2b3b   8Gi        RWO            standard       39s
redis-data-redis-slave-0             Bound    pvc-655ed8cc-b873-4c20-a474-2b2c61e15b58   8Gi        RWO            standard       2m43s
redis-data-redis-slave-1             Bound    pvc-7d523ddf-71f5-4295-8b9d-198ff635386c   8Gi        RWO            standard       2m2s

on ajoute un message dans le champ de notre appli :
on a deux messages visibles 

on va maintenant faire un rollback pour tester la resto de notre snap :

helm rollback   guestbook 1 .                                                                                               [☸ |minikube:chapter5]
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
Rollback was a success! Happy Helming!

on a donc trigger notre rollback qui a injecté le snap ..en examinant notre frontend on a plus que le message initial présent apres l'install de notre chart


= Ajout de controle de validation de saisie :

en travaillant avec kube et helm, des controles de saisies sont effectués par l'api de kube : si une ressource crée par helm est invalide : une erreur sera retournée par l'api de kube.
Il va rester des cas ou on va vouloir pendant le developpement de notre chart s'assurer de controle avant de pousser et atteindre l'api de kube.
On peut utiliser la fonction fail dans notre chart guestbook.

- function fail :

elle est utilisée pour echouer directement dans le process de templating si un user a saisi une mauvaise valeur par ex.

ex dans notre chart guestbook on a une valeur qui défini le type de service qui sera utilisé.
cat values.yaml
...
service:
  type: NodePort
  port: 80
...

ici on a defini NodePort (pour minikube) .
On pourrait vouloir limiter la saisi à NodePort ou ClusterIP par exemple.

si on regarde le template service.yaml, on a la section :
cat templates/service.yaml
..
spec:
  type: {{ .Values.service.type }}
...

on va vouloir qu'assurer que le service est défini à ClusterIP ou NodePort avant de définir le type de service.On va pouvoir définir une variable avec la liste des bon settings puis vérifier que la valeur de service.type est bien dans la liste de bon settings : sinon le processing de chart retournera une erreur sur le type de service saisi comme invalide.
Notre template va donc être structuré comme suivant :

apiVersion: v1
kind: Service
metadata:
  name: {{ include "guestbook.fullname" . }}
  labels:
    {{- include "guestbook.labels" . | nindent 4 }}
spec:
{{- $serviceTypes := list "ClusterIP" "NodePort" }}   <<<<< on defini une variable contenant la liste des type de services authorisés.
{{- if has .Values.service.type $serviceTypes }}      <<<<< on évalue la valeur définie pour .Values.service.type et on examine si on a bien une valeur a "ClusterIP" ou "NodePort"
  type: {{ .Values.service.type }}                    <<<<< si la valeur définie dans notre fichier de values est valide ("ClusterIP" ou "NodePort") : alors on défini notre type a la valeur saisie
{{- else }}                                       
  {{- fail "value 'service.type' must be either 'ClusterIP' or 'NodePort'" }}   <<<<<< sinon on lève une erreur en précisant que nous avons saisi une mauvaise valeur dans notre fichier pour le type de service.
{{- end }}

Si on test en définissant un service de type LoadBalancer on a bien une erreur de sortie :

helm upgrade guestbook . -n chapter5 --set service.type=LoadBalancer                                                        
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
Error: UPGRADE FAILED: template: guestbook/templates/service.yaml:12:6: executing "guestbook/templates/service.yaml" at <fail "value 'service.type' must be either 'ClusterIP' or 'NodePort'">: error calling fail: value 'service.type' must be either 'ClusterIP' or 'NodePort'


- require : fonction de validation de valeur saisie
on va aussi pouvoir s'assurer que les users ont bien saisi une valeur par defaut dans le fichier de values et qu'en aucun cas une valeur vide ne peut fonctionner 

exemple : on va pouvoir s'assurer qu'une image précise sra utilisée dans notre déploiement :

cat templates/deployment.yaml                                                                                     
          ..
          image: '{{ .Values.image.repository }}:{{ .Chart.AppVersion }}'
          #image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"   <<<<< valeur definie dans le tpls de base mais ne collant pas avec l'explication du book.
on va utiliser la fonction require pour nous assurer de l'tilisation d'une image précise :

require prend deux args :
> un message d'erreur pour afficher la valeur attendue 
> la valeur attendue

on aura dans notre cas donc :

          image: "{{ required "value 'image.repository' is required" .Values.image.repository }}:{{ .Chart.AppVersion }}"

si on essaye d'upgrader avec une valeur vide on aura un message d'erreur :

helm upgrade guestbook . -n chapter5 --set image.repository=''                                                            [☸ |minikube:chapter5]
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
Error: UPGRADE FAILED: execution error at (guestbook/templates/deployment.yaml:28:21): value 'image.repository' is required

= Publication de notre chart dans un repo : 

maintenant qu'on a un chart on va pouvoir le publier dans un repo pour le rendre dispo :

- creation de repo :

un repo helm contient :
> des chart sous la forme de tgz
> un fichier index.yaml contenant les metadata des charts présents dans le repo

un repo basic nécéssite qu'un maintener s'occupe de la gestion du fichier index.yaml.
Des solutions plus élaborées comme chartmuseum gèrent automatiquement ce fichier.
on va ici utiliser github pages pour faire un repo simple.

une fois notre compte crée, on cree un repo avec un readme et on active github pages dans les settings en choississant la branche main (anciennement master) et en choississant un theme graphique qui nous plait.

on clone en local de repo  dans une arbo voulu ex /tmp 

git clone https://github.com/billbongo/helm-charts.git

on va modifier la version du chart dans le Chart.yaml :
on passe en version 1.0.0 : une majeure. 

version: 1.0.0

on va maintenant packager notre chart :
cd ..
helm package guestbook


on a maintenant un chart tgz au même niveau que le repertoire de notre chart :

tree                                                                                                                               
.
├── guestbook
│   ├── Chart.lock
│   ├── charts
│   │   └── redis-10.5.14.tgz
│   ├── Chart.yaml
│   ├── helm-charts
│   │   └── README.md
│   ├── templates
│   │   ├── backup
│   │   │   ├── job.yaml
│   │   │   └── persistentvolumeclaim.yaml
│   │   ├── deployment.yaml
│   │   ├── _helpers.tpl
│   │   ├── hpa.yaml
│   │   ├── ingress.yaml
│   │   ├── NOTES.txt
│   │   ├── restore
│   │   │   └── job.yaml
│   │   ├── serviceaccount.yaml
│   │   └── service.yaml
│   └── values.yaml
└── guestbook-1.0.0.tgz


si on a un souci on peut s'assurer que les dépendances de notre chart sont bien présentes en ajoutant :
--dependency-update flag a la commande helm package ,ce qui a download les dependance et packager le chart dans la même commande 


on va maintenant copier le tgz dans notre repo git cloné :
cp guestbook-1.0.0.tgz /tmp/helm-charts

on va maintenant créer notre fichier index :
avec la commande helm repo index <nom du repo git cloné en local> 

helm repo index helm-charts                                     
 boogie@boogieland  /tmp  cd helm-charts                                                                                                                                                  [☸ |minikube:chapter5]
 boogie@boogieland  /tmp/helm-charts   main  ls                                                                                                                                         [☸ |minikube:chapter5]
guestbook-1.0.0.tgz  index.yaml  README.md
 boogie@boogieland  /tmp/helm-charts   main  cat index.yaml                                                                                                                             [☸ |minikube:chapter5]
apiVersion: v1
entries:
  guestbook:
  - apiVersion: v2
    appVersion: v4
    created: "2020-11-01T18:11:23.696154925+01:00"
    dependencies:
    - name: redis
      repository: https://charts.bitnami.com/bitnami
      version: 10.5.x
    description: A Helm chart for Kubernetes
    digest: a1bb696d44ebb3fdb7f0823b3f394b2b093712fda426081de08fec686403a725
    name: guestbook
    type: application
    urls:
    - guestbook-1.0.0.tgz
    version: 1.0.0
generated: "2020-11-01T18:11:23.691250105+01:00"

on ajoute les fichiers, on commit et on pousse dans le repo :

git add --all
git commit -m 'feat: adding the guestbook helm chart'
git push origin main

on va maintenant ajouter notre repo dans notre liste 
on doit récupérer l'url de github pages qu'on peut voir dans les settings 

https://billbongo.github.io/helm-charts/

helm repo add billbongo https://billbongo.github.io/helm-charts
"billbongo" has been added to your repositories


== Testing helm-charts ==

on va tester pour valider le fonctionnement de notre chart et eviter les régressions de code.
on va pouvoir vérifier de plusieurs manières :

on va créer un nouveau namespace pour tester notre chart 
kctl create ns chapter6 

= helm template :

une des premieres méthodes de test est de s'assurer que le processing des templates helm fonctionne bien et qu'on alimente correctement les données de nos fichiers values dans ces templates en testant juste sur la sortie standart. On s'assure egalement que nos section avec des if, range, with , fonctions, pipeline , indentations sont bien remplies. 

helm template <nom de release> <nom de chart>(path du chart)  

helm template myguestbook guestbook |head                   
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
---
# Source: guestbook/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myguestbook
  labels:
    helm.sh/chart: guestbook-1.0.0
    app.kubernetes.io/name: guestbook
    app.kubernetes.io/instance: myguestbook

on peut également tester le remplacement de nos valeurs de chartà la volée en utilisant --set :
ex: 
helm template my-chart $CHART_DIRECTORY --set replicas=2

ex on défini dans notre fichier de value un nombre de replicas a 1 
grep replica guestbook/values.yaml                                                                                                    [☸ |minikube:chapter6]
replicaCount: 1

on voit que dans le template de déploiement on a la valeur de notre fichier de value récupérée :

grep -r replicas *                                                                                                                    [☸ |minikube:chapter6]
guestbook/templates/deployment.yaml:  replicas: {{ .Values.replicaCount }}


on peut vouloir simuler la bonne prise en compte d'une autre valeur en cli :

helm template guestbook guestbook --set replicaCount=15

# Source: guestbook/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: guestbook
  labels:
..
.....
  replicas: 15

on peut aussi passer en argument un fichier de values pour voir si le rendering de template est correct : 
helm template guestbook guestbook --values guestbook/values.yaml |more                                                      
---
# Source: guestbook/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: guestbook
  labels:
    helm.sh/chart: guestbook-1.0.0
    app.kubernetes.io/name: guestbook
    app.kubernetes.io/instance: guestbook
    app.kubernetes.io/version: "v4"
..
.....

Il est important de tester régulièrement le rendu de nos templates avec helm template.
A noter que des valeurs d'un tableau et des valeurs de test (if) non définies dans nos fichiers de values ne génère aucune entrée dans le test de helm template : c'est un bon moyen de voir qu'on a rien oublier de définir dans nos fichiers de values.
 on peut mettre des commentaires explicites dans nos fichiers de values. 

ex :

#required :
image: repo/name

on peut tester les fonctions et pipeline également :
ex :

apiVersion: apps/v1
kind: Deployment
...  
  resources:
{{ .Values.resources | toYaml | indent 12 }}

dans notre fichier de values : 
resources:
  limits:
    cpu: 200m
    memory: 256Mi


= server side validation :

sans argument particulier helm template est fait pour vérifier le bon developpement de notre chart coté client.
On va pouvoir rajouter un flag pour s'assurer du rendu du chart en interrogeant l'api kube.

ex : 
helm template my-chart $CHART_DIRECTORY --validate
helm template guestbook guestbook --set replicaCount=2 --validate
si notre chart comporte une erreur exemple on met un mauvais num de version d'api dans le déploiment :

ex : 
apiVersion: apps/v5
kind: Deployment

...la commande template avec le flag --validate lévera une erreur car l'api kube attend une version correcte pour l'object déployment.
helm template guestbook guestbook --set replicaCount=0 --validate                                                              
Error: unable to build kubernetes objects from release manifest: unable to recognize "": no matches for kind "Deployment" in version "apps/v5"

on peut utiliser la commande install et --dry-run qui génère le template et passe l'option --validate :

helm install my-chart $CHART --dry-run

helm install --dry-run guestbook guestbook --set replicaCount=0                                                               
Error: unable to build kubernetes objects from release manifest: unable to recognize "": no matches for kind "Deployment" in version "apps/v5"

= Lint :

la commande helm lint va elle permettre de s'assurer que les fichiers sont correctement formatés (indentations ....) 
helm lint PATH [flags]
helm lint guestbook                                                                                                             
==> Linting guestbook
[INFO] Chart.yaml: icon is recommended

1 chart(s) linted, 0 chart(s) failed


on a ici un message d'info qui demande l'insertion d'une icon. Ceci n'est pas mandatory.
Les messages de warning et d'erreur doivent eux être traités.

Le linter va aussi s'assurer de la présence de fichiers mandatory,examiner les num de version d'api fournis, s'assurer que le num de version est bien compatible avec le standart semver.

on va pouvoir installer un linter qui va nous aider également :

pip install yamllint --user 

on peut combiner son utilisation avec la commande helm template pour générer nos ressources proprement indentées.

/!\ echec d'utilisation :

helm template mybook guestbook |yamllint                                                                                      
✖ YAML Lint failed.
✖ No YAML files were found matching your selection.

par contre yamllint ok sur fichier en argument :
yamllint guestbook/values.yaml    
✔ YAML Lint successful.

= test live dans cluster kube :

Il est important de créer des tests functionnels de nos charts pour nous assurer qu'il n'y a pas de régression et qu'on fixe bien ce que l'on veut.
Lss tests  consistent en deux etapes :
> créer un template de pod sous le rep templates qui contient une annotation helm.sh/hook: test
ces pods vont tester le fonctionnement de notre appli
> executer  la commande helm test sui va générer un hook qui va créer notre pod précédent.

Par mesures pratique on va stocker nos tests dans un repertoire tests placé dans le repertoire templates.

ex :
guestbook/templates
mkdir tests && touch tests/frontend-connection.yaml && touch tests/redis-connection.yaml

on va créer nos tests :

- le premier va tester qu'on a bien une connection fonctionnelle a notre db et qu'on recupere bien les messages : 

cat tests/backend-connection.yaml                                                                               
apiVersion: v1
kind: Pod
metadata:
  name: {{ include "guestbook.fullname" . }}-test-backend-connection
  labels:
    {{- include "guestbook.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  containers:
    - name: test-backend-connection
      image: redis:alpine3.11
      command: ["/bin/sh", "-c"]
      args: ["redis-cli -h {{ .Values.redis.fullnameOverride }}-master MGET messages"]
  restartPolicy: Never



- le second va tester qu'on a bien une reponse de nos frontends : en curlant l'ip de service du lb  
cat tests/frontend-connection.yaml   
apiVersion: v1
kind: Pod
metadata:
  name: {{ include "guestbook.fullname" . }}-test-frontend-connection
  labels:
    {{- include "guestbook.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  containers:
    - name: test-frontend-connection
      image: curlimages/curl:7.68.0
      command: ["/bin/sh", "-c"]
      args: ["curl {{ include "guestbook.fullname" . }}"]
  restartPolicy: Never

on va devoir installer si ce n'est fait notre chart avant de tester : 

helm install   my-guestbook .

on va maintenant tester notre chart :

helm test my-guestbook                                                                                           
Pod my-guestbook-test-backend-connection pending
Pod my-guestbook-test-backend-connection pending
Pod my-guestbook-test-backend-connection pending
Pod my-guestbook-test-backend-connection succeeded
Pod my-guestbook-test-frontend-connection pending
Pod my-guestbook-test-frontend-connection pending
Pod my-guestbook-test-frontend-connection succeeded
NAME: my-guestbook
LAST DEPLOYED: Sun Nov  8 17:03:04 2020
NAMESPACE: chapter6
STATUS: deployed
REVISION: 1
TEST SUITE:     my-guestbook-test-backend-connection
Last Started:   Sun Nov  8 17:08:57 2020
Last Completed: Sun Nov  8 17:08:58 2020
Phase:          Succeeded
TEST SUITE:     my-guestbook-test-frontend-connection
Last Started:   Sun Nov  8 17:08:58 2020
Last Completed: Sun Nov  8 17:09:01 2020
Phase:          Succeeded
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace chapter6 -o jsonpath="{.spec.ports[0].nodePort}" services my-guestbook)
  export NODE_IP=$(kubectl get nodes --namespace chapter6 -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT


on peut si on veut ajouter un mode plus verbeux et un niveau de logs bavard :
 helm test my-guestbook --logs         
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
Pod my-guestbook-test-backend-connection pending
Pod my-guestbook-test-backend-connection pending
Pod my-guestbook-test-backend-connection succeeded
Pod my-guestbook-test-frontend-connection pending
Pod my-guestbook-test-frontend-connection pending
Pod my-guestbook-test-frontend-connection succeeded
NAME: my-guestbook
LAST DEPLOYED: Sun Nov  8 17:03:04 2020
NAMESPACE: chapter6
STATUS: deployed
REVISION: 1
TEST SUITE:     my-guestbook-test-backend-connection
Last Started:   Sun Nov  8 17:10:33 2020
Last Completed: Sun Nov  8 17:10:34 2020
Phase:          Succeeded
TEST SUITE:     my-guestbook-test-frontend-connection
Last Started:   Sun Nov  8 17:10:34 2020
Last Completed: Sun Nov  8 17:10:35 2020
Phase:          Succeeded
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace chapter6 -o jsonpath="{.spec.ports[0].nodePort}" services my-guestbook)
  export NODE_IP=$(kubectl get nodes --namespace chapter6 -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT

POD LOGS: my-guestbook-test-backend-connection


POD LOGS: my-guestbook-test-frontend-connection
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
<html ng-app="redis">
  <head>
    <title>Guestbook</title>
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/angularjs/1.2.12/angular.min.js"></script>
    <script src="controllers.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/angular-ui-bootstrap/0.13.0/ui-bootstrap-tpls.js"></script>
  </head>
  <body ng-controller="RedisCtrl">
    <div style="width: 50%; margin-left: 20px">
      <h2>Guestbook</h2>
    <form>
    <fieldset>
    <input ng-model="msg" placeholder="Messages" class="form-control" type="text" name="input"><br>
    <button type="button" class="btn btn-primary" ng-click="controller.onRedis()">Submit</button>
    </fieldset>
    </form>
    <div>
      <div ng-repeat="msg in messages track by $index">
        {{msg}}
      </div>
    </div>
    </div>
  </body>
</html>
100   921  100   921    0     0   899k      0 --:--:-- --:--:-- --:--:--  899k


test avec niveau de verbosité augmenté : 

helm test my-guestbook --logs -v7       
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
I1108 17:11:01.695842  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.696896  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.697773  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.698638  125524 cert_rotation.go:137] Starting client certificate rotation controller
I1108 17:11:01.700110  125524 round_trippers.go:420] GET https://192.168.39.214:8443/version?timeout=32s
I1108 17:11:01.700120  125524 round_trippers.go:427] Request Headers:
I1108 17:11:01.700125  125524 round_trippers.go:431]     Accept: application/json, */*
I1108 17:11:01.700129  125524 round_trippers.go:431]     User-Agent: helm/v0.0.0 (linux/amd64) kubernetes/$Format
I1108 17:11:01.708476  125524 round_trippers.go:446] Response Status: 200 OK in 8 milliseconds
I1108 17:11:01.709122  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.710555  125524 round_trippers.go:420] GET https://192.168.39.214:8443/api/v1/namespaces/chapter6/secrets?labelSelector=name%3Dmy-guestbook%2Cowner%3Dhelm
I1108 17:11:01.710578  125524 round_trippers.go:427] Request Headers:
I1108 17:11:01.710583  125524 round_trippers.go:431]     User-Agent: helm/v0.0.0 (linux/amd64) kubernetes/$Format
I1108 17:11:01.710587  125524 round_trippers.go:431]     Accept: application/json, */*
I1108 17:11:01.713302  125524 round_trippers.go:446] Response Status: 200 OK in 2 milliseconds
I1108 17:11:01.715923  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.716981  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.720422  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.720839  125524 round_trippers.go:420] DELETE https://192.168.39.214:8443/api/v1/namespaces/chapter6/pods/my-guestbook-test-backend-connection
I1108 17:11:01.720851  125524 round_trippers.go:427] Request Headers:
I1108 17:11:01.720860  125524 round_trippers.go:431]     Accept: application/json
I1108 17:11:01.720867  125524 round_trippers.go:431]     Content-Type: application/json
I1108 17:11:01.737311  125524 round_trippers.go:446] Response Status: 200 OK in 16 milliseconds
I1108 17:11:01.738225  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.738627  125524 round_trippers.go:420] GET https://192.168.39.214:8443/openapi/v2?timeout=32s
I1108 17:11:01.738636  125524 round_trippers.go:427] Request Headers:
I1108 17:11:01.738642  125524 round_trippers.go:431]     Accept: application/com.github.proto-openapi.spec.v2@v1.0+protobuf
I1108 17:11:01.738648  125524 round_trippers.go:431]     User-Agent: helm/v0.0.0 (linux/amd64) kubernetes/$Format
I1108 17:11:01.743634  125524 round_trippers.go:446] Response Status: 200 OK in 4 milliseconds
I1108 17:11:01.808435  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.809202  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.810963  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:01.814274  125524 round_trippers.go:420] PUT https://192.168.39.214:8443/api/v1/namespaces/chapter6/secrets/sh.helm.release.v1.my-guestbook.v1
I1108 17:11:01.814290  125524 round_trippers.go:427] Request Headers:
I1108 17:11:01.814295  125524 round_trippers.go:431]     Content-Type: application/json
I1108 17:11:01.814299  125524 round_trippers.go:431]     Accept: application/json, */*
I1108 17:11:01.814302  125524 round_trippers.go:431]     User-Agent: helm/v0.0.0 (linux/amd64) kubernetes/$Format
I1108 17:11:01.817577  125524 round_trippers.go:446] Response Status: 200 OK in 3 milliseconds
I1108 17:11:01.818662  125524 round_trippers.go:420] POST https://192.168.39.214:8443/api/v1/namespaces/chapter6/pods
I1108 17:11:01.818677  125524 round_trippers.go:427] Request Headers:
I1108 17:11:01.818685  125524 round_trippers.go:431]     Accept: application/json
I1108 17:11:01.818690  125524 round_trippers.go:431]     Content-Type: application/json
I1108 17:11:01.823343  125524 round_trippers.go:446] Response Status: 201 Created in 4 milliseconds
I1108 17:11:01.823716  125524 round_trippers.go:420] GET https://192.168.39.214:8443/api/v1/namespaces/chapter6/pods?fieldSelector=metadata.name%3Dmy-guestbook-test-backend-connection
I1108 17:11:01.823730  125524 round_trippers.go:427] Request Headers:
I1108 17:11:01.823738  125524 round_trippers.go:431]     Accept: application/json
I1108 17:11:01.832003  125524 round_trippers.go:446] Response Status: 200 OK in 8 milliseconds
Pod my-guestbook-test-backend-connection pending
I1108 17:11:01.837754  125524 retrywatcher.go:244] Starting RetryWatcher.
I1108 17:11:01.837851  125524 round_trippers.go:420] GET https://192.168.39.214:8443/api/v1/namespaces/chapter6/pods?fieldSelector=metadata.name%3Dmy-guestbook-test-backend-connection&resourceVersion=371180&watch=true
I1108 17:11:01.837863  125524 round_trippers.go:427] Request Headers:
I1108 17:11:01.837870  125524 round_trippers.go:431]     Accept: application/json
I1108 17:11:01.838991  125524 round_trippers.go:446] Response Status: 200 OK in 1 milliseconds
Pod my-guestbook-test-backend-connection pending
Pod my-guestbook-test-backend-connection pending
Pod my-guestbook-test-backend-connection succeeded
I1108 17:11:02.751443  125524 retrywatcher.go:146] Stopping RetryWatcher.
I1108 17:11:02.751573  125524 retrywatcher.go:272] Stopping RetryWatcher.
I1108 17:11:02.752131  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:02.753055  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:02.755448  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:02.755833  125524 round_trippers.go:420] DELETE https://192.168.39.214:8443/api/v1/namespaces/chapter6/pods/my-guestbook-test-frontend-connection
I1108 17:11:02.755844  125524 round_trippers.go:427] Request Headers:
I1108 17:11:02.755849  125524 round_trippers.go:431]     Accept: application/json
I1108 17:11:02.755853  125524 round_trippers.go:431]     Content-Type: application/json
I1108 17:11:02.774864  125524 round_trippers.go:446] Response Status: 200 OK in 18 milliseconds
I1108 17:11:02.776695  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:02.778474  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:02.779811  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:02.783136  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:02.786099  125524 round_trippers.go:420] PUT https://192.168.39.214:8443/api/v1/namespaces/chapter6/secrets/sh.helm.release.v1.my-guestbook.v1
I1108 17:11:02.786120  125524 round_trippers.go:427] Request Headers:
I1108 17:11:02.786130  125524 round_trippers.go:431]     Accept: application/json, */*
I1108 17:11:02.786138  125524 round_trippers.go:431]     Content-Type: application/json
I1108 17:11:02.786145  125524 round_trippers.go:431]     User-Agent: helm/v0.0.0 (linux/amd64) kubernetes/$Format
I1108 17:11:02.796165  125524 round_trippers.go:446] Response Status: 200 OK in 10 milliseconds
I1108 17:11:02.796975  125524 round_trippers.go:420] POST https://192.168.39.214:8443/api/v1/namespaces/chapter6/pods
I1108 17:11:02.796993  125524 round_trippers.go:427] Request Headers:
I1108 17:11:02.797003  125524 round_trippers.go:431]     Accept: application/json
I1108 17:11:02.797013  125524 round_trippers.go:431]     Content-Type: application/json
I1108 17:11:02.801998  125524 round_trippers.go:446] Response Status: 201 Created in 4 milliseconds
I1108 17:11:02.802471  125524 round_trippers.go:420] GET https://192.168.39.214:8443/api/v1/namespaces/chapter6/pods?fieldSelector=metadata.name%3Dmy-guestbook-test-frontend-connection
I1108 17:11:02.802492  125524 round_trippers.go:427] Request Headers:
I1108 17:11:02.802503  125524 round_trippers.go:431]     Accept: application/json
I1108 17:11:02.828145  125524 round_trippers.go:446] Response Status: 200 OK in 25 milliseconds
Pod my-guestbook-test-frontend-connection pending
I1108 17:11:02.828996  125524 retrywatcher.go:244] Starting RetryWatcher.
I1108 17:11:02.829103  125524 round_trippers.go:420] GET https://192.168.39.214:8443/api/v1/namespaces/chapter6/pods?fieldSelector=metadata.name%3Dmy-guestbook-test-frontend-connection&resourceVersion=371192&watch=true
I1108 17:11:02.829119  125524 round_trippers.go:427] Request Headers:
I1108 17:11:02.829129  125524 round_trippers.go:431]     Accept: application/json
I1108 17:11:02.830364  125524 round_trippers.go:446] Response Status: 200 OK in 1 milliseconds
Pod my-guestbook-test-frontend-connection pending
Pod my-guestbook-test-frontend-connection succeeded
I1108 17:11:03.778710  125524 retrywatcher.go:146] Stopping RetryWatcher.
I1108 17:11:03.778788  125524 retrywatcher.go:272] Stopping RetryWatcher.
I1108 17:11:03.781352  125524 round_trippers.go:420] PUT https://192.168.39.214:8443/api/v1/namespaces/chapter6/secrets/sh.helm.release.v1.my-guestbook.v1
I1108 17:11:03.781372  125524 round_trippers.go:427] Request Headers:
I1108 17:11:03.781381  125524 round_trippers.go:431]     User-Agent: helm/v0.0.0 (linux/amd64) kubernetes/$Format
I1108 17:11:03.781389  125524 round_trippers.go:431]     Accept: application/json, */*
I1108 17:11:03.781396  125524 round_trippers.go:431]     Content-Type: application/json
I1108 17:11:03.785335  125524 round_trippers.go:446] Response Status: 200 OK in 3 milliseconds
NAME: my-guestbook
LAST DEPLOYED: Sun Nov  8 17:03:04 2020
NAMESPACE: chapter6
STATUS: deployed
REVISION: 1
TEST SUITE:     my-guestbook-test-backend-connection
Last Started:   Sun Nov  8 17:11:01 2020
Last Completed: Sun Nov  8 17:11:02 2020
Phase:          Succeeded
TEST SUITE:     my-guestbook-test-frontend-connection
Last Started:   Sun Nov  8 17:11:02 2020
Last Completed: Sun Nov  8 17:11:03 2020
Phase:          Succeeded
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace chapter6 -o jsonpath="{.spec.ports[0].nodePort}" services my-guestbook)
  export NODE_IP=$(kubectl get nodes --namespace chapter6 -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT

I1108 17:11:03.786415  125524 loader.go:375] Config loaded from file:  /home/boogie/.kube/config
I1108 17:11:03.787809  125524 round_trippers.go:420] GET https://192.168.39.214:8443/api/v1/namespaces/chapter6/pods/my-guestbook-test-backend-connection/log
I1108 17:11:03.787826  125524 round_trippers.go:427] Request Headers:
I1108 17:11:03.787838  125524 round_trippers.go:431]     Accept: application/json, */*
I1108 17:11:03.787846  125524 round_trippers.go:431]     User-Agent: helm/v0.0.0 (linux/amd64) kubernetes/$Format
I1108 17:11:03.795397  125524 round_trippers.go:446] Response Status: 200 OK in 7 milliseconds
POD LOGS: my-guestbook-test-backend-connection
,bob,lapin   <<<<<<<<<<<<<<<<<<< ici on voit les messages qui sont affichés en db ( ce qu'on a saisi dans notre formulaire web au préalable. )


I1108 17:11:03.813691  125524 round_trippers.go:420] GET https://192.168.39.214:8443/api/v1/namespaces/chapter6/pods/my-guestbook-test-frontend-connection/log
I1108 17:11:03.813704  125524 round_trippers.go:427] Request Headers:
I1108 17:11:03.813714  125524 round_trippers.go:431]     Accept: application/json, */*
I1108 17:11:03.813724  125524 round_trippers.go:431]     User-Agent: helm/v0.0.0 (linux/amd64) kubernetes/$Format
I1108 17:11:03.823649  125524 round_trippers.go:446] Response Status: 200 OK in 9 milliseconds
POD LOGS: my-guestbook-test-frontend-connection
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
<html ng-app="redis">
  <head>
    <title>Guestbook</title>
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/angularjs/1.2.12/angular.min.js"></script>
    <script src="controllers.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/angular-ui-bootstrap/0.13.0/ui-bootstrap-tpls.js"></script>
  </head>
  <body ng-controller="RedisCtrl">
    <div style="width: 50%; margin-left: 20px">
      <h2>Guestbook</h2>
    <form>
    <fieldset>
    <input ng-model="msg" placeholder="Messages" class="form-control" type="text" name="input"><br>
    <button type="button" class="btn btn-primary" ng-click="controller.onRedis()">Submit</button>
    </fieldset>
    </form>
    <div>
      <div ng-repeat="msg in messages track by $index">
        {{msg}}
      </div>
    </div>
    </div>
  </body>
</html>
100   921  100   921    0     0   899k      0 --:--:-- --:--:-- --:--:--  899k


= Amélioration de test avec le project de chart testing : 

on a des limitations avec la commande test : si on va pouvoir tester des éléments précis on est soumis a un workflow assez lourd pour evaluer une nouvelle valeur par exemple :

1. Install your chart with an initial set of values.
2. Run helm test against your release.
3. Delete your release.
4. Install your chart with a different set of values.
5. Repeat steps 2 through 4 until a significant amount of value possibilities are tested.

idem pour les upgrade on a un workflow pénible à gérer :

1. Install a previous chart version.
2. Upgrade your release to the newer chart version.
3. Delete the release.
4. Install the newer chart version.


si on a plusieurs charts à maintenir on va utiliser un repo git de type monorepo : les charts auront une structure identique :
exemple : 

helm-charts/
  guestbook/
    Chart.yaml
    templates/
    README.md
    values.yaml
  redis/           # Contains the same file structure as 'guestbook'
  wordpress/       # Contains the same file structure as 'guestbook'
  README.md

quand on va faire evoluer un chart on va modifier le semver en fonction dy type de modif (MAJOR.MINOR.PATCH version numbering format.) dans le fichier Chart.yaml
il faut nous assurer que tout fonctionne dans l'arbo du repo 

• Increment the MAJOR version if you are making a breaking change to your chart. A breaking change is a change that is not backward-compatible with the previous chart version.
• Increment the MINOR version if you are adding a feature but you are not making a breaking change. You should increment this version if the change you are making is backward-compatible with the previous chart version.
• Increment the PATCH version if you are making a patch to a bug or a security vulnerability that will not result in a breaking change. This version should be incremented if the change is backward-compatible with the previous chart version.

il faut donc nous assurer que les num de versions sont automatiquement incrémentés de manière auto.
La communauté a crée un outil ct pour structurer et automatiser les tests et la maintenance.
https://github.com/helm/chart-testing


git clone https://github.com/helm/chart-testing

Les tests auto vont utiliser git pour voir les modifs sur une branche cible : si le chart n'a pas bouger pas de test, sinon les tests sont éxécutés.
ct va dont :
• lint : Lints and validates charts that have been modified
• install : Installs and tests charts that have been modified
• lint-and-install : Lints, installs, and tests charts that have been modified
• list-changed : Lists charts that have been modified


list-changed ne fait pas de test.
lint-and-install Lints, installs, and tests et s'assure que le num de version a été changé : si le num de version n'a pas été changé mais que le contenu du chart si : alors il y'a un fail.

on va egalement pouvoir tester différents fichiers de values.
On va pouvoir créer des values dédiées à notre ci avec l'arbo suivante par exemple : ce qui permettra d'overrider / preserver ce qu'il y a dans le fichier de values original.

guestbook/
  Chart.yaml
  ci/
    nodeport-service-values.yaml
    ingress-values.yaml
  templates/
  values.yaml


La commande principale que l'on va utiliser est lint-and-install
command. 
The following lists the steps that this command uses to lint, install, and test
charts that are modified in a git monorepo:
1. Detect the charts that have been modified.
2. Update the local Helm cache with the helm repo update command.
3. Download each modified chart's dependencies with the helm dependency
build command.
4. Check whether each modified chart's version has been incremented.
5. For each chart that evaluates to true in step 4, lint the chart and each values file
under the ci/ folder.
6. For each chart that evaluates to true in step 4, perform the following
additional steps:
Install the chart in an automatically created namespace.
Run tests by executing helm test .
Delete the namespace.
Repeat for each values file under the ci/ folder.

As you can see, this command performs a variety of different steps to ensure that your charts are properly linted and tested by installing and testing each modified chart in a separate namespace, repeating the process for each values file defined under the ci/ folder. 
However, by default, the lint-and-install command does not check for backward compatibility by performing an upgrade from an older version of the chart. This feature can be enabled by adding the --upgrade flag:
If a breaking change is not indicated, the --upgrade flag modifies step 6 of the previous set of steps by running the following steps:

1. Install the older version of the chart in an automatically created namespace.
2. Run tests by executing helm test .
3. Upgrade the release to the modified version of the chart and run the tests again.
4. Delete the namespace.
5. Install the modified version of the chart in a new, automatically created namespace.
6. Run tests by executing helm test .
7. Upgrade the release again using the same chart version and rerun the tests.
8. Delete the namespace.
9. Repeat for each values file under the ci/ folder.

It is recommended that you add the --upgrade flag to perform additional testing on
Helm upgrades and to prevent possible regressions.

Important note:
The --upgrade flag will not take effect if you have incremented the MAJOR version of your Helm chart as this indicates that you made a breaking change and that an in-place upgrade on this version would not be successful.

- Installing the chart testing : 

pre requi :

helm / git /yamllint/ yamale /kubectl

yamale : outil qui va valider le Chart.yaml files avec un Chart.yaml "schema file".

pip install yamale --user
Collecting yamale
  Downloading yamale-3.0.4.tar.gz (30 kB)
Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from yamale) (5.3.1)
Building wheels for collected packages: yamale
  Building wheel for yamale (setup.py) ... done
  Created wheel for yamale: filename=yamale-3.0.4-py3-none-any.whl size=48030 sha256=cbe2199a8eb99a9bf16d5dc49e51de0012f505c04c322498491198d9f61cbd1b
  Stored in directory: /home/boogie/.cache/pip/wheels/76/b8/66/e3e6d60082384b3e52071ad81b3f3ed8aac98cb7f62a6033fb
Successfully built yamale
Installing collected packages: yamale
Successfully installed yamale-3.0.4

on recupere un tgz de ct : 
https://github.com/helm/chart-testing/releases

wget  https://github.com/helm/chart-testing/releases/download/v3.3.0/chart-testing_3.3.0_linux_amd64.tar.gz

tar -xzvf chart-testing_3.3.0_linux_arm64.tar.gz        
LICENSE
README.md
etc/chart_schema.yaml
etc/lintconf.yaml
ct

on supprime le fichier de licence et le README.md. On déplace les fichiers yaml dans un rep qu'on crée pour l'occasion et on deplace le binaire dans un rep du path 
mkdir $HOME/.ct
mv etc/* $HOME/.ct/ 
ls ~/.ct      
chart_schema.yaml  lintconf.yaml
sudo mv ct /usr/local/bin

ct  version                                                                                                                                   [☸ |minikube:chapter6]
Version:	 v3.3.0
Git commit:	 2a4dfd420d066f821b39724fba89133f930a9953
Date:		 2020-11-03T19:17:04Z
License:	 Apache 2.0

tests avec ct dans un repos "monorepo" contenant plusieurs charts : 

cat ct.yaml       
chart-dirs:                     <<<< ici on indique que le chart dir pour ct est le path relatif au fichier ct.yaml
  - helm-charts/charts

chart-repos:                    <<<< le chart repos indique une liste de repo qui doivent être ajoutés pour être sur de pouvoir dl les dependances de charts.
  - bitnami=https://charts.bitnami.com/bitnami

validate-maintainers: false

on peut definir beaucoup d'options supplémentaires pour la conf de l'outil : voir les options en examinant le repo  https://github.com/helm/chart-testing

on va se mettre a la racine de notre repo qui va contenir plusieurs charts comme definis dans le path du fichier : ct.yaml 
ls                                                                                                             [☸ |minikube:chapter6]
chart_schema.yaml  ct.yaml  guestbook-operator  helm-charts  jenkins  LICENSE  nginx-cd  README.md
 boogie@boogieland  ~/Documents/lab/helm/learn_helm/Learn-Helm   master  ls helm-charts                                                                                                 [☸ |minikube:chapter6]
charts  Jenkinsfile
 boogie@boogieland  ~/Documents/lab/helm/learn_helm/Learn-Helm   master  ls helm-charts/charts                                                                                          [☸ |minikube:chapter6]
guestbook  nginx

on a donc deux charts .

On lance la commande : 

ct lint-and-install                                                                                          [☸ |minikube:chapter6]
Linting and installing charts...
Using config file:  /home/boogie/Documents/lab/helm/learn_helm/Learn-Helm/ct.yaml
------------------------------------------------------------------------------------------------------------------------
 Configuration
------------------------------------------------------------------------------------------------------------------------
Remote: origin
TargetBranch: master
Since: HEAD
BuildId: 
LintConf: /home/boogie/.ct/lintconf.yaml
ChartYamlSchema: /home/boogie/.ct/chart_schema.yaml
ValidateMaintainers: false
ValidateChartSchema: true
ValidateYaml: true
CheckVersionIncrement: true
ProcessAllCharts: false
Charts: []
ChartRepos: [bitnami=https://charts.bitnami.com/bitnami]
ChartDirs: [helm-charts/charts]
ExcludedCharts: []
HelmExtraArgs: 
HelmRepoExtraArgs: []
Debug: false
Upgrade: false
SkipMissingValues: false
Namespace: 
ReleaseLabel: app.kubernetes.io/instance
------------------------------------------------------------------------------------------------------------------------
Error: Invalid Semantic Version
Invalid Semantic Version


on a donc des erreurs dans le versionning d'un ou de plusieurs de nos charts :

ex  :
version: v4

> on change en mettant un semver et c'est ok 

version: 1.1.0

ct lint-install 


Si on modifie ne serais que basiquement dans la description d'un chart son role et qu'on ne modifie pas le num de version on a une erreur.

Si on veut upgrader notre chart on va pouvoir le faire en utilisant le flag --upgrade :

ct lint-and-install --upgrade

Dans ce cas nous devons upgrader la version MAJOR de notre Chart.yaml version sinon on a une erreur.


=== automating helm : ci / cd /gitops ===

Il va être fastidieux de maintenir manuellement plusieurs charts, les tester et déployer le code à chaque modif.On va pouvoir intégrer les concepts de ci / cd et gitops pour builder, tester et packager nos charts.
On va devoir s'assurer pour les déployments en production que les bonnes pratiques de developpement de charts sont respectées, et s'assurer de la bonne implication de toutes les personnes travaillant sur le chart.
L'utilisation de concept comme ci / cd va permettre la répétition de tâches helm en cli pour prendre de bonnes habitudes.
Le but de la ci va être de supprimer le fameux "c'a marche sur mon pc." ...
on va donc utiliser un vcs (code versionning system) pour stocker le code source de notre helm.
Les taches de test seront automatisées par la ci et le developpement pur sera augmenté. Une notification est souvent envoyée pour avertir du resultat du test afin d'avertir le / les devs du chart.
On va incorporer le déployement dans la même idée que la ci et donc déployer de nouvelles versions de nos applis avec les mêmes concepts.
"The process of implementing a fully automated build, test, deployment, and release process without any human intervention is known as continuous deployment."

kube utilise un language déclaratif dont le code sera stocké dans git puis seront ensuite excutés le build, test, deployment process ..
Des outils comme Argocd et Flux ont été dev pour permettre de s'integrer avec kube : ces outils srcutent les repos git et controlle que la conf du cluster kube correspond aux valeurs définies dans notre repo giten utilisant le referentiel défini comme source (branche , tag etc ...)

- setting de l'env :

on va utiliser jenkins pour nos tests ci / cd avec un minikube comportant 4gb de ram 

> check de l'env minikube :
minikube config view                                                                                                                                  [☸ |minikube:default]
- memory: 4000
- vm-driver: kvm2
> si on a pas 4 gb de ram : on delete puis on rebuild minikube avec les bons params :

minikube delete
minikube start --memory=4g

creation d'un namespace dédié :
kubectl create ns chapter7

on  fork le projet sur github > clic sur icone fork > https://github.com/billbongo/-Learn-Helm
on clone en local le repo dans un rep nommé spécialement : 

git clone https://github.com/billbongo/-Learn-Helm.git learn_helm_fork

on va delete le chart initial poussé sur github : on va clean le repo initial 

billbongo/helm-charts/guestbook   main  rm -f guestbook-1.0.0.tgz index.yaml
git commit -am 'clean up bedore ci' && git push origin main

= creation d'un pipeline de CI : =

The concept of CI can be applied to the perspective of a chart developer who builds, tests,
packages, and releases Helm charts to a chart repository.

Workflow de developpement : 

The following steps outline an example CI workflow using the commands and tooling . It will assume that the resulting charts are saved in
a GitHub Pages repository:

1. A chart developer makes a code change to a chart or a set of charts in a
git monorepo.
2. The developer pushes the change(s) to the remote repository.
3. The charts that have been modified are automatically linted and tested in a
Kubernetes namespace by running the ct lint and ct install commands.
4. If linting and testing is successful, the charts are automatically packaged with the
helm package command.

- jenkins :

server qui va exécuter des taches et workflows regulierement utilisé pour créer des pipelines de ci / cd en utilisant une feature de jenkins qui decrit le pipeline sous forme dans un fichier nommé Jenkinsfile utilisant le languange DSL (description specific language) groovy (language similaire à java mais utilisable en modele de scripting pour faciliter la lecture et la comprehension du code.)
quand un Jenkinsfile est crée on a donc de défini toutes les étapes de nos workflows qui seront exécutées sur le server jenkins ou sur un agent dédié à ce job.
Dans kube on peut instancier des agents qui des que la tache est finie sont détruits. Ce qui permet de toujours demarrer avec un agent from scratch.
Jenkins permet d'utiliser la methode gitops en scannant les branch d'un repo à la decouverte des jenkinsfile : des qu'un fichier est trouvé : un nouveau job est crée d'apres le clone de la branche concernée : cela permet de tester de nouvelles features ou fix comme les jobs sont crées automatiquement d'après leur branches.

Installation :

ajout de repo :

helm repo add codecentric https://codecentric.github.io/helm-charts                 [☸ |minikube:chapter7]
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
"codecentric" has been added to your repositories

Since configuring these values requires a deeper understanding of Jenkins that is beyond
the scope of this book, a values file is provided for you that will automatically prepare
the following Jenkins configurations:
• Add relevant Jenkins plugins that are not included in the base image.
• Configure the credentials required to authenticate with GitHub.
• Configure a Jenkins agent specifically designed for testing and installing Helm
charts.
• Configure Jenkins to automatically create a new job based on the presence of the
Jenkinsfile file.
• Skip manual prompts that normally occur on the startup of a new installation.
• Disable authentication to simplify Jenkins access for this chapter.
The values file will also configure the following Kubernetes-related details:
• Set resource limits against the Jenkins server.
• Set the Jenkins service type to NodePort.
• Create the ServiceAccounts and RBAC rules required for Jenkins and Jenkins agents
to run jobs and deploy Helm charts in the Kubernetes environment.
• Set the Jenkins PersistentVolumeClaim size to 2Gi.

on va recupérer un fichier de values avec le setting correct :
on remarquera la section en commentaire de la partie fileContent permettant d'utiliser du templating go dans un fichier de values.

resources:
  limits:
    cpu: 500m
    memory: 2Gi
service:
  master:
    type: NodePort
serviceAccount:
  master:
    create: true
  agent:
    create: true
rbac:
  master:
    create: true
    rules:
      - apiGroups:
          - ""
        resources:
          - pods
        verbs:
          - create
          - delete
          - get
          - list
          - watch
      - apiGroups:
          - ""
        resources:
          - pods/exec
        verbs:
          - get
  agent:
    create: true
    rules:
      - apiGroups:
          - ""
        resources:
          - pods
          - pods/log
          - services
          - configmaps
          - secrets
          - serviceaccounts
          - persistentvolumeclaims
          - namespaces
          - events
        verbs:
          - get
          - list
          - update
          - watch
          - create
          - delete
      - apiGroups:
          - "apps"
        resources:
          - statefulsets
          - deployments
          - replicasets
        verbs:
          - get
          - list
          - update
          - watch
          - create
          - delete
persistence:
  size: 2Gi
javaOpts: >-
  -Dhudson.slaves.NodeProvisioner.initialDelay=0
  -Dhudson.model.LoadStatistics.decay=0.7
  -Dhudson.slaves.NodeProvisioner.MARGIN=30
  -Dhudson.slaves.NodeProvisioner.MARGIN0=0.6
  -XX:+UnlockExperimentalVMOptions
  -XX:+UseCGroupMemoryLimitForHeap
  -XX:MaxRAMFraction=2
  -XshowSettings:vm
  -Djenkins.install.runSetupWizard=false
referenceContent:
  - data:
      - fileName: plugins.txt
        fileContent: |
          configuration-as-code:1.35
          configuration-as-code-support:1.18
          git:4.1.1
          job-dsl:1.76
          kubernetes:1.23.2
          workflow-aggregator:2.6
      - fileName: jenkins.yaml
        fileContent: |
          credentials:
            system:
              domainCredentials:
                - credentials:
                    - usernamePassword:
                        scope: GLOBAL
                        id: github-auth
                        username: {{ required "value 'githubUsername' is required" .Values.githubUsername }}     <<<<<<<< templating go dans un values utilisé grace a la fonction fileContent: | ex dans la partie template : {{- tpl .Values.fileContent }}  : ce qui va permettre d'utiliser du templating go dans un fichier values.yaml
                        password: {{ required "value 'githubPassword' is required" .Values.githubPassword }}
                        description: Password to authenticate with GitHub
          jenkins:
            numExecutors: 0
            clouds:
              - kubernetes:
                  name: kubernetes
                  serverUrl: https://kubernetes.default
                  namespace: {{ .Release.Namespace }}
                  jenkinsUrl: http://{{ include "jenkins.fullname" . }}-master:{{ .Values.service.master.port }}
                  jenkinsTunnel: {{ include "jenkins.fullname" . }}-agent:{{ .Values.service.agent.port }}
                  templates:
                    - name: chart-testing-agent
                      label: chart-testing-agent
                      idleMinutes: 0
                      namespace: {{ .Release.Namespace }}
                      nodeUsageMode: NORMAL
                      serviceAccount: {{ if .Values.serviceAccount.agent.name }}{{ .Values.serviceAccount.agent.name }}{{ else }}{{ include "jenkins.fullname" . }}-agent{{ end }}
                      containers:
                        - name: chart-testing
                          image: quay.io/helmpack/chart-testing:v3.0.0-beta.1
                          ttyEnabled: true
                          resourceLimitCpu: 200m
                          resourceLimitMemory: 500Mi
                      envVars:
                        - envVar:
                            key: GITHUB_PAGES_REPO_URL
                            value: {{ required "value 'githubPagesRepoUrl' is required" .Values.githubPagesRepoUrl }}
                        - envVar:
                            key: GITHUB_PAGES_SITE_URL
                            value: {{ .Values.githubPagesSiteUrl }}
          jobs:
            - script: |-
                multibranchPipelineJob('Test and Release Helm Charts') {
                  factory {
                    workflowBranchProjectFactory {
                      scriptPath('helm-charts/Jenkinsfile')
                    }
                  }
                  branchSources {
                    git {
                      id('test')
                      remote({{ required "value 'githubForkUrl' is required" .Values.githubForkUrl | quote }})
                      credentialsId('github-auth')
                    }
                  }
                  orphanedItemStrategy {
                    discardOldItems {
                      numToKeep(10)
                    }
                  }
                }
                {{- /*
                The below job will be configured if the githubPagesSiteUrl value has been configured.
                This will imply that the reader is on the CD portion of Chapter 7, and that this
                  job is now relevant and should be displayed.
                */ -}}
                {{- if .Values.githubPagesSiteUrl }}
                multibranchPipelineJob('Deploy NGINX Chart') {
                  factory {
                    workflowBranchProjectFactory {
                      scriptPath('nginx-cd/Jenkinsfile')
                    }
                  }
                  branchSources {
                    git {
                      id('test')
                      remote({{ required "value 'githubForkUrl' is required" .Values.githubForkUrl | quote }})
                      credentialsId('github-auth')
                    }
                  }
                  orphanedItemStrategy {
                    discardOldItems {
                      numToKeep(10)
                    }
                  }
                }
                {{- end }}


lancement de l'installation du helm en passant en argument les valeurs avec --set . On peut crée un fichier de values mais attention a ne pas le commiter a cause des infos de connection 

helm install jenkins codecentric/jenkins -n chapter7 --version 1.5.1  --values learn_helm_fork/jenkins/values.yaml --set githubUsername=bob --set githubPassword=xxx --set githubForkUrl=https://github.com/billbongo/-Learn-Helm.git --set githubPagesRepoUrl=https://github.com/billbongo/Learn-Helm.git
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
NAME: jenkins
LAST DEPLOYED: Wed Nov 11 17:26:56 2020
NAMESPACE: chapter7
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
************************************************************************
*                                                                      *
*                 Jenkins Helm Chart by codecentric AG                 *
*                                                                      *
************************************************************************

In case the chart was installed with the default configuration and you did not
configure an admin user, Jenkins creates one per default. The initial password
is logged and also stored in '/var/jenkins_home/secrets/initialAdminPassword'.

Use the following command to retrieve it:

export POD_NAME=$(kubectl get pods --namespace chapter7 -l "app.kubernetes.io/name=jenkins,app.kubernetes.io/instance=jenkins" -o jsonpath="{.items[0].metadata.name}")
kubectl exec --namespace chapter7 "$POD_NAME" cat /var/jenkins_home/secrets/initialAdminPassword


Accessing your Jenkins server:

export NODE_PORT=$(kubectl get service --namespace chapter7 -o jsonpath="{.spec.ports[0].nodePort}" jenkins-master)
export NODE_IP=$(kubectl get nodes --namespace chapter7 -o jsonpath="{.items[0].status.addresses[0].address}")
echo "http://$NODE_IP:$NODE_PORT"

apres un petit moment (~10 mnts ) jenkins est accessible via l'url identifiée http://$NODE_IP:$NODE_PORT
http://192.168.39.214:30441/

on voit un job deja configuré . On remarque le status de l'executeur a gauche aussi.

- revue principale du fonctionnement du pipeline via l'examen des grandes section du jenkinsfile qui seront alimentées en partie par le fichier de values de jenkins affiché précedemment : 

> agent { label "chart-testing-agent" }  :  la premiere chose qui est faite est la creation de l'agent "chart-testing-agent" qui va utiliser l'image  quay.io/helmpack/chart-testing contenant les outils de tests (yamale yamllint  ct ..) 
 quand un agent run il clone notre repo fork avec l'argument "githubForkUrl" en utilisant githubUsername et  githubPassword pour l'authent : ceci est fait de manière implicite par jenkins.
 sans code additionnel dans le jenkinsfile.
des le clone du repo fait l'agent commence les stage du jenkinsfile et commence donc a tester. les  stages servent a regrouper les etapes du job de maniere logique.

agent { label "chart-testing-agent" }    <<<<<<  la premiere chose qui est faite est la creation de l'agent 

> la premiere etape du stage est le lint et le container "chart-testing" va lancer un shell puis "ct lint" > sh "ct lint"

    stages {
        stage("Lint") {
            steps {
                container("chart-testing") {
                    sh "ct lint"
                }

> on poursuit apres le test si c'est ok avec le stage "Install & Test" >  sh 'ct install --upgrade' qui va installer la version du chart sur la branch master et s'assurer du bon set up.

        stage("Install & Test") {
            steps {
                container("chart-testing") {
                    sh "ct install --upgrade"
                }
            }
Nb :on aura pu réunir les deux etapes précédentes en une seule avec "ct lint-and-install --upgrade" ..mais pour plus de visibilité c'est mieux de séparer.

> on va maintenant que tout est ok passer au stage suivant "Package Charts"" : packager notre chart : sh 'helm package --dependency-update helm-charts/charts/*'

        stage("package charts") {
            steps {
                script {
                    container("chart-testing") {
			sh "helm package --dependency-update helm-charts/charts/*"
                    }
                }


> si c'est ok on va pousser dans notre repo  :
on va decomposer les etapes : 
on doit pousser le helm chart dans le repo github pages ( qui est different du repo git de base) pour publier et avoir l'index.yaml. 
On clone ce repo , a ce moment la la variable repoType est défini en fonction de la branche  sur laquelle la ci /cd run afin de déterminer si le chart packagé doit etre poussé en stable ou staging .
ceci implique que pour stable le chart a été testé, validé et mergé sur la branche master.Staging implique que le chart est en dev et pas publié sur la branche master.
on va pouvoir créer deux repertoires distincts pour héberger les charts stable ou en dev :
ex : 
Learn-Helm-Repository/
  stable/
  staging


        stage("Push Charts to Chart Repo") {
            steps {
                script {
                    container("chart-testing") {
                        // Clone GitHub Pages repository to a folder called "chart-repo"
                        sh "git clone ${env.GITHUB_PAGES_REPO_URL} chart-repo"

                        // Determine if these charts should be pushed to "stable" or "staging" based on the branch
                        def repoType
                        if (env.BRANCH_NAME == "master") {
                            repoType = "stable"
                        } else {
                            repoType = "staging"
                        }

                        // Create the corresponding "stable" or "staging" folder if it does not exist
                        def files = sh(script: "ls chart-repo", returnStdout: true)
                        if (!files.contains(repoType)) {
                            sh "mkdir chart-repo/${repoType}"
                        }


on poursuit le stage avec la copie des charts packagés dans les repo cibles (stable ou staging ) et on lance un helm repo index pour mettre a jour les fichiers index de nos charts ( pas nécéssaire quand on utilise un chartmuseum pour mémo repo chart maintenu par la communauté helm.)  
on va ensuite preparer la conf git pour pousser (username et email adresse) puis pousser la conf . Les credentials ne vont pas être en clair car enregistrés dans "github-auth" lors du set up de notre chart.


// Move charts from the packaged-charts folder to the
corresponding 'stable' or 'staging' folder
sh "mv packaged-charts/*.tgz chart-repo/${repoType}"
// Generate the updated index.yaml
sh "helm repo index chart-repo/${repoType}"
// Update git config details
sh "git config --global user.email ' chartrepo-robot@example.
com'"
sh "git config --global user.name 'chartrepo-robot'"
...
dir("chart-repo") {
           // Add and commit the changes
           sh "git add --all"
           sh "git commit -m 'pushing charts from branch ${env.BRANCH_NAME}'"
           withCredentials([usernameColonPassword(credentialsId: 'github-auth', variable: 'USERPASS')]) {
               script {

                   // Inject GitHub auth and push to the master branch, where the charts are being served
                   def authRepo = env.GITHUB_PAGES_REPO_URL.replace("://", "://${USERPASS}@")
                   sh "git push ${authRepo} master"
               }
           }


fichier Jenkinsfile complet : 

pipeline {
    agent { label "chart-testing-agent" }    <<<<<<  la premiere chose qui est faite est la creation de l'agent 
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
    }
    stages {
        stage("Lint") {
            steps {
                container("chart-testing") {
                    sh "ct lint"
                }
            }
        }
        stage("Install & Test") {
            steps {
                container("chart-testing") {
                    sh "ct install --upgrade"
                }
            }
        }
        stage("package charts") {
            steps {
                script {
                    container("chart-testing") {
			sh "helm package --dependency-update helm-charts/charts/*"
                    }
                }
            }
        }
        stage("Push Charts to Chart Repo") {
            steps {
                script {
                    container("chart-testing") {
                        // Clone GitHub Pages repository to a folder called "chart-repo"
                        sh "git clone ${env.GITHUB_PAGES_REPO_URL} chart-repo"

                        // Determine if these charts should be pushed to "stable" or "staging" based on the branch
                        def repoType
                        if (env.BRANCH_NAME == "master") {
                            repoType = "stable"
                        } else {
                            repoType = "staging"
                        }

                        // Create the corresponding "stable" or "staging" folder if it does not exist
                        def files = sh(script: "ls chart-repo", returnStdout: true)
                        if (!files.contains(repoType)) {
                            sh "mkdir chart-repo/${repoType}"
                        }

                        // Move packaged charts to the corresponding "stable" or "staging" folder
                        sh "mv *.tgz chart-repo/${repoType}"

                        // Generate the updated index.yaml
                        sh "helm repo index chart-repo/${repoType}"

                        // Update git config details
                        sh "git config --global user.email 'chartrepo-robot@example.com'"
                        sh "git config --global user.name 'chartrepo-robot'"

                        dir("chart-repo") {
			    // Add and commit the changes
		 	    sh "git add --all"
			    sh "git commit -m 'pushing charts from branch ${env.BRANCH_NAME}'"
			    withCredentials([usernameColonPassword(credentialsId: 'github-auth', variable: 'USERPASS')]) {
			        script {

				    // Inject GitHub auth and push to the master branch, where the charts are being served
				    def authRepo = env.GITHUB_PAGES_REPO_URL.replace("://", "://${USERPASS}@")
				    sh "git push ${authRepo} master"
			        }
			    }
                        }
                    }
                }
            }
        }
    }
}

Note that the Helm community has published a tool called Chart Releaser( https://github.com/helm/chart-releaser ) that can be used as analternative to generating the index.yaml file with the helm repo index command and uploading it to GitHub with git push . The Chart Releaser tool is designed to abstract some of this additional complexity by managing the Helm charts contained in GitHub Pages


Ce premier pipeline montre ce qui se passe lors de la creation initiale d'un set de charts stable.
Nous allons voir maintenant comment les charts passent de staging a stable et ainsi dispos pour les users :
on va pource faire :

créer une branche depuis notre repo master :
git checkout -b FeatureNginx
on va changer la version de nginx pour valider le lint, test
ex: 
helm-charts/charts/nginx/Chart.yaml
version: 1.0.0
on passe la version à 
version: 1.0.1

on commit la diff 
$ gitcommit -am 'bumping NGINX chart version to demonstrate testing pipeline'
git push origin FeatureNginx


on retourne dans jenkins on clic sur notre job et on clic sur "scan multibranch pipeline now" > cela va permettre a jenkins de parser les repos et voir qu'une nouvelle branche est dispo
> le job va ensuite être lancé automatiquement.

on va avoir ensuite le push du chart dans notre repo staging distant.

on va donc maintenant merger notre branche FeatureNginx  dna master et pousser sur le repo
git checkout master
git merge FeatureNginx
git push origin master


on va retourner sur jenkins dans notre job et sur la branch master > on clic sur build now 
a la fin on doit avoir sur github dans le repo stable notre chart nginx 

on peut tester notre chart en ajoutant notre repo distant :

helm repo add learnhelm $GITHUB_PAGES_SITE_URL/stable

on peut trouver le lien exact dans les settings de notre repo.

= CD :

on va maintenant s'occuper de la creation d'un pipeline de cd du chart nginx créer et pousser dans notre repo stable.
La phase de cd fait abstraction des envs et permet de déployer dans differents envs 
nous allons en créer pour l'occasion :

kubectl create ns dev
kubectl create ns qa
kubectl create ns prod

on detruit notre branch precédente FeatureNginx : car jenkins va scanner toutes les branches pour eviter les confusions on ne conserve que la branche master.
git branch -d FeatureNginx


on va associer GITHUB_PAGES_SITE_URL à notre repo stable : https://$GITHUB_USERNAME.github.io/Learn-Helm-Chart-Repository/stable


on va ensuite lancer un upgrade de helm en réutilisant les valeurs précédentes grace à --reuse-values
on change juste notre repo : 
$ helm upgrade jenkins codecentric/jenkins -n chapter7 --version 1.5.1 --reuse-values --set githubPagesSiteUrl=$GITHUB_PAGES_SITE_URL

on va ensuite voir dans jenkins une fois up un nouveau job : Deploy-nginx-chart 
etude du jenkinsfile par etapes 

> creation de l'agent :
agent { label 'chart-testing-agent' }

> ensuite vient le setup : on ajoute notre repo stable dans la conf helm de notre agent jenkins : 
sh "helm repo add learnhelm ${env.GITHUB_PAGES_SITE_URL}"

> on va maintenant deployer le chart dans chacun des envs :
on va definir le rep nginx-cd comme notre working dir : syntaxe jenkins / groovy 
on lance ensuite l'upgraed de notre chart avec la valeur --install : c'est tres pratique pour une cd : on upgrade si le chart existe mais on install q'il n'existe pas dans la foulée. ( pas besoin de faire un test d'existence d'un chart au préalable.) 
On voit qu'on charge deux fichiers de values : common-values et /dev/values.yaml <<<< on override donc par env 
On voit qu'on lance la commande dans le namespace dev

dir('nginx-cd') { <<<< nginx-cd working dir 
  sh "helm upgrade --install nginx-${env.BRANCH_NAME}
learnhelm/nginx --values common-values.yaml --values dev/
values.yaml -n dev --wait"
}


> on a ensuite un test  qui basiquement va tester que la connex a un pod est ok 

(ex: 
cat templates/tests/test-connection.yaml                                                   [☸ |minikube:default]
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "nginx.fullname" . }}-test-connection"
  labels:
{{ include "nginx.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args:  ['{{ include "nginx.fullname" . }}:{{ .Values.service.port }}']
  restartPolicy: Never
)


sh 'helm test nginx -n dev'

> on va ensuite si le test est ok deployer dans l'env qa : 
on va ici grace au test conditionnel pourvoir tester le deployment de code contenu dans le values.yaml sur des branches de feature 

        stage("Deploy to QA") {
            when {
                expression {
                    return env.BRANCH_NAME == "master"
                }
            }
            steps {
                container("chart-testing") {
                    dir("nginx-cd") {
                        sh "helm upgrade --install nginx-${env.BRANCH_NAME} learnhelm/nginx --values common-values.yaml --values qa/values.yaml -n qa --wait"
                    }
                }
            }
        }



> le stage suivant est "Wait for Input" : ici jenkins va faire une pause et poser une question au user "Deploy to Prod?"  : est ce qu'on continue et on deploy en prod ou on reste en qa ?
Le user peut saisir Proceed ou Abort

stage('Wait for Input') {
    when {
        expression {
            return env.BRANCH_NAME == 'master'
        }
    }
    steps {
        container('chart-testing') {
            input 'Deploy to Prod?'
        }
    }
}

> on va pousser en prod dans le stage suivant si le user a bien saisi Proceed dans la question précédente . Comme d'habitude le fichier de values relatives au stage est sélectionné en plus des common-values.

dir('nginx-cd') {
  sh "helm upgrade --install nginx-${env.BRANCH_NAME}
learnhelm/nginx --values common-values.yaml --values prod/
values.yaml -n prod --wait"
}



Jenkinsfile complet : 

cat nginx-cd/Jenkinsfile                                              
pipeline {
    agent { label "chart-testing-agent" }
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
    }
    stages {
        stage("Setup") {
            steps {
                container("chart-testing") {
                    sh "helm repo add learnhelm ${env.GITHUB_PAGES_SITE_URL}"
                }
            }
        }
        stage("Deploy to Dev") {
            steps {
                container("chart-testing") {
                    dir("nginx-cd") {
                        sh "helm upgrade --install nginx-${env.BRANCH_NAME} learnhelm/nginx --values common-values.yaml --values dev/values.yaml -n dev --wait"
                    }
                }
            }
        }
        stage("Smoke Test") {
            steps {
                container("chart-testing") {
                    sh "helm test nginx-${env.BRANCH_NAME} -n dev"
                }
            }
        }
        stage("Deploy to QA") {
            when {
                expression {
                    return env.BRANCH_NAME == "master"
                }
            }
            steps {
                container("chart-testing") {
                    dir("nginx-cd") {
                        sh "helm upgrade --install nginx-${env.BRANCH_NAME} learnhelm/nginx --values common-values.yaml --values qa/values.yaml -n qa --wait"
                    }
                }
            }
        }
        stage("Wait for Input") {
            when {
                expression {
                    return env.BRANCH_NAME == "master"
                }
            }
            steps {
                container("chart-testing") {
                    input "Deploy to Prod?"
                }
            }
        }
        stage("Deploy to Prod") {
            when {
                expression {
                    return env.BRANCH_NAME == "master"
                }
            }
            steps {
                container("chart-testing") {
                    dir("nginx-cd") {
                        sh "helm upgrade --install nginx-${env.BRANCH_NAME} learnhelm/nginx --values common-values.yaml --values prod/values.yaml -n prod --wait"
                    }
                }
            }
        }
    }
}


en examinant la console jenkins et Deploy NGINX Chart branche master on va lancer le job et examiner les logs  de la console

on voit le deployement en dev, le test, le deployment en qa puis la question :
si on clic sur proceed on deploy en prod sinon en reste en qa 

on va pouvoir examiner dans chaque namespace le deployment de nginx .


To learn more about the chart testing container image, go to https://helm.sh/blog/chart-testing-intro/.

To learn more about Jenkins and Jenkins pipelines, check out the Jenkins project documentation 
( https://jenkins.io/doc/ )

the Jenkins pipeline documentation
( https://jenkins.io/doc/book/pipeline/ ) 

and the Multibranch Pipeline plugin documentation 
( https://plugins.jenkins.io/workflow-multibranch/ ).


== utilisation de helm avec le framework operator ==

un des avantages de helm est de synchro notre conf de fichiers de valeurs locale avec le cluster via les commandes install / upgrade.
un autre moyen est de créer une application dans le cluster qui va checker périodiquement que la conf désirée est bien celle en place dans le cluster.
Si c'a ne match pas l'application peut setter automatiquement l'env avec les bonnes valeurs et restaurer la conf.
Cette application se nomme un operator kubernetes.

- operators kubernetes : 

on peut injecter des données dans kube via kubectl , des manifests ..dans tous les cas on aura une systeme de controle loop qui sera effectué pour s'assurer que les ressources déclarées et attendues sont bien celles qui tournent dans le cluster.
Ils y a beaucoup de type de controller dans kube ( admission controller qui vont intercepter les requettes faites à l'api kube, replication controllers qui assure le nbre de replicats d'une app ....)

L'interet pour kube ayant grandi, un second type de developpement est apparu pour permettre d'étendre les capacités de kube. ce qui a permi l'émergence :
> crds : custom ressources definitions : permettent au users d'etendre les fonctionnalités de kube en creant des nouvelles ressources.
Enregistrer un nouveau type de ressource consiste a creer un nouveau path de l'api restfull . Donc de la même manière qu'on peut faire un kubctl get pods on pourra par exemple faire un kubectl get guestbook si on a enregistrer un nouvel object guestbook de type crd .
avec ces particularités les devs peuvent maintenant créer un controller pour monitorer ces crs et manager le déployment d'application qui peuvent être décrites a l'aide de l'usage de crds.

Une seconde tendance dans le monde kube a été l'émergence de deployment de plus en plus complexes (avec des statefull, replications ...) : necessitant le deployment de multiples composants et des activitées classique de prod ( backup, restauration ..) 
Biensur il faut une connaissance approfondi de ces applications .
La combinaison de l'utilisation de CRs de leur pilotage s'appelle un operator.
Les operators sont déployes en tant que pods classiques et que reagissent au changement de l'api sur les custom ressources.

Les operators sont classiquement  ecrits en utilisant des toolkit 'operator frameworks' (souvent basés sur les technos suivante : go / ansible /helm.)

"Go-based Operators leverage the Go programming language to implement control loop
logic. 
Ansible-based Operators leverage the Ansible CLI tool and Ansible playbooks.
Ansible is an automation tool whose logic is written in YAML files called playbooks.
Helm Operators base their control loop logic on Helm charts and a subset of the features provided by the Helm CLI.
As a result, they represent an easy way for Helm users to implement their Operators."


= creation d'un helm operator : =

on va créer un operator basé sur helm.
un operator est basé sur une image de container qui contient une logique de boucle de controle qui maintien l'application en état.
l'example portera sur l'application guestbook :  front php / cluster redis master - slave

> user : crée un object de type cr pour guestbook  < l'operator guestbook examine la creation des cr guestbook 

l'operator install le le chart guestbook quand le cr guestbook est crée : guestbook deployment / nodeport + redis master headless service cluster ip stateful set  + redis slace cluster ip statefullset

l'operator guestbook va constamment examiner la modif de crs pour guestbook. quand un cr est crée , l'operator va installer le chart. Quand le cr est supprimé : l'operator va delete le chart.

minikube start 
kctl create ns guestbookoperator

comme l'operateur guestbook est construit comme une image de container, on va créer un repo pour stocker cette image : on peut uploader ou on veut , on peut par exemple  créer un compte chez quay.io
on peut s'enregistrer avec notre compte github 
https://quay.io/updateuser

bill
bongo
bongobongo

creation d'un repo public billbongo/guestbook-operator

on a besoin de l'operator sdk, de docker / podman ou buildah pour construire notre image.

-> le sdk operator est le kit d'outillage pour développer un operator kube : il contient toute la logique pour simplifier le process de developpement de l'operator.
le sdk nécéssite un outil de management de container ( docker, podman buildah ...) 








=== notes the complete kubernetes course  ===

= ch1 intro =

- Commandes utiles  : 

- se plugger a une pod en cours d'excution : 
kubectl attach podname   

 kubectl attach nodehelloworld.example.com                                                                                                  [☸ minikube:default]
Defaulting container name to k8s-demo.
Use 'kubectl describe pod/nodehelloworld.example.com -n default' to see all of the containers in this pod.
If you don't see a command prompt, try pressing enter.

- execution de commande au sein d'un pod :

boogie$ kubectl exec debian-pod -- ip a                                                                                                        [☸ minikube:default]
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
12: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.5/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever

boogie$ kubectl -it exec debian-pod -- bash                                                                                                        [☸ minikube:default]
root@debian-pod:/# date
Tue Sep 10 19:28:13 UTC 2019


- port forwarding : on va pouvoir rediriger un port de notre machine vers le port d'un pod ecoutant dessus :
boogie$ kubectl port-forward nodehelloworld.example.com 8080:3000                                                                [☸ minikube:default]
Forwarding from [::1]:8080 -> 3000

sur notre poste :
curl http://localhost:8080                            [☸ minikube:default]
{
  "paths": [
    "/api",
    "/api/v1",
    "/apis",
    "/apis/",
    "/apis/admissionregistration.k8s.io",
...
boogie$ kubectl logs nodehelloworld.example.com               [☸ minikube:default]
npm info it worked if it ends with ok
npm info using npm@2.15.11
npm info using node@v4.6.2
npm info prestart myapp@0.0.1
npm info start myapp@0.0.1

> myapp@0.0.1 start /app
> node index.js

Example app listening at http://:::3000


- premiers manifests :

pod :
boogie$ cat first-app/helloworld.yml                                                                                                               [☸ minikube:default]
apiVersion: v1
kind: Pod
metadata:
  name: nodehelloworld.example.com
  labels:
    app: helloworld
spec:
  containers:
  - name: k8s-demo
    image: wardviaene/k8s-demo
    ports:
    - name: nodejs-port
      containerPort: 3000

service avec node port qui va matcher le container du pod précédent 

boogie$ cat first-app/helloworld-nodeport-service.yml                                                                                              [☸ minikube:default]
apiVersion: v1
kind: Service
metadata:
  name: helloworld-service
spec:
  ports:
  - port: 31001
    nodePort: 31001
    targetPort: nodejs-port
    protocol: TCP
  selector:
    app: helloworld
  type: NodePort


=== ch2 bases ===

= nodes :

kubelet -> pilote les set up de container dans les pods
kubeproxy > va s'occuper du routage des pods : permettre a tous les pods d'être contactables depuis le cluster et hors cluster en alimentant les regles iptables 

= replication controller : 

pour une appli stateless ( qui n'ecrit dans aucun fichier local ) pas d'etat : on va facilement pouvoir scaler.
La plupart des applis web sont stateless contrairement aux dbs qui sont stateful 
Attention pour les infrmations de sessions users web doivent être stockées hors containers.
Aucun fichier ne doit être sauvegarder dans un pod qui de part leur nature sont ephémeres.

On va scaller avec un object s'appelant replication controller : on defini le nombre de pod qu'on veut et kube s'assure que le nombre de ses containers actifs est toujours présent. Le container sera toujours recréee si il est delete ou ko 

boogie$ cat replication-controller/helloworld-repl-controller.yml                                                                                  [☸ minikube:default]
apiVersion: v1
kind: ReplicationController
metadata:
  name: helloworld-controller
spec:
  replicas: 2      <<< on defini le nombre de réplica
  selector:
    app: helloworld  <<< le selector va matcher les pods ayant le label app: helloworld
  template:
    metadata:
      labels:
        app: helloworld    <<<< le label de notre pod
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port      <<< on defini un nom a notre port pour l'identifier plus facilement lors d'appels ulterieurs (utiles pour les ports moins connus que 80 /443 par ex)
          containerPort: 3000


On voit qu"on a bien deux pods issus de notre replication controller : 

boogie$ kubectl get pod                                                                                                                            [☸ minikube:default]
NAME                          READY   STATUS    RESTARTS   AGE
debian-pod                    1/1     Running   9          36d
helloworld-controller-dv92r   1/1     Running   1          11h
helloworld-controller-ndlrd   1/1     Running   1          11h

si on delete un pod : le controller va en réinstancier un automatiquement :

boogie$ kubectl delete pod helloworld-controller-dv92r                                                                                             [☸ minikube:default]
pod "helloworld-controller-dv92r" deleted

boogie$ kubectl get pod                                                                                                                            [☸ minikube:default]
NAME                          READY   STATUS    RESTARTS   AGE
debian-pod                    1/1     Running   9          36d
helloworld-controller-fxhs8   1/1     Running   0          36s   <<< on voit ici qu'un nouveau pod a été popé pour remplacer celui qui a été détruit.
helloworld-controller-ndlrd   1/1     Running   1          11h

on peut scale en live le nombre de réplicat :

->on peut passer le fichier de replica en argumant en specifiant le nombre de pod désiré avant : 

boogie$ kubectl scale --replicas=3 -f helloworld-repl-controller.yml                                                                               [☸ minikube:default]
replicationcontroller/helloworld-controller scaled
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course/replication-controller (master) [10:00:37]
boogie$ kubectl get pod                                                                                                                            [☸ minikube:default]
NAME                          READY   STATUS    RESTARTS   AGE
debian-pod                    1/1     Running   9          36d
helloworld-controller-fxhs8   1/1     Running   0          5m8s
helloworld-controller-ndlrd   1/1     Running   1          11h
helloworld-controller-qw6c4   1/1     Running   0          6s

->  on peut directement modifier le type d'objet replicationcontroller ( rc ) en le passant en argument :
ex : on reduit le nombre de pod en utilisant le nom du rc qu'on recupere avant avec un get rc :
 ~ [10:17:32]
boogie$ kubectl get rc                                                                                                                             [☸ minikube:default]
NAME                    DESIRED   CURRENT   READY   AGE
helloworld-controller   3         3         3       11h

boogie$ kubectl scale --replicas=1 rc/helloworld-controller                                                                                        [☸ minikube:default]
replicationcontroller/helloworld-controller scaled
 ~ [10:19:40]

boogie$ kubectl get pod                                                                                                                            [☸ minikube:default]
NAME                          READY   STATUS             RESTARTS   AGE
debian-pod                    1/1     Running            10         36d
helloworld-controller-fxhs8   0/1     ImagePullBackOff   0          24m
boogie$ kubectl get rc                                                                                                                             [☸ minikube:default]
NAME                    DESIRED   CURRENT   READY   AGE
helloworld-controller   1         1         0       11h

Biensur nous scallons de maniere horizontale notre appli c'est qu'elle est stateless 
on peut delete notre replicationcontroller facilement :

boogie$ kubectl delete rc helloworld-controller                                                                                                    [☸ minikube:default]
replicationcontroller "helloworld-controller" deleted
 ~ [10:22:41]

boogie$ kubectl get rc                                                                                                                             [☸ minikube:default]
No resources found.
 ~ [10:22:43]
boogie$ kubectl get pod                                                                                                                            [☸ minikube:default]
NAME                          READY   STATUS        RESTARTS   AGE
debian-pod                    1/1     Running       10         36d
helloworld-controller-fxhs8   0/1     Terminating   0          27m


= Deployment : 

on va d'abord  voir le replicaset qui est la version maintenue par kube puisque le replication  controller est deprecated.
on va pouvoir dans cet object utiliser des selectors qui vont nous permettre de filtrer de manière plus étendu qu'avec les replicationcontroller qui se contentaient de pouvoir fournir des filtres d'egalite ( ex :env: prod) 

le replicaset est utilisé dans l'object deployment : le deployment va nous permettre de déployer des app, de les updater . On defini un status pour notre appli et kube assure que ce status est bien up and running.
L'object deployment est plus facile a utiliser que le replicatcontroller / replicaset , demande moins d'intervention manuelle et offre plus de possiblilitées.

-> create un deployment
-> update un deployment 
-> rolling update : deployment sans impact
-> rollback a une version donnée
-> pause / resume : deployment d'un certain pourcentage de nos pods.

cat deployment/helloworld.yml 

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000

- deployments commandes :

- kubectl get deployments
- kubectl get rs ( replicaset)
- kubectl get pods --show-labels                                                                                                             [☸ minikube:default]
NAME                         READY   STATUS    RESTARTS   AGE    LABELS
debian-pod                   1/1     Running   13         38d    <none>
nodehelloworld.example.com   1/1     Running   2          2d5h   app=helloworld

- kubectl rollout status  deployment/helloworld-deployment  : etat du deployment 
- kubectl set image deployment/helloworld-deployment k8s-demo=k8s-demo:2 run k8s-demo avec l'image k8s-demo:2 du label
- kubectl edit deployment/helloworld-deployment  : edition du deployment 
- kubectl rollout history deployment/helloworld-deployment  permet de  voir l'historique des versions déployées.
- kubectl rollout undo  deployment/helloworld-deployment permet de rollback sur la version déployée précédemment.
- kubectl rollout undo  deployment/helloworld-deployment --to-revision=N permet de rollback sur la version N déployée précédemment.


apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000

 kubectl create -f deployment/helloworld.yml

boogie$ kubectl get deployments                                                                                                                    [☸ minikube:default]
NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
helloworld-deployment   3/3     3            3           2m8s

boogie$ kubectl get replicasets                                                                                                                    [☸ minikube:default]
NAME                               DESIRED   CURRENT   READY   AGE
helloworld-deployment-748f49d795   3         3         3       2m20s


boogie$ kubectl get pods --show-labels                                                                                                             [☸ minikube:default]
NAME                                     READY   STATUS    RESTARTS   AGE     LABELS
debian-pod                               1/1     Running   13         38d     <none>
helloworld-deployment-748f49d795-56h2r   1/1     Running   0          3m26s   app=helloworld,pod-template-hash=748f49d795
helloworld-deployment-748f49d795-n4fnd   1/1     Running   0          3m26s   app=helloworld,pod-template-hash=748f49d795
helloworld-deployment-748f49d795-rvczv   1/1     Running   0          3m26s   app=helloworld,pod-template-hash=748f49d795
nodehelloworld.example.com               1/1     Running   2          2d5h    app=helloworld

-> check de l'etat du deployment : 
boogie$ kubectl rollout status deployment helloworld-deployment                                                                                    [☸ minikube:default]
deployment "helloworld-deployment" successfully rolled out

-> modification de l'image utilisée : deploy d'une nouvelle version de notre app: 
boogie$ kubectl set image deployment/helloworld-deployment k8s-demo=wardviaene/k8s-demo:2                                                          [☸ minikube:default]
deployment.extensions/helloworld-deployment image updated
boogie$ kubectl get pods  --show-labels                                                                                                            [☸ minikube:default]
NAME                                     READY   STATUS        RESTARTS   AGE    LABELS
debian-pod                               1/1     Running       13         38d    <none>
helloworld-deployment-748d88f59f-8xw74   1/1     Running       0          36s    app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748d88f59f-pwzl9   1/1     Running       0          30s    app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748d88f59f-zh27v   1/1     Running       0          36s    app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748f49d795-56h2r   1/1     Terminating   0          11m    app=helloworld,pod-template-hash=748f49d795
helloworld-deployment-748f49d795-n4fnd   1/1     Terminating   0          11m    app=helloworld,pod-template-hash=748f49d795
nodehelloworld.example.com               1/1     Running       2          2d5h   app=helloworld


boogie$ kubectl describe pod helloworld-deployment-748d88f59f-pwzl9                                                                                [☸ minikube:default]
Name:           helloworld-deployment-748d88f59f-pwzl9
Namespace:      default
Priority:       0
Node:           minikube/10.0.2.15
Start Time:     Fri, 13 Sep 2019 16:24:16 +0200
Labels:         app=helloworld
                pod-template-hash=748d88f59f
Annotations:    <none>
Status:         Running
IP:             172.17.0.15
Controlled By:  ReplicaSet/helloworld-deployment-748d88f59f
Containers:
  k8s-demo:
    Container ID:   docker://6d211169aa8da1588bf5a96d01e6df1e19646e53142e41f8030a1391a25abfc4
    Image:          wardviaene/k8s-demo:2
    Image ID:       docker-pullable://wardviaene/k8s-demo@sha256:c7536949ff900fb7dc923cf9f2475d1209766c65aa07325caf880e754e7e0fae
    Port:           3000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 13 Sep 2019 16:24:18 +0200
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wfc8r (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-wfc8r:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-wfc8r
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  61s   default-scheduler  Successfully assigned default/helloworld-deployment-748d88f59f-pwzl9 to minikube
  Normal  Pulling    60s   kubelet, minikube  Pulling image "wardviaene/k8s-demo:2"
  Normal  Pulled     59s   kubelet, minikube  Successfully pulled image "wardviaene/k8s-demo:2"
  Normal  Created    59s   kubelet, minikube  Created container k8s-demo
  Normal  Started    59s   kubelet, minikube  Started container k8s-demo

on a donc notre nouvelle appli déployée.

- Examen de l'historique des versions déployés :

boogie$ kubectl rollout history  deployment/helloworld-deployment                                                                                  [☸ minikube:default]
deployment.extensions/helloworld-deployment
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

on va pouvoir rollback et revenir a notre version précédente : 

boogie$ kubectl rollout undo  deployment/helloworld-deployment                                                                                     [☸ minikube:default]
deployment.extensions/helloworld-deployment rolled back

le rollback se passe bien ; 

boogie$ kubectl rollout status  deployment/helloworld-deployment                                                                                   [☸ minikube:default]
Waiting for deployment "helloworld-deployment" rollout to finish: 2 of 3 updated replicas are available...
deployment "helloworld-deployment" successfully rolled out

on voit nos différents  pods : 
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course (master) [04:32:27]
boogie$ kubectl get pods  --show-labels                                                                                                            [☸ minikube:default]
NAME                                     READY   STATUS        RESTARTS   AGE     LABELS
debian-pod                               1/1     Running       13         38d     <none>
helloworld-deployment-748d88f59f-8xw74   1/1     Terminating   0          8m22s   app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748d88f59f-pwzl9   1/1     Terminating   0          8m16s   app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748d88f59f-zh27v   1/1     Terminating   0          8m22s   app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748f49d795-fp79k   1/1     Running       0          11s     app=helloworld,pod-template-hash=748f49d795
helloworld-deployment-748f49d795-gp8qd   1/1     Running       0          11s     app=helloworld,pod-template-hash=748f49d795
helloworld-deployment-748f49d795-lddvl   1/1     Running       0          8s      app=helloworld,pod-template-hash=748f49d795
nodehelloworld.example.com               1/1     Running       2          2d5h    app=helloworld

on peut changer le nombre d'historique de version conservé de notre déployment :
avec le param revisionHistoryLimit: XX que l'on set dans notre déployment : 
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 3
    revisionHistoryLimit: 100
    selector:
      matchLabels:
        app: helloworld

on peut rollback : 
boogie$ kubectl rollout undo  deployment/helloworld-deployment                                                                                     [☸ minikube:default]
deployment.extensions/helloworld-deployment rolled back

boogie$ kubectl rollout history deployment                                                                                                         [☸ minikube:default]
deployment.extensions/helloworld-deployment 
REVISION  CHANGE-CAUSE
4         <none>

- on utilise une nouvelle image : 
boogie$ kubectl set image deployment/helloworld-deployment k8s-demo=wardviaene/k8s-demo:1                                                          [☸ minikube:default]
deployment.extensions/helloworld-deployment image updated

boogie$ kubectl rollout status  deployment/helloworld-deployment                                                                                   [☸ minikube:default]
Waiting for deployment "helloworld-deployment" rollout to finish: 2 out of 3 new replicas have been updated...

on voit qu'on a plusieurs versions historisées : 
boogie$ kubectl rollout history  deployment/helloworld-deployment
deployment.extensions/helloworld-deployment 
REVISION  CHANGE-CAUSE
4         <none>
5         <none>
6         <none>
7         <none>

on peut donc forcer le rollback dans une version particulière : 

boogie$ kubectl rollout undo  deployment/helloworld-deployment --to-revision=5                                                                     [☸ minikube:default]
deployment.extensions/helloworld-deployment rolled back


= services : 

les pods sont éphémeres  par nature :( detruits pendant les déployments etc ..)  nous ne devons pas acceder directement à eux : une couche intermédiaire est défini pour relier les clients ( au sens large: services, end-users ) aux services portés par les pods .
quand on utilise kubectl expose : on crée un service popur notre pod qui pourra être atteind de l'exterrieur .
Quand on crée un service on va créer un endpoint pour nos pods 
On peut utiliser plusieurs conf :

-> clusterip : ip uniquement accésible dans notre cluster kube ( c'est la conf de base si on ne précise rien : de base on crée un service en clusterip )
-> nodeport: c'est un port identique sur tous les nodes de notre cluster pour atteindre le service de l'exterrieur : on va utiliser un port situé dans un range entre 30000et 32267 pour faire matcher le port du pod sur le nodeport qui lui sera sticker sur le /les nodes de notre cluster.
-> loadbalancer : c'est utilisé dans le cloud. on va router le traffic externe vers le nodeport de nos nodes kube 

Les options montrées permettrent uniquement le creation de virtualsip ou de ports

on peut utiliser des entrées dns 
ExternalName peut fournir un nom dns a nos services.
ex : service discovery pour le dns.

-  service demo : 

boogie$ cat first-app/helloworld.yml                                                                                                               [☸ minikube:default]
apiVersion: v1
kind: Pod
metadata:
  name: nodehelloworld.example.com
  labels:
    app: helloworld
spec:
  containers:
  - name: k8s-demo
    image: wardviaene/k8s-demo
    ports:
    - name: nodejs-port
      containerPort: 3000

boogie$ cat first-app/helloworld-nodeport-service.yml                                                                                              [☸ minikube:default]
apiVersion: v1
kind: Service
metadata:
  name: helloworld-service
spec:
  ports:
  - port: 31001
    nodePort: 31001
    targetPort: nodejs-port
    protocol: TCP
  selector:
    app: helloworld
  type: NodePort

kubectl create -f first-app/helloworld.yml
kubectl create -f first-app/helloworld-nodeport-service.yml

boogie$ kubectl get svc                                                                                                                            [☸ minikube:default]
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE
helloworld-service   NodePort    10.106.205.39   <none>        31001:31001/TCP   13h
kubernetes           ClusterIP   10.96.0.1       <none>        443/TCP           36d

on recupere le couple ip/port exposé dans minikube 
boogie$ minikube service helloworld-service --url                                                                                                  [☸ minikube:default]
http://192.168.99.100:31001

on voit sur notre poste qu'un reseau est monté dans le range :
boogie$ ip a |grep vboxnet3                                                                                                                        [☸ minikube:default]
8: vboxnet3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    inet 192.168.99.1/24 brd 192.168.99.255 scope global vboxnet3


on voit en examinant le service crée qu'on a bien un service de type NodePort et une ClusterIp : cette ip n'est joignable que de notre cluster.

boogie$ kubectl get svc                                                                                                                            [☸ minikube:default]
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE
helloworld-service   NodePort    10.109.171.91   <none>        31001:31001/TCP   3m14s
kubernetes           ClusterIP   10.96.0.1       <none>        443/TCP           38d

La ClusterIp est une ip virtuelle qui change si on delete et on recree notre service . On peut fixer l'ip si on veut dans le yaml de notre service .
Comme pour le port static qu'on défini si on veut dans notre yaml

= labels : 

les labels sont des paires clé /valeurs qui peuvent être attachées à un object.
Ils sont utilisés pour tagguer les objects.
On peut taguer un pod par exemple en mettant un label de type key: environment valeur: dev/ prod ...
On peut mettre plusieurs labels sur un object : on peut rajouter sur notre pod précédent un label de type key: department / valeur: finance / it ...
Une fois que le/s label/s est /sont attachés a un object on va pouvoir les utiliser pour filtrer nos besoins en utilisant un selector.
Un label selector peut utiliser des regexp pour matcher notre object
ex: on peut dire qu'un pod ne doit tourner que dur un node avec le label environment equal dev ,  ou environment equal 'dev' or 'qa'

On va pouvoir tagger un node puis dire qu'un pod ira uniquement sur un node contenant le label : ceci se fera grace au nodeSelector qu'on ajoutera dans notre config.

- nodes labels :

ex :on peut ajouter un tag en cli :

boogie$ kubectl label node minikube environment=lab                                                                                                                                                                      [☸ minikube:default]
node/minikube labeled

on peut voir notre label :

boogie$ kubectl get nodes --show-labels                                                                                                                                                                                  [☸ minikube:default]
NAME       STATUS   ROLES    AGE   VERSION   LABELS
minikube   Ready    master   39d   v1.15.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,environment=lab,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,node-role.kubernetes.io/master=

On va pouvoir ensuite définir un pod en specifiant qu'il doit s"executer uniquement sur les nodes flaggués lab :

  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: nodehelloworld.example.com
    labels:
      app: helloworld
  spec:
    containers:
   - name: k8s-demo
      image: wardviaene/k8s-demo
      ports:
   - name: nodejs-port
        containerPort: 3000
   nodeSelector:
      environment: lab

Tant qu'un node avec le tag environment: lab n'existe pas alors notre object ne peut pas démarré.


= healthcheck :

On peut avoir un pod démarré mais qui ne répond pas vraiment : l'appli est donc ko. Il faut pouvoir nous assurer que le pod est bien  fonctionnel.

Pour detecter et resoudre des pb de fonctionnemnt on va ajouter un healtchcheck 

2 types existent :
-> lancer une commande périodiquement 
-> executer un check http sur une url



boogie$ cat deployment/helloworld-healthcheck.yml                                               [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
        livenessProbe:    <<< definition de la section du healthcheck 
          httpGet:        <<<< check http
            path: /       <<<<  on fait un get sur / 
            port: nodejs-port  <<< sur le port nodejs-port defini dans la section de notre notre pod 
          initialDelaySeconds: 15  <<<< on attend 15 secondes avant le premier check 
          timeoutSeconds: 30    <<<< on considere que le pod est ko apres 30 secondes sans réponse.

On voit quand on affiche la description du pod apres le deployment la section de la sonde :

..
    Liveness:       http-get http://:nodejs-port/ delay=15s timeout=30s period=10s #success=1 #failure=3
...
Quand on edite le déployment poussé dans kube on voit le detail que kube ajoute pour les healtchcheck 

    spec:
        containers:
>>      - image: wardviaene/k8s-demo
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3   <<< nombre d'occurence de fail conisdérer en critical : le pod sera redémarré
            httpGet:
              path: /
              port: nodejs-port
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10  <<<< frequence de check 
            successThreshold: 1   <<< nombre de fois ou le check est considérer comme ok 
            timeoutSeconds: 30


= readiness probe :

On a vu que le healtcheck va servir a verifier l'etat du pod : si celui ci est ko alors kube en redemarre un
Une readiness probe va nous assurer que le pod est bien pret a servir les requettes . Si le test est ko alors le pod n'est pas redémarré et son ip est supprimée de la liste des ip des pods du service. 

Au demarrage de notre object ( pod ,deployment ..) : l'object pourra être running mais pas ready : tant que la sonde de readiness ne sera pas passé et aura considérer le pod ok puis que répondant aux requettes.
On va mettre conjointement readiness et healtcheck dans la def de nos objects :
boogie$ cat deployment/helloworld-liveness-readiness.yml                                        [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-readiness
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
        livenessProbe:
          httpGet:
            path: /
            port: nodejs-port
          initialDelaySeconds: 15
          timeoutSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: nodejs-port
          initialDelaySeconds: 15
          timeoutSeconds: 30

NAME                                  READY   STATUS    RESTARTS   AGE
helloworld-readiness-dc8fc5bc-d9phj   0/1     Running   0          11s
helloworld-readiness-dc8fc5bc-gm6kx   0/1     Running   0          11s
helloworld-readiness-dc8fc5bc-tcwbx   0/1     Running   0          11s


= pods states :


les pods peuvent avoir plusieurs status :


-> running : ils sont en cours d'execution
-> pending : en cours de creation ( recup d'image , attente de ressources définie dans l'object : label , memoire ,cpu .. resources ...) 
-> succeeded : tous les pods ont été correctement déployés
-> failed : les containers dans le pod sont en etat terminated et au moins un container a renvoyé un code retour d'erreur.
-> unknow : impossible de connaitre l'etat

on peut connaitre les differentes etapes que le pod a connu en faisant un 
kubectl describe pod mon pod 

- pods conditions : il y a differentes conditions 

-> podScheduled : le pod a été schedule sur un node
-> Ready : le pod est pret a servir du traffic et va etre ajouté pour matcher un service 
-> initialized : les containers du pod sont initialisés correctement
-> unschedulable : les pod ne peuvent pas être dispatcher : pb de contrainte de ressource ( ex pod devant etre sur un node avec 2T de ram de libre ...) 
-> containersready : les onctainers du pod sont prets

on peut avoir aussi l'etat du container avec :

boogie$ kubectl get pod helloworld-readiness-dc8fc5bc-tcwbx -n default -o yaml                  [☸ minikube:default]
apiVersion: v1
kind: Pod
metadata:
  ....

  containerStatuses:
  - containerID: docker://95e9a8c9afc2092471107fa066acaf567613c4c71da186f5ef3aa3016e53cbcc
    image: wardviaene/k8s-demo:latest
    imageID: docker-pullable://wardviaene/k8s-demo@sha256:2c050f462f5d0b3a6430e7869bcdfe6ac48a447a89da79a56d0ef61460c7ab9e
    lastState: {}
    name: k8s-demo
    ready: true
    restartCount: 0
    ....

Les containers  peuvent être en running / terminated ou waiting


exemple de fichier comportant deux containers , et deux hooks servant a differents moment de la vie de notre container :


-> un init container va être utiliser pour la prépartion d'element pour l'acceuil de notre pod :
ex : on lance un init container pour préparer un volume qui sera ensuite monter par le pod principal.

Ici dans notre exemple l'init container va juste tourner quelques seconde ..puis laisser la main au hook, au second container puis au second hook : 


boogie$ cat pod-lifecycle/lifecycle.yaml                                                        [☸ minikube:default]
kind: Deployment
apiVersion: apps/v1beta1
metadata:
  name: lifecycle
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: lifecycle
    spec:
      initContainers:
      - name:           init
        image:          busybox
        command:       ['sh', '-c', 'sleep 10']
      containers:
      - name: lifecycle-container
        image: busybox
        command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
        readinessProbe:
          exec:
            command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
          initialDelaySeconds: 35
        livenessProbe:
          exec:
            command: ['sh', '-c', 'echo $(date +%s): livenessProbe >> /timing']
          initialDelaySeconds: 35
          timeoutSeconds: 30
        lifecycle:
          postStart:
            exec:
              command: ['sh', '-c', 'echo $(date +%s): postStart >> /timing && sleep 10 && echo $(date +%s): end postStart >> /timing']
          preStop:
            exec:
              command: ['sh', '-c', 'echo $(date +%s): preStop >> /timing && sleep 10']


quand on lance on obtient plusieurs etats : 

-> notre init container se lance : 
NAME                         READY   STATUS     RESTARTS   AGE
lifecycle-86977b4c47-d7hh2   0/1     Init:0/1   0          5s

-> notre second container se lance : les post et pré hook s'executent : 
NAME                         READY   STATUS            RESTARTS   AGE
lifecycle-86977b4c47-d7hh2   0/1     PodInitializing   0          16s

NAME                         READY   STATUS    RESTARTS   AGE
lifecycle-86977b4c47-d7hh2   0/1     Running   0          28s

NAME                         READY   STATUS             RESTARTS   AGE
lifecycle-86977b4c47-d7hh2   0/1     CrashLoopBackOff   1          4m36s

NAME                         READY   STATUS    RESTARTS   AGE
lifecycle-86977b4c47-d7hh2   0/1     Running   2          4m55s


jusqu'a ce que le nombre de heatlcheck ko soient arrivés au max et que le pod ne soit plus redémarré.



= Secrets : 

les secrets dans kube vont permettre de passer des credentials, password , data à nos pods.
kube utilise les mêmes mecanismes pour passer des secret a l'api 
On peut utiliser des solutions externes à kube pour passer des secrets à nos apps (vault etc ...) mais les Secrets sont la méthode native de kube.

On peut utiliser les Secrets :
-> en variable d'environment 
-> en fichier plat : qui seront dans un volume monté par le pod et donc accessible 
-> utilisation d'une registry pour stocker les secrets : une image contenant les secrets sera pull et utilisée par un pod ..(?)

- generation de secret depuis un fichier :


boogie$ echo -n "root" > ./username.txt                                                         [☸ minikube:default]
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course (master*) [11:11:13]
boogie$ echo -n "password" > ./password.tx

boogie$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt
secret/db-user-pass created

- generation d'un secret depuis un cle ssh  , un cert tls : 

boogie$ kubectl create secret generic ssl-certificate --from-file=ssh-privatekey=~/.ssh/id_rsa  --ssl-cert=mysslcert.c (KO)


- Generation depuis un yaml :

on va encoder nos credentials via base64 

boogie$ echo -n "root" |base64                                                                  [☸ minikube:default]
cm9vdA==
boogie$ echo -n "password" |base64                                                              [☸ minikube:default]
cGFzc3dvcmQ=

secret-data.yaml

apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data: 
   password: cGFzc3dvcmQ=
   username: cm9vdA==

Une fois qu'on a créer nos secret on va pouvoir les utiliser 
On va pouvoir les utiliser de différentes manieres 

- variables d'env :
on defini l'appel dans notre pod : 

...
 env:
   - name: SECRET USERNAME   
     ValueFrom:            <<<<<<<<  on va donc maintenant rattacher les valeurs definis dans notre secret au pod
      - secretRef:
        name: app-secret
        key: username 

- on defini un volume:

boogie$ cat deployment/helloworld-secrets-volumes.yml                                           [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
        volumeMounts:
        - name: cred-volume
          mountPath: /etc/creds
          readOnly: true
      volumes:
      - name: cred-volume
        secret: 
          secretName: db-secrets


On peut voir quand on crée nos ressources , en examinant un pod que notre fichier secret est défini :

kubectl describe pod helloworld-deployment-6b6585d49c-d6748                             [☸ minikube:default]
Name:           helloworld-deployment-6b6585d49c-d6748
..
    Mounts:
      /etc/creds from cred-volume (ro)   <<<< c'est le point de montage 
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wfc8r (ro)

On va examiner depuis notre pod et voir que les secrets sont présents dans le point de montage défini : 

boogie$ kubectl exec -it helloworld-deployment-6b6585d49c-d6748 -- bash                         [☸ minikube:default]
root@helloworld-deployment-6b6585d49c-d6748:/app# ls
Dockerfile  docker-compose.yml	index-db.js  index.js  misc  node_modules  package.json
passwordroot@helloworld-deployment-6b6585d49c-d6748:/app# ls /etc/creds/          
password  username

- Cas pratique : set up wordpress en stateless ( perte de données des que notre pod est ko )


 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course (master*) [10:05:49]
boogie$ cat wordpress/wordpress-secrets.yml                                                     [☸ minikube:default]
apiVersion: v1
kind: Secret
metadata:
  name: wordpress-secrets
type: Opaque
data:
  db-password: cGFzc3dvcmQ=
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course (master*) [10:06:03]
boogie$ cat wordpress/wordpress-single-deployment-no-volumes.yml                                [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: wordpress-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:4-php7.0
        ports:
        - name: http-port
          containerPort: 80
        env:
          - name: WORDPRESS_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: wordpress-secrets
                key: db-password
          - name: WORDPRESS_DB_HOST
            value: 127.0.0.1
      - name: mysql
        image: mysql:5.7
        ports:
        - name: mysql-port
          containerPort: 3306
        env:
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: wordpress-secrets
                key: db-password


boogie$ kubectl create -f  wordpress/wordpress-single-deployment-no-volumes.yml

boogie$ kubectl get pod                                                                         [☸ minikube:default]
NAME                                    READY   STATUS              RESTARTS   AGE
wordpress-deployment-58cd589c6c-bncrs   0/2     ContainerCreating   0          9s
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course (master*) [10:07:38]
boogie$ kubectl describe pod wordpress-deployment-58cd589c6c-bncrs                              [☸ minikube:default]
Name:           wordpress-deployment-58cd589c6c-bncrs
Namespace:      default
Priority:       0
Node:           minikube/10.0.2.15
Start Time:     Sun, 15 Sep 2019 10:07:29 +0200
Labels:         app=wordpress
                pod-template-hash=58cd589c6c
Annotations:    <none>
Status:         Pending
IP:
Controlled By:  ReplicaSet/wordpress-deployment-58cd589c6c
Containers:
  wordpress:
    Container ID:
    Image:          wordpress:4-php7.0
    Image ID:
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:
      WORDPRESS_DB_PASSWORD:  <set to the key 'db-password' in secret 'wordpress-secrets'>  Optional: false
      WORDPRESS_DB_HOST:      127.0.0.1
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wfc8r (ro)
  mysql:
    Container ID:
    Image:          mysql:5.7
    Image ID:
    Port:           3306/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:
      MYSQL_ROOT_PASSWORD:  <set to the key 'db-password' in secret 'wordpress-secrets'>  Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wfc8r (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  default-token-wfc8r:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-wfc8r
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  21s   default-scheduler  Successfully assigned default/wordpress-deployment-58cd589c6c-bncrs to minikube
  Normal  Pulling    20s   kubelet, minikube  Pulling image "wordpress:4-php7.0"

on va créer un service qui va nous permettre d'atteindre notre appli :

boogie$ cat wordpress/wordpress-service.yml                                                     [☸ minikube:default]
apiVersion: v1
kind: Service
metadata:
  name: wordpress-service
spec:
  ports:
  - port: 31001
    nodePort: 31001
    targetPort: http-port
    protocol: TCP
  selector:
    app: wordpress
  type: NodePort

on a donc un nodeport et le port 31001 de notre host va rediriger vers le port http-port de notre pod :

boogie$ kubectl create -f wordpress/wordpress-service.yml                                       [☸ minikube:default]
service/wordpress-service created
boogie$ kubectl get svc                                                                         [☸ minikube:default]
NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE
kubernetes          ClusterIP   10.96.0.1        <none>        443/TCP           40d
wordpress-service   NodePort    10.103.144.132   <none>        31001:31001/TCP   12s

boogie$ minikube service wordpress-service --url                                                [☸ minikube:default]
http://192.168.99.100:31001

on va remplir les infos essentielles au set up de wordpress via le browser :

Informations nécessaires
Veuillez renseigner les informations suivantes. Ne vous inquiétez pas, vous pourrez les modifier plus tard.
ex:
Titre du site ￼: boogie-blog
Identifiant ￼: boogie
Mot de passe XXXX
Votre adresse de messagerie ￼boogie@localhost.net

...si le pod est ko ..alors on devra de nouveau refaire notre setup puisqu'on a pas défini de volume pour préserver nos data.

= webui : 

on a la possibilité de faier des actions sur notre cluster kube via un dashboard 
il faut d'abord l'installer :

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml

si un passowrd est demandé on peut le recupérer avec un 
kubectl config view

avec minikube : 
minikube dashboard
pour recupérer l'url du dashboard : 
minikube dashboard --url

- mode op setting dashboard : 

boogie$ cat dashboard/README.md                                                                                                                                                                                          
# Setting up the dashboard / Start dashboard

Create dashboard:
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
```


boogie$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created


## Create user

Create sample user (if using RBAC - on by default on new installs with kops / kubeadm):

cat dashboard/sample-user.yaml                                                          [☸ minikube:default]
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

```
kubectl create -f sample-user.yaml
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created

```

## Get login token:
```
boogie$ kubectl -n kube-system get secret | grep admin-user                                     [☸ minikube:default]
admin-user-token-8fck6                           kubernetes.io/service-account-token   3      4m15s

boogie$ kubectl -n kube-system describe secret admin-user-token-8fck6                           [☸ minikube:default]
Name:         admin-user-token-8fck6
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: ba7e8fe6-6bec-44ac-98eb-3439ce74a5cb

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1066 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLThmY2s2Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiYTdlOGZlNi02YmVjLTQ0YWMtOThlYi0zNDM5Y2U3NGE1Y2IiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.h2TjbQkVFk5-wlyow2CIRhjO2R5cxSlPEHg0vPSK1cB446Jmnb8bNnPOe9QOfLLpSkYzpCEEZdJq_B-A3Uy165G6pGIveEBHeEYIbdTXhJGKkwG6pBQF1dZc69irZfbP5_WtwCCpVFtl5C2iRUcBj0JMn0-c6EY065j41b8vMsnBnZNpUcraNomu_oazD0ZN_YNMrpsOmVZ2YMI3mSOqHJq-uzokOlO90Dmc0cJwfDZrfPnigN78WPGxRWfky_49yXQdOcDBDVED-5NHvloOb_HkCnVCoG2L1REejdVTEzktFouYwdHmGoamqHmBlyf733PPNJ-6el0oXiwbTuAk6Q
```

## Login to dashboard
Go to http://api.yourdomain.com:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login

Login: admin
Password: the password that is listed in ~/.kube/config (open file in editor and look for "password: ..."

Choose for login token and enter the login token from the previous step


- minikube : 

 minikube dashboard --url                                                                [☸ minikube:default]
🤔  Verifying dashboard health ...
🚀  Launching proxy ...
🤔  Verifying proxy health ...
http://127.0.0.1:34313/api/v1/namespaces/kube-system/services/http:kubernetes-dashboard:/proxy/




== chapitre 3 :

= service discovery : 

dns est un service builtin lancé par le addons managers, qu'on peut voir sur un node master dans :
le service dns va être utilisé par les pods pour trouver des services tournant sur le même cluster.
Les containers au sein d'un pod n'utilsient pas le dns puisqu'ils communiquent en ip:port

ex: 

pod1
container1
service app1 / 10.0.0.1

pod2
container2
service app2 / 10.0.0.2


dans pod1 pour contacter le service app1 on fait :

host app1-service
app1-service has adress 10.0.0.1

dans pod1 pour contacter le service app2 on fait :
host app2-service
app2-service has adress 10.0.0.2

de base comme cela on interroge que les services dans le même namespace. On peut definir le namespace de recherche :
de base on est dans le namespace default : 

host app1-service.default
app1-service.default has adress 10.0.0.1

dans pod1 pour contacter le service app2 on fait :
host app2-service.default
app2-service.default has adress 10.0.0.2

Si on veut interroger via le fqdn global du cluster on utilisera :

dans pod1 pour contacter le service app2 on fait :
host app2-service.default.svc.cluster.local
app2-service.default.svc.local.cluster has adress 10.0.0.2

on peut mieux comprendre en examinant le fichier resolv.conf sur un pod :

boogie$ kubectl exec wordpress-deployment-58cd589c6c-bncrs -it -- bash                          [☸ minikube:default]
Defaulting container name to wordpress.
Use 'kubectl describe pod/wordpress-deployment-58cd589c6c-bncrs -n default' to see all of the containers in this pod.
root@wordpress-deployment-58cd589c6c-bncrs:/var/www/html# cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5

on voit que notre resolveur est 10.96.0.10 
le domain de recherche  default.svc.cluster.local   svc.cluster.local cluster.local
on voit que cette ip correspond au service dns qui est exposé dans notre cluster kube dans le namespace kube-system :

boogie$ kubectl get svc -n kube-system                                                          [☸ minikube:default]
NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
..
kube-dns               ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   40d
..

Les services crées sont publiés dans le dns. On pourra donc dans notre cluster interroger les différents services : il faudra biensur préciser le namespace de celui ci s'il est situé en dehors du namespace de notre pod /appli :

boogie$ kubectl exec wordpress-deployment-58cd589c6c-bncrs -it -- bash                          [☸ minikube:default]
Defaulting container name to wordpress.
Use 'kubectl describe pod/wordpress-deployment-58cd589c6c-bncrs -n default' to see all of the containers in this pod.
root@wordpress-deployment-58cd589c6c-bncrs:/var/www/html# host tiller-deploy
Host tiller-deploy not found: 3(NXDOMAIN)
root@wordpress-deployment-58cd589c6c-bncrs:/var/www/html# host tiller-deploy.kube-system
tiller-deploy.kube-system.svc.cluster.local has address 10.105.162.15


boogie$ kubectl get svc -n kube-system                                                          [☸ minikube:default]
NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default-http-backend   NodePort    10.101.219.90   <none>        80:30001/TCP             40d
kube-dns               ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   40d
kubernetes-dashboard   ClusterIP   10.98.41.81     <none>        80/TCP                   40d
tiller-deploy          ClusterIP   10.105.162.15   <none>        44134/TCP                40d


= configmap : 

on peut mettre de la config non secrete dans un config map : le principe est le même que pour les secrets.
Il s'agit de couple key/values
Les configmap peuvent être lues par l'appli en utilsiant :
> des variables d'env
> des arguments de la cli du container
> via un volume
Un configmap peut contenir un fichier complet de conf d'une appli : ex : une conf de vhost nginx
On va pouvoir monter le volume dans un container : dans ce volume on aura la conf de notre appli. On en touche pas à la conf du container pour autant.

Ex: on a une appli qui ecoute  sur le port 3000 : on va créer une conf de reverse proxy qui va forward les appels sur le port 80 à notre appli sur le port 3000. Cette conf va être notre configmap :

boogie$ cat configmap/reverseproxy.conf                                                         [☸ minikube:default]
server {
    listen       80;
    server_name  localhost;

    location / {
        proxy_bind 127.0.0.1;
        proxy_pass http://127.0.0.1:3000; <<< on redirige vers l'appli locale sur le port 3000
    }

    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}

boogie$ kubectl create configmap nginx-config --from-file=configmap/reverseproxy.conf           [☸ minikube:default]
configmap/nginx-config created

On va check la creation : 
boogie$ kubectl get configmaps                                                                  [☸ minikube:default]
NAME           DATA   AGE
nginx-config   1      49s

boogie$ kubectl get configmaps -o yaml                                                          [☸ minikube:default]
apiVersion: v1
items:
- apiVersion: v1
  data:
    reverseproxy.conf: |
      server {
          listen       80;
          server_name  localhost;

          location / {
              proxy_bind 127.0.0.1;
              proxy_pass http://127.0.0.1:3000;
          }

          error_page   500 502 503 504  /50x.html;
          location = /50x.html {
              root   /usr/share/nginx/html;
          }
      }
  kind: ConfigMap
  metadata:
    creationTimestamp: "2019-09-15T19:35:16Z"
    name: nginx-config
    namespace: default
    resourceVersion: "206154"
    selfLink: /api/v1/namespaces/default/configmaps/nginx-config
    uid: 7218c74c-ba1d-4757-93dc-71d0cff77356
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

On va setter l'appel à notre configmap dans notre pod / deployment :
celui ci aura deux container : nginx et l'autre contenant notre appli servant les requettes sur le port 3000

boogie$ cat configmap/nginx.yml                                                                 [☸ minikube:default]
apiVersion: v1
kind: Pod
metadata:
  name: helloworld-nginx
  labels:
    app: helloworld-nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.11
    ports:
    - containerPort: 80
    volumeMounts:
    - name: config-volume   <<<< ici on defini le nom du volume 
      mountPath: /etc/nginx/conf.d  <<< le path du volume contenant les data
  - name: k8s-demo
    image: wardviaene/k8s-demo
    ports:
    - containerPort: 3000
  volumes:
    - name: config-volume  <<<< on retrouve la définition du volume : même nom que la ressource pour le pod nginx
      configMap:
        name: nginx-config   <<<< c'est le nom du configmap dans kube 
        items:
        - key: reverseproxy.conf  <<<< ici on retrouve le nom de la clé du configmap dans kube qui contient notre conf 
          path: reverseproxy.conf


On cree un service de type nodeport pour l'exposer :

boogie$ cat configmap/nginx-service.yml                                                         [☸ minikube:default]
apiVersion: v1
kind: Service
metadata:
  name: helloworld-nginx-service
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    app: helloworld-nginx
  type: NodePort
boogie$ kubectl create -f configmap/nginx-service.yml                                           [☸ minikube:default]
service/helloworld-nginx-service created

On va tester et vérifier que notre conf de reverse fonctionne correctement : 


minikube service helloworld-nginx-service --url

http://192.168.99.100:31983


On voit bien qu'on a nginx qui nous repond : notre reverse proxy fait bien le travail.

boogie$ curl -vvv http://192.168.99.100:31983                                                   [☸ minikube:default]
*   Trying 192.168.99.100:31983...
* TCP_NODELAY set
* Connected to 192.168.99.100 (192.168.99.100) port 31983 (#0)
> GET / HTTP/1.1
> Host: 192.168.99.100:31983
> User-Agent: curl/7.65.1
> Accept: */*
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: nginx/1.11.13
< Date: Sun, 15 Sep 2019 19:48:26 GMT
< Content-Type: text/html; charset=utf-8
< Content-Length: 12
< Connection: keep-alive
< X-Powered-By: Express
< ETag: W/"c-7Qdih1MuhjZehB6Sv8UNjA"
<
* Connection #0 to host 192.168.99.100 left intact
Hello World!%

on peut verifier la conf de notre reverse en nous connectant dans le container nginx de notre pod :

boogie$ kubectl exec -it helloworld-nginx -c nginx -- bash                                      [☸ minikube:default]
root@helloworld-nginx:/# cat /etc/nginx/conf.d/reverseproxy.conf
server {
    listen       80;
    server_name  localhost;

    location / {
        proxy_bind 127.0.0.1;
        proxy_pass http://127.0.0.1:3000;
    }

    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}


ch2v52

= ingress : 

ingress est une solution pour les connexions externes a notre cluster , c'est une alternative aux external loadbalancer et nodeports.
Ingress permet d'exposer facilement les services devant etre accessible depuis l'exterrieur du cluster.

basiquement on va ce type de flux :

  www / 80/443
      |
      ingress controller 

      | ingress rules ->  host.exemple1.net -> pod1
                          host.exemple2.net -> pod2
                           host.exemple3.net/api/v2  -> pod3

   |             |           |
pod1             pod2        pod3



on peut utilsier different ingress-controller ( ex: ngin-ingress controller )
on a diffentes section dont des args qu'on peut passer a nginx
: on voit ici qu'on peut mettre en place un default backend  qui va recupérer les requettes qui ne matcheront aucune regles :


        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.17.1
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/echoheaders-default
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io

on peut definir le echo service qui va servir de fallback si nos rules ne matchent pas : 
apiVersion: v1
kind: Service
metadata:
  name: echoheaders-default
  labels:
    app: echoheaders
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30302
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: echoheaders



on va ensuite définir des rules por rediriger nos flux en fonctions des requettes entrantes :
ex: 
boogie$ cat ingress/ingress.yml                               [☸ minikube:default]
# An Ingress with 2 hosts and 3 endpoints
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: helloworld-rules
spec:
  rules:
  - host: helloworld-v1.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: helloworld-v1
          servicePort: 80
  - host: helloworld-v2.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: helloworld-v2
          servicePort: 80

on va creer nos objects : 

boogie$ kubectl create -f  ingress/nginx-ingress-controller.yml
boogie$ kubectl create -f  ingress/ingress.yml
kubectl create -f  ingress/echoservice.yml
boogie$ kubectl create -f  ingress/helloworld-v1.yml  
boogie$ kubectl create -f  ingress/helloworld-v2.yml          [☸ minikube:default]


si on test sans preciser de host : on va tomber sur notre default backend :


boogie$ curl http://192.168.99.100                            [☸ minikube:default]
default backend - 404%

si on test on forgeant un host header : 

boogie$ curl -H "Host: helloworld-v1.example.com" http://192.168.99.100
Hello World !%

boogie$ curl -H "Host: helloworld-v2.example.com" http://192.168.99.100
Hello World v2!%

                          
= external Dns :

https://github.com/kubernetes-incubator/external-dns


on va pouvoir utiliser un lb externe et rediriger le traffic vers un ingress controller pour les flux nécéssitant un fitrage ou redirect : ex typique pour les flux http / https

on va pouvoir utiliser un externaldns pour configurer les entrées dns des services que l'on va publier dans notre loadbalancer externe

pour chaque hostname que l'on utilise dans notre ingress on va pouvoir créer une entrée afin de diriger les flux vers notre lb puis vers notre ingress

la majorité des providers dns sont supportés.

on peut donc avoir une conf de type :


internet  

dns provider (route53 , pdns ..) 

lbexterne (metallb, elastic load balancer) 
 
service ingress   pod nginx ingress-controller

pod external dns 


on va créer des rule ingress qu'on va mettre a jour dans notre external dns 
notre external dns va envoyer les entrées dans le dns provider 
les requtees internet vont interroger le provider dns qu renvoit le reponses au client qui va initier la connection vers le loadbalancer externe qui contient les infos de services et va rediriger vers ingress qui a son tour va rediriger vers les bon pods.


on va pouvoir utiliser des deploiements pour gérer notre externaldns :

ex: 
boogie$ cat external-dns/external-dns.yaml                    [☸ minikube:default]
apiVersion: v1
kind: ServiceAccount
metadata:
  name: external-dns
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: external-dns
rules:
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get","watch","list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get","watch","list"]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: ["get","watch","list"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["list"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: external-dns-viewer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: external-dns
subjects:
- kind: ServiceAccount
  name: external-dns
  namespace: default
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: external-dns
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      serviceAccountName: external-dns
      containers:
      - name: external-dns
        image: registry.opensource.zalan.do/teapot/external-dns:latest
        args:
        - --source=service
        - --source=ingress
        - --domain-filter=kubernetes.newtech.academy # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones
        - --provider=aws
        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization
        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)
        - --registry=txt
        - --txt-owner-id=kubernetes.newtech.academy


= volumes : 

on va utiliser les volumes pour stocker des données en dehors des containers : puisqu'un container qui s'arrête perd ses données.
Les volumes persistents de kube vont permettre d'attacher un volume a un container même si celui-ci s'arrête dans ce cas le volume contenant les data pourra être rattaché à un nouveau container.
Les volumes peuvent être attachés via des volumes plugins (local, aws, gcp, ceph, nfs ...)
En utilisant des volumes on peut deployer des applications statefull : ces applis doivent pouvoir lire et ecrire sur le filesystem local qui doit être persistant dans le temps.
On peut donc faire tourner un mysql en utilisant un stockage persistent . Attention la gestion des volumes est encore récente dans kube.

1/ 
On va d'abord devoir créer un volume (on choisi en fonction du plugin manager que l'on veut utiliser) 
ex : 
## Create Volume in AWS

```
aws ec2 create-volume --size 10 --region your-region --availability-zone your-zone --volume-type gp2 --tag-specifications 'ResourceType=volume, Tags=[{Key= KubernetesCluster, Value=kubernetes.domain.tld}]'

2/ on va créer notre déploiment en précisant le point de montage du volume, on précise le volume , le plugin utilsié et l'id du volume (dans notre cas c'est de l'aws) :

boogie$ cat volumes/helloworld-with-volume.yml                                                  [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
        volumeMounts:
        - mountPath: /myvol
          name: myvolume
      volumes:
      - name: myvolume
        awsElasticBlockStore:
          volumeID: # insert AWS EBS volumeID here


- volume provisionning 
on va pouvoir créer nos volumes en fonction de nos volumes plugins avant de les allouer aux pods concernés.

on va declarer un volume :
ex: 

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  zone: us-east-1


on va maintenant associer ce volume a la physical volume claim : requete / demande de volume physique nécéssaire à notre pod :

# pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
  namespace: test
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi


cat first-app/helloworld.yml                                                            [☸ minikube:default]
---
apiVersion: v1
kind: Pod
metadata:
  name: nodehelloworld.example.com
  labels:
    app: myapp
spec:
  containers:
  - name: k8s-demo
    image: wardviaene/k8s-demo
    ports:
    - name: nodejs-port
      containerPort: 3000
    volumeMount: 
    - mountPath: "/var/www/html"
      name: mypvc
  volumes:
  - name: mypvc
    persistentVolumeClaim:
      claimName: myclaim   <<<<< on reference ici le nom du volume claim qu'on a déclarer dans le PersistentVolumeClaim
  nodeSelector:
    environment: lab

= pod presets : 

on va pouvoir injecter de la config dans nos containers au runtime.
on va pouvoir factoriser des data qu'on va mettre dans un pod preset afin de deployer ces ressources sur tous les pods comportant un label particulier : plutôt que dupliquer des conf similaires dans plusieurs manifests de pod /deployment.

ex: 

on defini un pod-preset qui contient des données qui s'appliqueront aux pods comportant le label matchant dans le pod preset :

boogie$ cat pod-presets/pod-presets.yaml                      [☸ minikube:default]
apiVersion: settings.k8s.io/v1alpha1   # you might have to change this after PodPresets become stable
kind: PodPreset
metadata:
  name: share-credential
spec:
  selector:
    matchLabels:
      app: myapp    <<<<<< tous les pods comportants ce label hériteront des conf suivantes : variable d'env, volume ....
  env:
    - name: MY_SECRET
      value: "123456"
  volumeMounts:
    - mountPath: /share
      name: share-volume
  volumes:
    - name: share-volume
      emptyDir: {}

par exemple le deploiment suivant verra ses pods hériter des data du pod preset :
boogie$ cat pod-presets/deployments.yaml                      [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: deployment-2
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000


en examinant les pods issus de ces déployment on verra les points de montage ..qui sont injectés via les pods preset ..

= statefulsets : 

Ils vont être utiles pour des appli qui vont avoir besoin d'un nom de pod stable et unique (contrairement au deploiement qui genere des noms de pod en random) 
les pods vont avoir un index d'identité : ex pod-0, pod-1 ... quand ils vont être reschedule ils vont toujours avoir ce meme suffixe.

Ceci va être particulierement utile pour les applis qui utilisent des noms dns pour communiquer : ex elasticsearch, cassandra 

Les statefulsets permettent la gestion de stockage : le volume associé a un statefulset n'est pas delete quand le statefulset lui meme est détruit.

QUand on scale une appli statefulset : l'incrementation des index se fait automatiquement par kube :

on va passer de 2 à  3 replicats en passant donc de pod-0, pod-1 à pod-0, pod-1, pod-2

idem quand on va scale down :
on passera de pod-0, pod-1, pod-2 à pop-0, pod-1 

on va pouvoir prendre comme exemple un cluster cassandra qui a besoin d'un node "seed" : une source avec laquelle les autres membres du cluster doivent communiquer :


on va créer notre appli comportant 3 nodes dont le nom sera fixe , ces nodes reprendront le meme hostname  s'ils sont detruit , un volume est présenté aux pods qui vont l'utiliser 

boogie$ cat statefulset/cassandra.yaml                                 [☸ N/A:N/A]
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
  labels:
    app: cassandra
spec:
  serviceName: cassandra
  replicas: 3
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
      - name: cassandra
        image: gcr.io/google-samples/cassandra:v13
        imagePullPolicy: Always
        ports:
        - containerPort: 7000
          name: intra-node
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx
        - containerPort: 9042
          name: cql
        resources:
          limits:
            cpu: "500m"
            memory: 1Gi
          requests:
           cpu: "500m"
           memory: 1Gi
        securityContext:
          capabilities:    <<<<< ici on defini des droits particuliers nécéssaire à cassandra : les IPC_LOCK 
            add:
              - IPC_LOCK
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - nodetool drain
        env:
          - name: MAX_HEAP_SIZE
            value: 512M
          - name: HEAP_NEWSIZE
            value: 100M
          - name: CASSANDRA_SEEDS
            value: "cassandra-0.cassandra.default.svc.cluster.local"   <<< on aura ici le nom de notre seed qui servira au cluster 
          - name: CASSANDRA_CLUSTER_NAME
            value: "K8Demo"
          - name: CASSANDRA_DC
            value: "DC1-K8Demo"
          - name: CASSANDRA_RACK
            value: "Rack1-K8Demo"
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - /ready-probe.sh
          initialDelaySeconds: 15
          timeoutSeconds: 5
        # These volume mounts are persistent. They are like inline claims,
        # but not exactly because the names need to match exactly one of
        # the stateful pod volumes.
        volumeMounts:
        - name: cassandra-data
          mountPath: /cassandra_data
  # These are converted to volume claims by the controller
  # and mounted at the paths mentioned above.
  # do not use these in production until ssd GCEPersistentDisk or other ssd pd
  volumeClaimTemplates:
  - metadata:
      name: cassandra-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard
      resources:
        requests:
          storage: 8Gi
---
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  zone: eu-west-1a
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cassandra
  name: cassandra
spec:
  clusterIP: None
  ports:
  - port: 9042
  selector:
    app: cassandra


= daemonset : 
on va s'assurer que toutes les nodes d'un cluster hébergent un type particulier de
ressource.
Ex: on veut s'assurer que chaque node possede un pod particulier 

en cas d'utilisation typique on a :

-> les aggregateurs de logs 
-> les monitor
-> les lb, reverse proxy, api gateway 

ex: 
boogie$ cat ingress/nginx-ingress-controller.yml              [☸ minikube:default]
# updated this file with the latest ingress-controller from https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: nginx-ingress-controller
spec:
  selector:
    matchLabels:
      app: ingress-nginx
  template:
    metadata:
      labels:
        app: ingress-nginx
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.17.1
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/echoheaders-default
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            capabilities:
                drop:
                - ALL
                add:
                - NET_BIND_SERVICE
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
          - name: http
            containerPort: 80
            hostPort: 80
          - name: https
            containerPort: 443
            hostPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1


= ressource monitoring =

heapster permet de monitorer un cluster et d'analyser les perfs.
il exporte ses metrics via des endpoints REST
on peut l'utiliser avec des backends differents ( influxdb, kafka ...)


=== notes the complete kubernetes course  ===

= ch1 intro =

- Commandes utiles  : 

- se plugger a une pod en cours d'excution : 
kubectl attach podname   

 kubectl attach nodehelloworld.example.com                                                                                                  [☸ minikube:default]
Defaulting container name to k8s-demo.
Use 'kubectl describe pod/nodehelloworld.example.com -n default' to see all of the containers in this pod.
If you don't see a command prompt, try pressing enter.

- execution de commande au sein d'un pod :

boogie$ kubectl exec debian-pod -- ip a                                                                                                        [☸ minikube:default]
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
12: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.5/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever

boogie$ kubectl -it exec debian-pod -- bash                                                                                                        [☸ minikube:default]
root@debian-pod:/# date
Tue Sep 10 19:28:13 UTC 2019


- port forwarding : on va pouvoir rediriger un port de notre machine vers le port d'un pod ecoutant dessus :
boogie$ kubectl port-forward nodehelloworld.example.com 8080:3000                                                                [☸ minikube:default]
Forwarding from [::1]:8080 -> 3000

sur notre poste :
curl http://localhost:8080                            [☸ minikube:default]
{
  "paths": [
    "/api",
    "/api/v1",
    "/apis",
    "/apis/",
    "/apis/admissionregistration.k8s.io",
...
boogie$ kubectl logs nodehelloworld.example.com               [☸ minikube:default]
npm info it worked if it ends with ok
npm info using npm@2.15.11
npm info using node@v4.6.2
npm info prestart myapp@0.0.1
npm info start myapp@0.0.1

> myapp@0.0.1 start /app
> node index.js

Example app listening at http://:::3000


- premiers manifests :

pod :
boogie$ cat first-app/helloworld.yml                                                                                                               [☸ minikube:default]
apiVersion: v1
kind: Pod
metadata:
  name: nodehelloworld.example.com
  labels:
    app: helloworld
spec:
  containers:
  - name: k8s-demo
    image: wardviaene/k8s-demo
    ports:
    - name: nodejs-port
      containerPort: 3000

service avec node port qui va matcher le container du pod précédent 

boogie$ cat first-app/helloworld-nodeport-service.yml                                                                                              [☸ minikube:default]
apiVersion: v1
kind: Service
metadata:
  name: helloworld-service
spec:
  ports:
  - port: 31001
    nodePort: 31001
    targetPort: nodejs-port
    protocol: TCP
  selector:
    app: helloworld
  type: NodePort


=== ch2 bases ===

= nodes :

kubelet -> pilote les set up de container dans les pods
kubeproxy > va s'occuper du routage des pods : permettre a tous les pods d'être contactables depuis le cluster et hors cluster en alimentant les regles iptables 

= replication controller : 

pour une appli stateless ( qui n'ecrit dans aucun fichier local ) pas d'etat : on va facilement pouvoir scaler.
La plupart des applis web sont stateless contrairement aux dbs qui sont stateful 
Attention pour les infrmations de sessions users web doivent être stockées hors containers.
Aucun fichier ne doit être sauvegarder dans un pod qui de part leur nature sont ephémeres.

On va scaller avec un object s'appelant replication controller : on defini le nombre de pod qu'on veut et kube s'assure que le nombre de ses containers actifs est toujours présent. Le container sera toujours recréee si il est delete ou ko 

boogie$ cat replication-controller/helloworld-repl-controller.yml                                                                                  [☸ minikube:default]
apiVersion: v1
kind: ReplicationController
metadata:
  name: helloworld-controller
spec:
  replicas: 2      <<< on defini le nombre de réplica
  selector:
    app: helloworld  <<< le selector va matcher les pods ayant le label app: helloworld
  template:
    metadata:
      labels:
        app: helloworld    <<<< le label de notre pod
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port      <<< on defini un nom a notre port pour l'identifier plus facilement lors d'appels ulterieurs (utiles pour les ports moins connus que 80 /443 par ex)
          containerPort: 3000


On voit qu"on a bien deux pods issus de notre replication controller : 

boogie$ kubectl get pod                                                                                                                            [☸ minikube:default]
NAME                          READY   STATUS    RESTARTS   AGE
debian-pod                    1/1     Running   9          36d
helloworld-controller-dv92r   1/1     Running   1          11h
helloworld-controller-ndlrd   1/1     Running   1          11h

si on delete un pod : le controller va en réinstancier un automatiquement :

boogie$ kubectl delete pod helloworld-controller-dv92r                                                                                             [☸ minikube:default]
pod "helloworld-controller-dv92r" deleted

boogie$ kubectl get pod                                                                                                                            [☸ minikube:default]
NAME                          READY   STATUS    RESTARTS   AGE
debian-pod                    1/1     Running   9          36d
helloworld-controller-fxhs8   1/1     Running   0          36s   <<< on voit ici qu'un nouveau pod a été popé pour remplacer celui qui a été détruit.
helloworld-controller-ndlrd   1/1     Running   1          11h

on peut scale en live le nombre de réplicat :

->on peut passer le fichier de replica en argumant en specifiant le nombre de pod désiré avant : 

boogie$ kubectl scale --replicas=3 -f helloworld-repl-controller.yml                                                                               [☸ minikube:default]
replicationcontroller/helloworld-controller scaled
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course/replication-controller (master) [10:00:37]
boogie$ kubectl get pod                                                                                                                            [☸ minikube:default]
NAME                          READY   STATUS    RESTARTS   AGE
debian-pod                    1/1     Running   9          36d
helloworld-controller-fxhs8   1/1     Running   0          5m8s
helloworld-controller-ndlrd   1/1     Running   1          11h
helloworld-controller-qw6c4   1/1     Running   0          6s

->  on peut directement modifier le type d'objet replicationcontroller ( rc ) en le passant en argument :
ex : on reduit le nombre de pod en utilisant le nom du rc qu'on recupere avant avec un get rc :
 ~ [10:17:32]
boogie$ kubectl get rc                                                                                                                             [☸ minikube:default]
NAME                    DESIRED   CURRENT   READY   AGE
helloworld-controller   3         3         3       11h

boogie$ kubectl scale --replicas=1 rc/helloworld-controller                                                                                        [☸ minikube:default]
replicationcontroller/helloworld-controller scaled
 ~ [10:19:40]

boogie$ kubectl get pod                                                                                                                            [☸ minikube:default]
NAME                          READY   STATUS             RESTARTS   AGE
debian-pod                    1/1     Running            10         36d
helloworld-controller-fxhs8   0/1     ImagePullBackOff   0          24m
boogie$ kubectl get rc                                                                                                                             [☸ minikube:default]
NAME                    DESIRED   CURRENT   READY   AGE
helloworld-controller   1         1         0       11h

Biensur nous scallons de maniere horizontale notre appli c'est qu'elle est stateless 
on peut delete notre replicationcontroller facilement :

boogie$ kubectl delete rc helloworld-controller                                                                                                    [☸ minikube:default]
replicationcontroller "helloworld-controller" deleted
 ~ [10:22:41]

boogie$ kubectl get rc                                                                                                                             [☸ minikube:default]
No resources found.
 ~ [10:22:43]
boogie$ kubectl get pod                                                                                                                            [☸ minikube:default]
NAME                          READY   STATUS        RESTARTS   AGE
debian-pod                    1/1     Running       10         36d
helloworld-controller-fxhs8   0/1     Terminating   0          27m


= Deployment : 

on va d'abord  voir le replicaset qui est la version maintenue par kube puisque le replication  controller est deprecated.
on va pouvoir dans cet object utiliser des selectors qui vont nous permettre de filtrer de manière plus étendu qu'avec les replicationcontroller qui se contentaient de pouvoir fournir des filtres d'egalite ( ex :env: prod) 

le replicaset est utilisé dans l'object deployment : le deployment va nous permettre de déployer des app, de les updater . On defini un status pour notre appli et kube assure que ce status est bien up and running.
L'object deployment est plus facile a utiliser que le replicatcontroller / replicaset , demande moins d'intervention manuelle et offre plus de possiblilitées.

-> create un deployment
-> update un deployment 
-> rolling update : deployment sans impact
-> rollback a une version donnée
-> pause / resume : deployment d'un certain pourcentage de nos pods.

cat deployment/helloworld.yml 

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000

- deployments commandes :

- kubectl get deployments
- kubectl get rs ( replicaset)
- kubectl get pods --show-labels                                                                                                             [☸ minikube:default]
NAME                         READY   STATUS    RESTARTS   AGE    LABELS
debian-pod                   1/1     Running   13         38d    <none>
nodehelloworld.example.com   1/1     Running   2          2d5h   app=helloworld

- kubectl rollout status  deployment/helloworld-deployment  : etat du deployment 
- kubectl set image deployment/helloworld-deployment k8s-demo=k8s-demo:2 run k8s-demo avec l'image k8s-demo:2 du label
- kubectl edit deployment/helloworld-deployment  : edition du deployment 
- kubectl rollout history deployment/helloworld-deployment  permet de  voir l'historique des versions déployées.
- kubectl rollout undo  deployment/helloworld-deployment permet de rollback sur la version déployée précédemment.
- kubectl rollout undo  deployment/helloworld-deployment --to-revision=N permet de rollback sur la version N déployée précédemment.


apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000

 kubectl create -f deployment/helloworld.yml

boogie$ kubectl get deployments                                                                                                                    [☸ minikube:default]
NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
helloworld-deployment   3/3     3            3           2m8s

boogie$ kubectl get replicasets                                                                                                                    [☸ minikube:default]
NAME                               DESIRED   CURRENT   READY   AGE
helloworld-deployment-748f49d795   3         3         3       2m20s


boogie$ kubectl get pods --show-labels                                                                                                             [☸ minikube:default]
NAME                                     READY   STATUS    RESTARTS   AGE     LABELS
debian-pod                               1/1     Running   13         38d     <none>
helloworld-deployment-748f49d795-56h2r   1/1     Running   0          3m26s   app=helloworld,pod-template-hash=748f49d795
helloworld-deployment-748f49d795-n4fnd   1/1     Running   0          3m26s   app=helloworld,pod-template-hash=748f49d795
helloworld-deployment-748f49d795-rvczv   1/1     Running   0          3m26s   app=helloworld,pod-template-hash=748f49d795
nodehelloworld.example.com               1/1     Running   2          2d5h    app=helloworld

-> check de l'etat du deployment : 
boogie$ kubectl rollout status deployment helloworld-deployment                                                                                    [☸ minikube:default]
deployment "helloworld-deployment" successfully rolled out

-> modification de l'image utilisée : deploy d'une nouvelle version de notre app: 
boogie$ kubectl set image deployment/helloworld-deployment k8s-demo=wardviaene/k8s-demo:2                                                          [☸ minikube:default]
deployment.extensions/helloworld-deployment image updated
boogie$ kubectl get pods  --show-labels                                                                                                            [☸ minikube:default]
NAME                                     READY   STATUS        RESTARTS   AGE    LABELS
debian-pod                               1/1     Running       13         38d    <none>
helloworld-deployment-748d88f59f-8xw74   1/1     Running       0          36s    app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748d88f59f-pwzl9   1/1     Running       0          30s    app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748d88f59f-zh27v   1/1     Running       0          36s    app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748f49d795-56h2r   1/1     Terminating   0          11m    app=helloworld,pod-template-hash=748f49d795
helloworld-deployment-748f49d795-n4fnd   1/1     Terminating   0          11m    app=helloworld,pod-template-hash=748f49d795
nodehelloworld.example.com               1/1     Running       2          2d5h   app=helloworld


boogie$ kubectl describe pod helloworld-deployment-748d88f59f-pwzl9                                                                                [☸ minikube:default]
Name:           helloworld-deployment-748d88f59f-pwzl9
Namespace:      default
Priority:       0
Node:           minikube/10.0.2.15
Start Time:     Fri, 13 Sep 2019 16:24:16 +0200
Labels:         app=helloworld
                pod-template-hash=748d88f59f
Annotations:    <none>
Status:         Running
IP:             172.17.0.15
Controlled By:  ReplicaSet/helloworld-deployment-748d88f59f
Containers:
  k8s-demo:
    Container ID:   docker://6d211169aa8da1588bf5a96d01e6df1e19646e53142e41f8030a1391a25abfc4
    Image:          wardviaene/k8s-demo:2
    Image ID:       docker-pullable://wardviaene/k8s-demo@sha256:c7536949ff900fb7dc923cf9f2475d1209766c65aa07325caf880e754e7e0fae
    Port:           3000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 13 Sep 2019 16:24:18 +0200
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wfc8r (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-wfc8r:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-wfc8r
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  61s   default-scheduler  Successfully assigned default/helloworld-deployment-748d88f59f-pwzl9 to minikube
  Normal  Pulling    60s   kubelet, minikube  Pulling image "wardviaene/k8s-demo:2"
  Normal  Pulled     59s   kubelet, minikube  Successfully pulled image "wardviaene/k8s-demo:2"
  Normal  Created    59s   kubelet, minikube  Created container k8s-demo
  Normal  Started    59s   kubelet, minikube  Started container k8s-demo

on a donc notre nouvelle appli déployée.

- Examen de l'historique des versions déployés :

boogie$ kubectl rollout history  deployment/helloworld-deployment                                                                                  [☸ minikube:default]
deployment.extensions/helloworld-deployment
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

on va pouvoir rollback et revenir a notre version précédente : 

boogie$ kubectl rollout undo  deployment/helloworld-deployment                                                                                     [☸ minikube:default]
deployment.extensions/helloworld-deployment rolled back

le rollback se passe bien ; 

boogie$ kubectl rollout status  deployment/helloworld-deployment                                                                                   [☸ minikube:default]
Waiting for deployment "helloworld-deployment" rollout to finish: 2 of 3 updated replicas are available...
deployment "helloworld-deployment" successfully rolled out

on voit nos différents  pods : 
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course (master) [04:32:27]
boogie$ kubectl get pods  --show-labels                                                                                                            [☸ minikube:default]
NAME                                     READY   STATUS        RESTARTS   AGE     LABELS
debian-pod                               1/1     Running       13         38d     <none>
helloworld-deployment-748d88f59f-8xw74   1/1     Terminating   0          8m22s   app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748d88f59f-pwzl9   1/1     Terminating   0          8m16s   app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748d88f59f-zh27v   1/1     Terminating   0          8m22s   app=helloworld,pod-template-hash=748d88f59f
helloworld-deployment-748f49d795-fp79k   1/1     Running       0          11s     app=helloworld,pod-template-hash=748f49d795
helloworld-deployment-748f49d795-gp8qd   1/1     Running       0          11s     app=helloworld,pod-template-hash=748f49d795
helloworld-deployment-748f49d795-lddvl   1/1     Running       0          8s      app=helloworld,pod-template-hash=748f49d795
nodehelloworld.example.com               1/1     Running       2          2d5h    app=helloworld

on peut changer le nombre d'historique de version conservé de notre déployment :
avec le param revisionHistoryLimit: XX que l'on set dans notre déployment : 
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 3
    revisionHistoryLimit: 100
    selector:
      matchLabels:
        app: helloworld

on peut rollback : 
boogie$ kubectl rollout undo  deployment/helloworld-deployment                                                                                     [☸ minikube:default]
deployment.extensions/helloworld-deployment rolled back

boogie$ kubectl rollout history deployment                                                                                                         [☸ minikube:default]
deployment.extensions/helloworld-deployment 
REVISION  CHANGE-CAUSE
4         <none>

- on utilise une nouvelle image : 
boogie$ kubectl set image deployment/helloworld-deployment k8s-demo=wardviaene/k8s-demo:1                                                          [☸ minikube:default]
deployment.extensions/helloworld-deployment image updated

boogie$ kubectl rollout status  deployment/helloworld-deployment                                                                                   [☸ minikube:default]
Waiting for deployment "helloworld-deployment" rollout to finish: 2 out of 3 new replicas have been updated...

on voit qu'on a plusieurs versions historisées : 
boogie$ kubectl rollout history  deployment/helloworld-deployment
deployment.extensions/helloworld-deployment 
REVISION  CHANGE-CAUSE
4         <none>
5         <none>
6         <none>
7         <none>

on peut donc forcer le rollback dans une version particulière : 

boogie$ kubectl rollout undo  deployment/helloworld-deployment --to-revision=5                                                                     [☸ minikube:default]
deployment.extensions/helloworld-deployment rolled back


= services : 

les pods sont éphémeres  par nature :( detruits pendant les déployments etc ..)  nous ne devons pas acceder directement à eux : une couche intermédiaire est défini pour relier les clients ( au sens large: services, end-users ) aux services portés par les pods .
quand on utilise kubectl expose : on crée un service popur notre pod qui pourra être atteind de l'exterrieur .
Quand on crée un service on va créer un endpoint pour nos pods 
On peut utiliser plusieurs conf :

-> clusterip : ip uniquement accésible dans notre cluster kube ( c'est la conf de base si on ne précise rien : de base on crée un service en clusterip )
-> nodeport: c'est un port identique sur tous les nodes de notre cluster pour atteindre le service de l'exterrieur : on va utiliser un port situé dans un range entre 30000et 32267 pour faire matcher le port du pod sur le nodeport qui lui sera sticker sur le /les nodes de notre cluster.
-> loadbalancer : c'est utilisé dans le cloud. on va router le traffic externe vers le nodeport de nos nodes kube 

Les options montrées permettrent uniquement le creation de virtualsip ou de ports

on peut utiliser des entrées dns 
ExternalName peut fournir un nom dns a nos services.
ex : service discovery pour le dns.

-  service demo : 

boogie$ cat first-app/helloworld.yml                                                                                                               [☸ minikube:default]
apiVersion: v1
kind: Pod
metadata:
  name: nodehelloworld.example.com
  labels:
    app: helloworld
spec:
  containers:
  - name: k8s-demo
    image: wardviaene/k8s-demo
    ports:
    - name: nodejs-port
      containerPort: 3000

boogie$ cat first-app/helloworld-nodeport-service.yml                                                                                              [☸ minikube:default]
apiVersion: v1
kind: Service
metadata:
  name: helloworld-service
spec:
  ports:
  - port: 31001
    nodePort: 31001
    targetPort: nodejs-port
    protocol: TCP
  selector:
    app: helloworld
  type: NodePort

kubectl create -f first-app/helloworld.yml
kubectl create -f first-app/helloworld-nodeport-service.yml

boogie$ kubectl get svc                                                                                                                            [☸ minikube:default]
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE
helloworld-service   NodePort    10.106.205.39   <none>        31001:31001/TCP   13h
kubernetes           ClusterIP   10.96.0.1       <none>        443/TCP           36d

on recupere le couple ip/port exposé dans minikube 
boogie$ minikube service helloworld-service --url                                                                                                  [☸ minikube:default]
http://192.168.99.100:31001

on voit sur notre poste qu'un reseau est monté dans le range :
boogie$ ip a |grep vboxnet3                                                                                                                        [☸ minikube:default]
8: vboxnet3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    inet 192.168.99.1/24 brd 192.168.99.255 scope global vboxnet3


on voit en examinant le service crée qu'on a bien un service de type NodePort et une ClusterIp : cette ip n'est joignable que de notre cluster.

boogie$ kubectl get svc                                                                                                                            [☸ minikube:default]
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE
helloworld-service   NodePort    10.109.171.91   <none>        31001:31001/TCP   3m14s
kubernetes           ClusterIP   10.96.0.1       <none>        443/TCP           38d

La ClusterIp est une ip virtuelle qui change si on delete et on recree notre service . On peut fixer l'ip si on veut dans le yaml de notre service .
Comme pour le port static qu'on défini si on veut dans notre yaml

= labels : 

les labels sont des paires clé /valeurs qui peuvent être attachées à un object.
Ils sont utilisés pour tagguer les objects.
On peut taguer un pod par exemple en mettant un label de type key: environment valeur: dev/ prod ...
On peut mettre plusieurs labels sur un object : on peut rajouter sur notre pod précédent un label de type key: department / valeur: finance / it ...
Une fois que le/s label/s est /sont attachés a un object on va pouvoir les utiliser pour filtrer nos besoins en utilisant un selector.
Un label selector peut utiliser des regexp pour matcher notre object
ex: on peut dire qu'un pod ne doit tourner que dur un node avec le label environment equal dev ,  ou environment equal 'dev' or 'qa'

On va pouvoir tagger un node puis dire qu'un pod ira uniquement sur un node contenant le label : ceci se fera grace au nodeSelector qu'on ajoutera dans notre config.

- nodes labels :

ex :on peut ajouter un tag en cli :

boogie$ kubectl label node minikube environment=lab                                                                                                                                                                      [☸ minikube:default]
node/minikube labeled

on peut voir notre label :

boogie$ kubectl get nodes --show-labels                                                                                                                                                                                  [☸ minikube:default]
NAME       STATUS   ROLES    AGE   VERSION   LABELS
minikube   Ready    master   39d   v1.15.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,environment=lab,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,node-role.kubernetes.io/master=

On va pouvoir ensuite définir un pod en specifiant qu'il doit s"executer uniquement sur les nodes flaggués lab :

  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: nodehelloworld.example.com
    labels:
      app: helloworld
  spec:
    containers:
   - name: k8s-demo
      image: wardviaene/k8s-demo
      ports:
   - name: nodejs-port
        containerPort: 3000
   nodeSelector:
      environment: lab

Tant qu'un node avec le tag environment: lab n'existe pas alors notre object ne peut pas démarré.


= healthcheck :

On peut avoir un pod démarré mais qui ne répond pas vraiment : l'appli est donc ko. Il faut pouvoir nous assurer que le pod est bien  fonctionnel.

Pour detecter et resoudre des pb de fonctionnemnt on va ajouter un healtchcheck 

2 types existent :
-> lancer une commande périodiquement 
-> executer un check http sur une url



boogie$ cat deployment/helloworld-healthcheck.yml                                               [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
        livenessProbe:    <<< definition de la section du healthcheck 
          httpGet:        <<<< check http
            path: /       <<<<  on fait un get sur / 
            port: nodejs-port  <<< sur le port nodejs-port defini dans la section de notre notre pod 
          initialDelaySeconds: 15  <<<< on attend 15 secondes avant le premier check 
          timeoutSeconds: 30    <<<< on considere que le pod est ko apres 30 secondes sans réponse.

On voit quand on affiche la description du pod apres le deployment la section de la sonde :

..
    Liveness:       http-get http://:nodejs-port/ delay=15s timeout=30s period=10s #success=1 #failure=3
...
Quand on edite le déployment poussé dans kube on voit le detail que kube ajoute pour les healtchcheck 

    spec:
        containers:
>>      - image: wardviaene/k8s-demo
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3   <<< nombre d'occurence de fail conisdérer en critical : le pod sera redémarré
            httpGet:
              path: /
              port: nodejs-port
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10  <<<< frequence de check 
            successThreshold: 1   <<< nombre de fois ou le check est considérer comme ok 
            timeoutSeconds: 30


= readiness probe :

On a vu que le healtcheck va servir a verifier l'etat du pod : si celui ci est ko alors kube en redemarre un
Une readiness probe va nous assurer que le pod est bien pret a servir les requettes . Si le test est ko alors le pod n'est pas redémarré et son ip est supprimée de la liste des ip des pods du service. 

Au demarrage de notre object ( pod ,deployment ..) : l'object pourra être running mais pas ready : tant que la sonde de readiness ne sera pas passé et aura considérer le pod ok puis que répondant aux requettes.
On va mettre conjointement readiness et healtcheck dans la def de nos objects :
boogie$ cat deployment/helloworld-liveness-readiness.yml                                        [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-readiness
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
        livenessProbe:
          httpGet:
            path: /
            port: nodejs-port
          initialDelaySeconds: 15
          timeoutSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: nodejs-port
          initialDelaySeconds: 15
          timeoutSeconds: 30

NAME                                  READY   STATUS    RESTARTS   AGE
helloworld-readiness-dc8fc5bc-d9phj   0/1     Running   0          11s
helloworld-readiness-dc8fc5bc-gm6kx   0/1     Running   0          11s
helloworld-readiness-dc8fc5bc-tcwbx   0/1     Running   0          11s


= pods states :


les pods peuvent avoir plusieurs status :


-> running : ils sont en cours d'execution
-> pending : en cours de creation ( recup d'image , attente de ressources définie dans l'object : label , memoire ,cpu .. resources ...) 
-> succeeded : tous les pods ont été correctement déployés
-> failed : les containers dans le pod sont en etat terminated et au moins un container a renvoyé un code retour d'erreur.
-> unknow : impossible de connaitre l'etat

on peut connaitre les differentes etapes que le pod a connu en faisant un 
kubectl describe pod mon pod 

- pods conditions : il y a differentes conditions 

-> podScheduled : le pod a été schedule sur un node
-> Ready : le pod est pret a servir du traffic et va etre ajouté pour matcher un service 
-> initialized : les containers du pod sont initialisés correctement
-> unschedulable : les pod ne peuvent pas être dispatcher : pb de contrainte de ressource ( ex pod devant etre sur un node avec 2T de ram de libre ...) 
-> containersready : les onctainers du pod sont prets

on peut avoir aussi l'etat du container avec :

boogie$ kubectl get pod helloworld-readiness-dc8fc5bc-tcwbx -n default -o yaml                  [☸ minikube:default]
apiVersion: v1
kind: Pod
metadata:
  ....

  containerStatuses:
  - containerID: docker://95e9a8c9afc2092471107fa066acaf567613c4c71da186f5ef3aa3016e53cbcc
    image: wardviaene/k8s-demo:latest
    imageID: docker-pullable://wardviaene/k8s-demo@sha256:2c050f462f5d0b3a6430e7869bcdfe6ac48a447a89da79a56d0ef61460c7ab9e
    lastState: {}
    name: k8s-demo
    ready: true
    restartCount: 0
    ....

Les containers  peuvent être en running / terminated ou waiting


exemple de fichier comportant deux containers , et deux hooks servant a differents moment de la vie de notre container :


-> un init container va être utiliser pour la prépartion d'element pour l'acceuil de notre pod :
ex : on lance un init container pour préparer un volume qui sera ensuite monter par le pod principal.

Ici dans notre exemple l'init container va juste tourner quelques seconde ..puis laisser la main au hook, au second container puis au second hook : 


boogie$ cat pod-lifecycle/lifecycle.yaml                                                        [☸ minikube:default]
kind: Deployment
apiVersion: apps/v1beta1
metadata:
  name: lifecycle
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: lifecycle
    spec:
      initContainers:
      - name:           init
        image:          busybox
        command:       ['sh', '-c', 'sleep 10']
      containers:
      - name: lifecycle-container
        image: busybox
        command: ['sh', '-c', 'echo $(date +%s): Running >> /timing && echo "The app is running!" && /bin/sleep 120']
        readinessProbe:
          exec:
            command: ['sh', '-c', 'echo $(date +%s): readinessProbe >> /timing']
          initialDelaySeconds: 35
        livenessProbe:
          exec:
            command: ['sh', '-c', 'echo $(date +%s): livenessProbe >> /timing']
          initialDelaySeconds: 35
          timeoutSeconds: 30
        lifecycle:
          postStart:
            exec:
              command: ['sh', '-c', 'echo $(date +%s): postStart >> /timing && sleep 10 && echo $(date +%s): end postStart >> /timing']
          preStop:
            exec:
              command: ['sh', '-c', 'echo $(date +%s): preStop >> /timing && sleep 10']


quand on lance on obtient plusieurs etats : 

-> notre init container se lance : 
NAME                         READY   STATUS     RESTARTS   AGE
lifecycle-86977b4c47-d7hh2   0/1     Init:0/1   0          5s

-> notre second container se lance : les post et pré hook s'executent : 
NAME                         READY   STATUS            RESTARTS   AGE
lifecycle-86977b4c47-d7hh2   0/1     PodInitializing   0          16s

NAME                         READY   STATUS    RESTARTS   AGE
lifecycle-86977b4c47-d7hh2   0/1     Running   0          28s

NAME                         READY   STATUS             RESTARTS   AGE
lifecycle-86977b4c47-d7hh2   0/1     CrashLoopBackOff   1          4m36s

NAME                         READY   STATUS    RESTARTS   AGE
lifecycle-86977b4c47-d7hh2   0/1     Running   2          4m55s


jusqu'a ce que le nombre de heatlcheck ko soient arrivés au max et que le pod ne soit plus redémarré.



= Secrets : 

les secrets dans kube vont permettre de passer des credentials, password , data à nos pods.
kube utilise les mêmes mecanismes pour passer des secret a l'api 
On peut utiliser des solutions externes à kube pour passer des secrets à nos apps (vault etc ...) mais les Secrets sont la méthode native de kube.

On peut utiliser les Secrets :
-> en variable d'environment 
-> en fichier plat : qui seront dans un volume monté par le pod et donc accessible 
-> utilisation d'une registry pour stocker les secrets : une image contenant les secrets sera pull et utilisée par un pod ..(?)

- generation de secret depuis un fichier :


boogie$ echo -n "root" > ./username.txt                                                         [☸ minikube:default]
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course (master*) [11:11:13]
boogie$ echo -n "password" > ./password.tx

boogie$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt
secret/db-user-pass created

- generation d'un secret depuis un cle ssh  , un cert tls : 

boogie$ kubectl create secret generic ssl-certificate --from-file=ssh-privatekey=~/.ssh/id_rsa  --ssl-cert=mysslcert.c (KO)


- Generation depuis un yaml :

on va encoder nos credentials via base64 

boogie$ echo -n "root" |base64                                                                  [☸ minikube:default]
cm9vdA==
boogie$ echo -n "password" |base64                                                              [☸ minikube:default]
cGFzc3dvcmQ=

secret-data.yaml

apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data: 
   password: cGFzc3dvcmQ=
   username: cm9vdA==

Une fois qu'on a créer nos secret on va pouvoir les utiliser 
On va pouvoir les utiliser de différentes manieres 

- variables d'env :
on defini l'appel dans notre pod : 

...
 env:
   - name: SECRET USERNAME   
     ValueFrom:            <<<<<<<<  on va donc maintenant rattacher les valeurs definis dans notre secret au pod
      - secretRef:
        name: app-secret
        key: username 

- on defini un volume:

boogie$ cat deployment/helloworld-secrets-volumes.yml                                           [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
        volumeMounts:
        - name: cred-volume
          mountPath: /etc/creds
          readOnly: true
      volumes:
      - name: cred-volume
        secret: 
          secretName: db-secrets


On peut voir quand on crée nos ressources , en examinant un pod que notre fichier secret est défini :

kubectl describe pod helloworld-deployment-6b6585d49c-d6748                             [☸ minikube:default]
Name:           helloworld-deployment-6b6585d49c-d6748
..
    Mounts:
      /etc/creds from cred-volume (ro)   <<<< c'est le point de montage 
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wfc8r (ro)

On va examiner depuis notre pod et voir que les secrets sont présents dans le point de montage défini : 

boogie$ kubectl exec -it helloworld-deployment-6b6585d49c-d6748 -- bash                         [☸ minikube:default]
root@helloworld-deployment-6b6585d49c-d6748:/app# ls
Dockerfile  docker-compose.yml	index-db.js  index.js  misc  node_modules  package.json
passwordroot@helloworld-deployment-6b6585d49c-d6748:/app# ls /etc/creds/          
password  username

- Cas pratique : set up wordpress en stateless ( perte de données des que notre pod est ko )


 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course (master*) [10:05:49]
boogie$ cat wordpress/wordpress-secrets.yml                                                     [☸ minikube:default]
apiVersion: v1
kind: Secret
metadata:
  name: wordpress-secrets
type: Opaque
data:
  db-password: cGFzc3dvcmQ=
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course (master*) [10:06:03]
boogie$ cat wordpress/wordpress-single-deployment-no-volumes.yml                                [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: wordpress-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:4-php7.0
        ports:
        - name: http-port
          containerPort: 80
        env:
          - name: WORDPRESS_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: wordpress-secrets
                key: db-password
          - name: WORDPRESS_DB_HOST
            value: 127.0.0.1
      - name: mysql
        image: mysql:5.7
        ports:
        - name: mysql-port
          containerPort: 3306
        env:
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: wordpress-secrets
                key: db-password


boogie$ kubectl create -f  wordpress/wordpress-single-deployment-no-volumes.yml

boogie$ kubectl get pod                                                                         [☸ minikube:default]
NAME                                    READY   STATUS              RESTARTS   AGE
wordpress-deployment-58cd589c6c-bncrs   0/2     ContainerCreating   0          9s
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course (master*) [10:07:38]
boogie$ kubectl describe pod wordpress-deployment-58cd589c6c-bncrs                              [☸ minikube:default]
Name:           wordpress-deployment-58cd589c6c-bncrs
Namespace:      default
Priority:       0
Node:           minikube/10.0.2.15
Start Time:     Sun, 15 Sep 2019 10:07:29 +0200
Labels:         app=wordpress
                pod-template-hash=58cd589c6c
Annotations:    <none>
Status:         Pending
IP:
Controlled By:  ReplicaSet/wordpress-deployment-58cd589c6c
Containers:
  wordpress:
    Container ID:
    Image:          wordpress:4-php7.0
    Image ID:
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:
      WORDPRESS_DB_PASSWORD:  <set to the key 'db-password' in secret 'wordpress-secrets'>  Optional: false
      WORDPRESS_DB_HOST:      127.0.0.1
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wfc8r (ro)
  mysql:
    Container ID:
    Image:          mysql:5.7
    Image ID:
    Port:           3306/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:
      MYSQL_ROOT_PASSWORD:  <set to the key 'db-password' in secret 'wordpress-secrets'>  Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wfc8r (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  default-token-wfc8r:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-wfc8r
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  21s   default-scheduler  Successfully assigned default/wordpress-deployment-58cd589c6c-bncrs to minikube
  Normal  Pulling    20s   kubelet, minikube  Pulling image "wordpress:4-php7.0"

on va créer un service qui va nous permettre d'atteindre notre appli :

boogie$ cat wordpress/wordpress-service.yml                                                     [☸ minikube:default]
apiVersion: v1
kind: Service
metadata:
  name: wordpress-service
spec:
  ports:
  - port: 31001
    nodePort: 31001
    targetPort: http-port
    protocol: TCP
  selector:
    app: wordpress
  type: NodePort

on a donc un nodeport et le port 31001 de notre host va rediriger vers le port http-port de notre pod :

boogie$ kubectl create -f wordpress/wordpress-service.yml                                       [☸ minikube:default]
service/wordpress-service created
boogie$ kubectl get svc                                                                         [☸ minikube:default]
NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE
kubernetes          ClusterIP   10.96.0.1        <none>        443/TCP           40d
wordpress-service   NodePort    10.103.144.132   <none>        31001:31001/TCP   12s

boogie$ minikube service wordpress-service --url                                                [☸ minikube:default]
http://192.168.99.100:31001

on va remplir les infos essentielles au set up de wordpress via le browser :

Informations nécessaires
Veuillez renseigner les informations suivantes. Ne vous inquiétez pas, vous pourrez les modifier plus tard.
ex:
Titre du site ￼: boogie-blog
Identifiant ￼: boogie
Mot de passe XXXX
Votre adresse de messagerie ￼boogie@localhost.net

...si le pod est ko ..alors on devra de nouveau refaire notre setup puisqu'on a pas défini de volume pour préserver nos data.

= webui : 

on a la possibilité de faier des actions sur notre cluster kube via un dashboard 
il faut d'abord l'installer :

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml

si un passowrd est demandé on peut le recupérer avec un 
kubectl config view

avec minikube : 
minikube dashboard
pour recupérer l'url du dashboard : 
minikube dashboard --url

- mode op setting dashboard : 

boogie$ cat dashboard/README.md                                                                                                                                                                                          
# Setting up the dashboard / Start dashboard

Create dashboard:
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
```


boogie$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created


## Create user

Create sample user (if using RBAC - on by default on new installs with kops / kubeadm):

cat dashboard/sample-user.yaml                                                          [☸ minikube:default]
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

```
kubectl create -f sample-user.yaml
serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created

```

## Get login token:
```
boogie$ kubectl -n kube-system get secret | grep admin-user                                     [☸ minikube:default]
admin-user-token-8fck6                           kubernetes.io/service-account-token   3      4m15s

boogie$ kubectl -n kube-system describe secret admin-user-token-8fck6                           [☸ minikube:default]
Name:         admin-user-token-8fck6
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: ba7e8fe6-6bec-44ac-98eb-3439ce74a5cb

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1066 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLThmY2s2Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiYTdlOGZlNi02YmVjLTQ0YWMtOThlYi0zNDM5Y2U3NGE1Y2IiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.h2TjbQkVFk5-wlyow2CIRhjO2R5cxSlPEHg0vPSK1cB446Jmnb8bNnPOe9QOfLLpSkYzpCEEZdJq_B-A3Uy165G6pGIveEBHeEYIbdTXhJGKkwG6pBQF1dZc69irZfbP5_WtwCCpVFtl5C2iRUcBj0JMn0-c6EY065j41b8vMsnBnZNpUcraNomu_oazD0ZN_YNMrpsOmVZ2YMI3mSOqHJq-uzokOlO90Dmc0cJwfDZrfPnigN78WPGxRWfky_49yXQdOcDBDVED-5NHvloOb_HkCnVCoG2L1REejdVTEzktFouYwdHmGoamqHmBlyf733PPNJ-6el0oXiwbTuAk6Q
```

## Login to dashboard
Go to http://api.yourdomain.com:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login

Login: admin
Password: the password that is listed in ~/.kube/config (open file in editor and look for "password: ..."

Choose for login token and enter the login token from the previous step


- minikube : 

 minikube dashboard --url                                                                [☸ minikube:default]
🤔  Verifying dashboard health ...
🚀  Launching proxy ...
🤔  Verifying proxy health ...
http://127.0.0.1:34313/api/v1/namespaces/kube-system/services/http:kubernetes-dashboard:/proxy/




== chapitre 3 :

= service discovery : 

dns est un service builtin lancé par le addons managers, qu'on peut voir sur un node master dans :
le service dns va être utilisé par les pods pour trouver des services tournant sur le même cluster.
Les containers au sein d'un pod n'utilsient pas le dns puisqu'ils communiquent en ip:port

ex: 

pod1
container1
service app1 / 10.0.0.1

pod2
container2
service app2 / 10.0.0.2


dans pod1 pour contacter le service app1 on fait :

host app1-service
app1-service has adress 10.0.0.1

dans pod1 pour contacter le service app2 on fait :
host app2-service
app2-service has adress 10.0.0.2

de base comme cela on interroge que les services dans le même namespace. On peut definir le namespace de recherche :
de base on est dans le namespace default : 

host app1-service.default
app1-service.default has adress 10.0.0.1

dans pod1 pour contacter le service app2 on fait :
host app2-service.default
app2-service.default has adress 10.0.0.2

Si on veut interroger via le fqdn global du cluster on utilisera :

dans pod1 pour contacter le service app2 on fait :
host app2-service.default.svc.cluster.local
app2-service.default.svc.local.cluster has adress 10.0.0.2

on peut mieux comprendre en examinant le fichier resolv.conf sur un pod :

boogie$ kubectl exec wordpress-deployment-58cd589c6c-bncrs -it -- bash                          [☸ minikube:default]
Defaulting container name to wordpress.
Use 'kubectl describe pod/wordpress-deployment-58cd589c6c-bncrs -n default' to see all of the containers in this pod.
root@wordpress-deployment-58cd589c6c-bncrs:/var/www/html# cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5

on voit que notre resolveur est 10.96.0.10 
le domain de recherche  default.svc.cluster.local   svc.cluster.local cluster.local
on voit que cette ip correspond au service dns qui est exposé dans notre cluster kube dans le namespace kube-system :

boogie$ kubectl get svc -n kube-system                                                          [☸ minikube:default]
NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
..
kube-dns               ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   40d
..

Les services crées sont publiés dans le dns. On pourra donc dans notre cluster interroger les différents services : il faudra biensur préciser le namespace de celui ci s'il est situé en dehors du namespace de notre pod /appli :

boogie$ kubectl exec wordpress-deployment-58cd589c6c-bncrs -it -- bash                          [☸ minikube:default]
Defaulting container name to wordpress.
Use 'kubectl describe pod/wordpress-deployment-58cd589c6c-bncrs -n default' to see all of the containers in this pod.
root@wordpress-deployment-58cd589c6c-bncrs:/var/www/html# host tiller-deploy
Host tiller-deploy not found: 3(NXDOMAIN)
root@wordpress-deployment-58cd589c6c-bncrs:/var/www/html# host tiller-deploy.kube-system
tiller-deploy.kube-system.svc.cluster.local has address 10.105.162.15


boogie$ kubectl get svc -n kube-system                                                          [☸ minikube:default]
NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default-http-backend   NodePort    10.101.219.90   <none>        80:30001/TCP             40d
kube-dns               ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   40d
kubernetes-dashboard   ClusterIP   10.98.41.81     <none>        80/TCP                   40d
tiller-deploy          ClusterIP   10.105.162.15   <none>        44134/TCP                40d


= configmap : 

on peut mettre de la config non secrete dans un config map : le principe est le même que pour les secrets.
Il s'agit de couple key/values
Les configmap peuvent être lues par l'appli en utilsiant :
> des variables d'env
> des arguments de la cli du container
> via un volume
Un configmap peut contenir un fichier complet de conf d'une appli : ex : une conf de vhost nginx
On va pouvoir monter le volume dans un container : dans ce volume on aura la conf de notre appli. On en touche pas à la conf du container pour autant.

Ex: on a une appli qui ecoute  sur le port 3000 : on va créer une conf de reverse proxy qui va forward les appels sur le port 80 à notre appli sur le port 3000. Cette conf va être notre configmap :

boogie$ cat configmap/reverseproxy.conf                                                         [☸ minikube:default]
server {
    listen       80;
    server_name  localhost;

    location / {
        proxy_bind 127.0.0.1;
        proxy_pass http://127.0.0.1:3000; <<< on redirige vers l'appli locale sur le port 3000
    }

    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}

boogie$ kubectl create configmap nginx-config --from-file=configmap/reverseproxy.conf           [☸ minikube:default]
configmap/nginx-config created

On va check la creation : 
boogie$ kubectl get configmaps                                                                  [☸ minikube:default]
NAME           DATA   AGE
nginx-config   1      49s

boogie$ kubectl get configmaps -o yaml                                                          [☸ minikube:default]
apiVersion: v1
items:
- apiVersion: v1
  data:
    reverseproxy.conf: |
      server {
          listen       80;
          server_name  localhost;

          location / {
              proxy_bind 127.0.0.1;
              proxy_pass http://127.0.0.1:3000;
          }

          error_page   500 502 503 504  /50x.html;
          location = /50x.html {
              root   /usr/share/nginx/html;
          }
      }
  kind: ConfigMap
  metadata:
    creationTimestamp: "2019-09-15T19:35:16Z"
    name: nginx-config
    namespace: default
    resourceVersion: "206154"
    selfLink: /api/v1/namespaces/default/configmaps/nginx-config
    uid: 7218c74c-ba1d-4757-93dc-71d0cff77356
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

On va setter l'appel à notre configmap dans notre pod / deployment :
celui ci aura deux container : nginx et l'autre contenant notre appli servant les requettes sur le port 3000

boogie$ cat configmap/nginx.yml                                                                 [☸ minikube:default]
apiVersion: v1
kind: Pod
metadata:
  name: helloworld-nginx
  labels:
    app: helloworld-nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.11
    ports:
    - containerPort: 80
    volumeMounts:
    - name: config-volume   <<<< ici on defini le nom du volume 
      mountPath: /etc/nginx/conf.d  <<< le path du volume contenant les data
  - name: k8s-demo
    image: wardviaene/k8s-demo
    ports:
    - containerPort: 3000
  volumes:
    - name: config-volume  <<<< on retrouve la définition du volume : même nom que la ressource pour le pod nginx
      configMap:
        name: nginx-config   <<<< c'est le nom du configmap dans kube 
        items:
        - key: reverseproxy.conf  <<<< ici on retrouve le nom de la clé du configmap dans kube qui contient notre conf 
          path: reverseproxy.conf


On cree un service de type nodeport pour l'exposer :

boogie$ cat configmap/nginx-service.yml                                                         [☸ minikube:default]
apiVersion: v1
kind: Service
metadata:
  name: helloworld-nginx-service
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    app: helloworld-nginx
  type: NodePort
boogie$ kubectl create -f configmap/nginx-service.yml                                           [☸ minikube:default]
service/helloworld-nginx-service created

On va tester et vérifier que notre conf de reverse fonctionne correctement : 


minikube service helloworld-nginx-service --url

http://192.168.99.100:31983


On voit bien qu'on a nginx qui nous repond : notre reverse proxy fait bien le travail.

boogie$ curl -vvv http://192.168.99.100:31983                                                   [☸ minikube:default]
*   Trying 192.168.99.100:31983...
* TCP_NODELAY set
* Connected to 192.168.99.100 (192.168.99.100) port 31983 (#0)
> GET / HTTP/1.1
> Host: 192.168.99.100:31983
> User-Agent: curl/7.65.1
> Accept: */*
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: nginx/1.11.13
< Date: Sun, 15 Sep 2019 19:48:26 GMT
< Content-Type: text/html; charset=utf-8
< Content-Length: 12
< Connection: keep-alive
< X-Powered-By: Express
< ETag: W/"c-7Qdih1MuhjZehB6Sv8UNjA"
<
* Connection #0 to host 192.168.99.100 left intact
Hello World!%

on peut verifier la conf de notre reverse en nous connectant dans le container nginx de notre pod :

boogie$ kubectl exec -it helloworld-nginx -c nginx -- bash                                      [☸ minikube:default]
root@helloworld-nginx:/# cat /etc/nginx/conf.d/reverseproxy.conf
server {
    listen       80;
    server_name  localhost;

    location / {
        proxy_bind 127.0.0.1;
        proxy_pass http://127.0.0.1:3000;
    }

    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}


ch2v52

= ingress : 

ingress est une solution pour les connexions externes a notre cluster , c'est une alternative aux external loadbalancer et nodeports.
Ingress permet d'exposer facilement les services devant etre accessible depuis l'exterrieur du cluster.

basiquement on va ce type de flux :

  www / 80/443
      |
      ingress controller 

      | ingress rules ->  host.exemple1.net -> pod1
                          host.exemple2.net -> pod2
                           host.exemple3.net/api/v2  -> pod3

   |             |           |
pod1             pod2        pod3



on peut utilsier different ingress-controller ( ex: ngin-ingress controller )
on a diffentes section dont des args qu'on peut passer a nginx
: on voit ici qu'on peut mettre en place un default backend  qui va recupérer les requettes qui ne matcheront aucune regles :


        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.17.1
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/echoheaders-default
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io

on peut definir le echo service qui va servir de fallback si nos rules ne matchent pas : 
apiVersion: v1
kind: Service
metadata:
  name: echoheaders-default
  labels:
    app: echoheaders
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30302
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: echoheaders



on va ensuite définir des rules por rediriger nos flux en fonctions des requettes entrantes :
ex: 
boogie$ cat ingress/ingress.yml                               [☸ minikube:default]
# An Ingress with 2 hosts and 3 endpoints
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: helloworld-rules
spec:
  rules:
  - host: helloworld-v1.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: helloworld-v1
          servicePort: 80
  - host: helloworld-v2.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: helloworld-v2
          servicePort: 80

on va creer nos objects : 

boogie$ kubectl create -f  ingress/nginx-ingress-controller.yml
boogie$ kubectl create -f  ingress/ingress.yml
kubectl create -f  ingress/echoservice.yml
boogie$ kubectl create -f  ingress/helloworld-v1.yml  
boogie$ kubectl create -f  ingress/helloworld-v2.yml          [☸ minikube:default]


si on test sans preciser de host : on va tomber sur notre default backend :


boogie$ curl http://192.168.99.100                            [☸ minikube:default]
default backend - 404%

si on test on forgeant un host header : 

boogie$ curl -H "Host: helloworld-v1.example.com" http://192.168.99.100
Hello World !%

boogie$ curl -H "Host: helloworld-v2.example.com" http://192.168.99.100
Hello World v2!%

                          
= external Dns :

https://github.com/kubernetes-incubator/external-dns


on va pouvoir utiliser un lb externe et rediriger le traffic vers un ingress controller pour les flux nécéssitant un fitrage ou redirect : ex typique pour les flux http / https

on va pouvoir utiliser un externaldns pour configurer les entrées dns des services que l'on va publier dans notre loadbalancer externe

pour chaque hostname que l'on utilise dans notre ingress on va pouvoir créer une entrée afin de diriger les flux vers notre lb puis vers notre ingress

la majorité des providers dns sont supportés.

on peut donc avoir une conf de type :


internet  

dns provider (route53 , pdns ..) 

lbexterne (metallb, elastic load balancer) 
 
service ingress   pod nginx ingress-controller

pod external dns 


on va créer des rule ingress qu'on va mettre a jour dans notre external dns 
notre external dns va envoyer les entrées dans le dns provider 
les requtees internet vont interroger le provider dns qu renvoit le reponses au client qui va initier la connection vers le loadbalancer externe qui contient les infos de services et va rediriger vers ingress qui a son tour va rediriger vers les bon pods.


on va pouvoir utiliser des deploiements pour gérer notre externaldns :

ex: 
boogie$ cat external-dns/external-dns.yaml                    [☸ minikube:default]
apiVersion: v1
kind: ServiceAccount
metadata:
  name: external-dns
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: external-dns
rules:
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get","watch","list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get","watch","list"]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: ["get","watch","list"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["list"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: external-dns-viewer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: external-dns
subjects:
- kind: ServiceAccount
  name: external-dns
  namespace: default
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: external-dns
spec:
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      serviceAccountName: external-dns
      containers:
      - name: external-dns
        image: registry.opensource.zalan.do/teapot/external-dns:latest
        args:
        - --source=service
        - --source=ingress
        - --domain-filter=kubernetes.newtech.academy # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones
        - --provider=aws
        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization
        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)
        - --registry=txt
        - --txt-owner-id=kubernetes.newtech.academy


= volumes : 

on va utiliser les volumes pour stocker des données en dehors des containers : puisqu'un container qui s'arrête perd ses données.
Les volumes persistents de kube vont permettre d'attacher un volume a un container même si celui-ci s'arrête dans ce cas le volume contenant les data pourra être rattaché à un nouveau container.
Les volumes peuvent être attachés via des volumes plugins (local, aws, gcp, ceph, nfs ...)
En utilisant des volumes on peut deployer des applications statefull : ces applis doivent pouvoir lire et ecrire sur le filesystem local qui doit être persistant dans le temps.
On peut donc faire tourner un mysql en utilisant un stockage persistent . Attention la gestion des volumes est encore récente dans kube.

1/ 
On va d'abord devoir créer un volume (on choisi en fonction du plugin manager que l'on veut utiliser) 
ex : 
## Create Volume in AWS

```
aws ec2 create-volume --size 10 --region your-region --availability-zone your-zone --volume-type gp2 --tag-specifications 'ResourceType=volume, Tags=[{Key= KubernetesCluster, Value=kubernetes.domain.tld}]'

2/ on va créer notre déploiment en précisant le point de montage du volume, on précise le volume , le plugin utilsié et l'id du volume (dans notre cas c'est de l'aws) :

boogie$ cat volumes/helloworld-with-volume.yml                                                  [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: helloworld
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
        volumeMounts:
        - mountPath: /myvol
          name: myvolume
      volumes:
      - name: myvolume
        awsElasticBlockStore:
          volumeID: # insert AWS EBS volumeID here


- volume provisionning 
on va pouvoir créer nos volumes en fonction de nos volumes plugins avant de les allouer aux pods concernés.

on va declarer un volume :
ex: 

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  zone: us-east-1


on va maintenant associer ce volume a la physical volume claim : requete / demande de volume physique nécéssaire à notre pod :

# pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
  namespace: test
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi


cat first-app/helloworld.yml                                                            [☸ minikube:default]
---
apiVersion: v1
kind: Pod
metadata:
  name: nodehelloworld.example.com
  labels:
    app: myapp
spec:
  containers:
  - name: k8s-demo
    image: wardviaene/k8s-demo
    ports:
    - name: nodejs-port
      containerPort: 3000
    volumeMount: 
    - mountPath: "/var/www/html"
      name: mypvc
  volumes:
  - name: mypvc
    persistentVolumeClaim:
      claimName: myclaim   <<<<< on reference ici le nom du volume claim qu'on a déclarer dans le PersistentVolumeClaim
  nodeSelector:
    environment: lab

= pod presets : 

on va pouvoir injecter de la config dans nos containers au runtime.
on va pouvoir factoriser des data qu'on va mettre dans un pod preset afin de deployer ces ressources sur tous les pods comportant un label particulier : plutôt que dupliquer des conf similaires dans plusieurs manifests de pod /deployment.

ex: 

on defini un pod-preset qui contient des données qui s'appliqueront aux pods comportant le label matchant dans le pod preset :

boogie$ cat pod-presets/pod-presets.yaml                      [☸ minikube:default]
apiVersion: settings.k8s.io/v1alpha1   # you might have to change this after PodPresets become stable
kind: PodPreset
metadata:
  name: share-credential
spec:
  selector:
    matchLabels:
      app: myapp    <<<<<< tous les pods comportants ce label hériteront des conf suivantes : variable d'env, volume ....
  env:
    - name: MY_SECRET
      value: "123456"
  volumeMounts:
    - mountPath: /share
      name: share-volume
  volumes:
    - name: share-volume
      emptyDir: {}

par exemple le deploiment suivant verra ses pods hériter des data du pod preset :
boogie$ cat pod-presets/deployments.yaml                      [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: deployment-2
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000


en examinant les pods issus de ces déployment on verra les points de montage ..qui sont injectés via les pods preset ..

= statefulsets : 

Ils vont être utiles pour des appli qui vont avoir besoin d'un nom de pod stable et unique (contrairement au deploiement qui genere des noms de pod en random) 
les pods vont avoir un index d'identité : ex pod-0, pod-1 ... quand ils vont être reschedule ils vont toujours avoir ce meme suffixe.

Ceci va être particulierement utile pour les applis qui utilisent des noms dns pour communiquer : ex elasticsearch, cassandra 

Les statefulsets permettent la gestion de stockage : le volume associé a un statefulset n'est pas delete quand le statefulset lui meme est détruit.

QUand on scale une appli statefulset : l'incrementation des index se fait automatiquement par kube :

on va passer de 2 à  3 replicats en passant donc de pod-0, pod-1 à pod-0, pod-1, pod-2

idem quand on va scale down :
on passera de pod-0, pod-1, pod-2 à pop-0, pod-1 

on va pouvoir prendre comme exemple un cluster cassandra qui a besoin d'un node "seed" : une source avec laquelle les autres membres du cluster doivent communiquer :


on va créer notre appli comportant 3 nodes dont le nom sera fixe , ces nodes reprendront le meme hostname  s'ils sont detruit , un volume est présenté aux pods qui vont l'utiliser 

boogie$ cat statefulset/cassandra.yaml                                 [☸ N/A:N/A]
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
  labels:
    app: cassandra
spec:
  serviceName: cassandra
  replicas: 3
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
      - name: cassandra
        image: gcr.io/google-samples/cassandra:v13
        imagePullPolicy: Always
        ports:
        - containerPort: 7000
          name: intra-node
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx
        - containerPort: 9042
          name: cql
        resources:
          limits:
            cpu: "500m"
            memory: 1Gi
          requests:
           cpu: "500m"
           memory: 1Gi
        securityContext:
          capabilities:    <<<<< ici on defini des droits particuliers nécéssaire à cassandra : les IPC_LOCK 
            add:
              - IPC_LOCK
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - nodetool drain
        env:
          - name: MAX_HEAP_SIZE
            value: 512M
          - name: HEAP_NEWSIZE
            value: 100M
          - name: CASSANDRA_SEEDS
            value: "cassandra-0.cassandra.default.svc.cluster.local"   <<< on aura ici le nom de notre seed qui servira au cluster 
          - name: CASSANDRA_CLUSTER_NAME
            value: "K8Demo"
          - name: CASSANDRA_DC
            value: "DC1-K8Demo"
          - name: CASSANDRA_RACK
            value: "Rack1-K8Demo"
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - /ready-probe.sh
          initialDelaySeconds: 15
          timeoutSeconds: 5
        # These volume mounts are persistent. They are like inline claims,
        # but not exactly because the names need to match exactly one of
        # the stateful pod volumes.
        volumeMounts:
        - name: cassandra-data
          mountPath: /cassandra_data
  # These are converted to volume claims by the controller
  # and mounted at the paths mentioned above.
  # do not use these in production until ssd GCEPersistentDisk or other ssd pd
  volumeClaimTemplates:
  - metadata:
      name: cassandra-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard
      resources:
        requests:
          storage: 8Gi
---
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  zone: eu-west-1a
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cassandra
  name: cassandra
spec:
  clusterIP: None
  ports:
  - port: 9042
  selector:
    app: cassandra


= daemonset : 
on va s'assurer que toutes les nodes d'un cluster hébergent un type particulier de
ressource.
Ex: on veut s'assurer que chaque node possede un pod particulier 

en cas d'utilisation typique on a :

-> les aggregateurs de logs 
-> les monitor
-> les lb, reverse proxy, api gateway 

ex: 
boogie$ cat ingress/nginx-ingress-controller.yml              [☸ minikube:default]
# updated this file with the latest ingress-controller from https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: nginx-ingress-controller
spec:
  selector:
    matchLabels:
      app: ingress-nginx
  template:
    metadata:
      labels:
        app: ingress-nginx
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.17.1
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/echoheaders-default
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            capabilities:
                drop:
                - ALL
                add:
                - NET_BIND_SERVICE
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
          - name: http
            containerPort: 80
            hostPort: 80
          - name: https
            containerPort: 443
            hostPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1


= ressource monitoring =

heapster permet de monitorer un cluster et d'analyser les perfs.
il exporte ses metrics via des endpoints REST
on peut l'utiliser avec des backends differents ( influxdb, kafka ...)

>> Attention heapster est deprecated <<< 

on doit donc maintenant utiliser metrics-server pour exporter les datas.


on va pouvoir une fois installé recolter les metrics de nos éléments :

ex :
kubectl top node / pods ....

= autoscaling : 

kube peut automatiquement grace aux data des metrics autoscaller des ressources ( deployment, replicaset ..)


HorizontalPodAutoscaler est un object kube 

Le cluster doit être démarré avec la variable d'environnemnent : 
ENABLE_CUSTOM_METRICS définie à  true ( à verifier : doc ancienne )

l'autoscaling va examiner toutes les 30secondes (de base mais la frequence est paramétrable : en changeant la valeur du param de horizontal-pod-autoscaler-sync-periodmais cela se fait au bootstrap du cluster ) les metrics afin de voir la situation

ex : on va definir un pod ayant besoin de resource ex
200m = 200milicpu = 0.2 = 20% d'un cpu de core sur notre node.
Si le node a deux core c'est toujours 20% d'un core .
on introduit l'autoscalling pour une valeur de 50% d'occupation cpu
-> 100m / 10%cpu core 

ex: 

boogie$ cat autoscaling/hpa-example.yml                                [☸ N/A:N/A]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hpa-example
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: hpa-example
    spec:
      containers:
      - name: hpa-example
        image: gcr.io/google_containers/hpa-example
        ports:
        - name: http-port
          containerPort: 80
        resources:
          requests:    <<< ici on va definir les ressources nécéssaires à notre pod on definit qu'il doit disposer de 200m de cpu ( il ne pourra pas avoir plus )
            cpu: 200m
---
apiVersion: v1
kind: Service
metadata:
  name: hpa-example
spec:
  ports:
  - port: 31001
    nodePort: 31001
    targetPort: http-port
    protocol: TCP
  selector:
    app: hpa-example
  type: NodePort
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-example-autoscaler
spec:
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: hpa-example
  minReplicas: 1               <<<< ici on va avoir les spec du nombre mini de replicat pour notre deployment
  maxReplicas: 10             <<<< ici le nombre max de replica qu'on aura pour assurer que les 50% de cpu sont bien respectés.
  targetCPUUtilizationPercentage: 50


si on execute ce manifest on va avoir plusieurs objects crées :

hpa (horizontal pod austoscaler) / pod / service ..

on va pouvoir recupérer les infos du service , lancer un container pour loader notre service ...on va pouvoir faire des get en continu sur le service depuis un pod , cela va générer de la charge ( on peut la voir evoluer via un kubectl get hpa ..)

ex: kubectl run -i -tty load-generator --image=busybox /bin/bash

# wget http://hpa-example.svc.cluster.local:31001/
# while  true; do wget http://hpa-example.svc.cluster.local:31001/; done 

des que  la charge global atteind les 50% de cpu : kube crée des nouveaux pods 
( max 10 comme on voit dans le manifest) 

des que la charge repasse sous les 50% kube detruit les pods en trop, jusqu'a un miimum defini dans notre manifest.


= affinity /anti affinity : 


affinity /anti affinity vont nous permettre de mettre en place des regles de déployments plus complexes qu'en utilisant juste un node selector. On va aussi pouvoir appliquer ces regles sur les pods.
On va pouvoir etablir des regles de préferences qui vont être plus souples : ex on peut definir un profile préféré ce qui implique que le scheduleur pourra quand même déploye les pods si les regles ne sont pas trouvées.
On va pouvoir mettre en place des regles qui vont se baser sur les labels situés sur d'autre pods.
ex: on peut s'assurer que deux pods ne sont pas présents sur le même node.

kube peut faire : node affinity et pod affinity / anti-affinity

-> node affinity : 
c'est similaire à du nodeselector

-> pod affinity/ antiaffinity : 
on va pouvoir etablir des regles de schedule tenant compte des labels présents su des pods runnings

Ces regles ne sont applicables qu'au moment du scheduling.
Si les pods sont deja actifs ils faut les recréer pour que les regles soient prises en compte.

A / node affinity :

2 types de regles :

1/ RequireduringSchedulingIgnoreddDuringExecution

Regle hard / comme le node selector : cette regle doit être matchée avant le scheduling

2/ preferredDuringSchedulingIgnoreddDuringExecution

Regle soft : si on a la capacité d'appliquer la regle car on trouve les ressource c'est parfait sinon on schedule quand même.


Les regles se présentes de la maniere suivante : 


boogie$ cat affinity/node-affinity.yaml                                                         [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: node-affinity
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: node-affinity
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:       <<<<<< on voit ici que le pod va etre schedule sur un node comportant le label env: dev 
            nodeSelectorTerms:
            - matchExpressions:
              - key: env
                operator: In
                values:
                - dev
          preferredDuringSchedulingIgnoredDuringExecution:       
          - weight: 1                                           <<<<< ici on etabli un poids pour notre régle
            preference:
              matchExpressions:
              - key: team
                operator: In
                values:
                - engineering-project1
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000


Quand des poids sont donnée dans les affinity / anti affinity : kube va schedule les pods sur les nodes ayant les combinaisons les plus élevées

si un deployment donne un poids de 1 a certaines regles et qu'un second déploiment donne 2 regle de 4 et 2 alors les pods de se deployment seront schedule sur le node ayant le plus de poids.

En plus des labels que nous créons, kube va lui même avoir des "pre-populated" labels définis que l'on va pouvoir utiliser.
ex : kubernetes.io/hostname kubernetes.io/os=linux

on pourra par exemple forcer le schedule sur un node , une zone géographique .
ex: 
si on lance notre déployment les pods ne seront pas schedulent tant qu'aucun node ne portera la label obligatoire env= dev 

on va pouvoir tester en créant des labels sur un cluster 

kubectl label node node1 env=dev
kubectl label node node2 env=dev
kubectl label node node3 env=dev

maintenant qu'un node ayant le label mandatory est présent : les pods du deploiement sont scheduled dessus 

On voit que le label soft n'est donc pas pri en compte.

Si maintenant un met un label de type team=engineering-project1 sur un second node 

kubectl label node node2 team=engineering-project1

si on delete un pod présent sur le node 2 , et le node 3 , et qu'on le recrée alors il sera automatiquement crée sur le node 2 qui possede le hard label et le soft label en plus.

Les pods seront donc en priorité schedule sur le node2 qui posséde maintenant les meilleurs prérequis grace aux labels définis.

B/ interpod affinity / antiaffinity : 

ces mecanismes vont pouvoir influer sur le déployments des pods running dans le cluster.
Les pods appartiennent à des namespaces : nos règles vont s'appliquer dans ces namespaces. Si rien n'est préciser alors on sera dans le namespace default.

Comme pour les nodes ont a les rules : 


1/ RequireduringSchedulingIgnoreddDuringExecution

Regle hard : cette regle doit être matchée avant le scheduling

2/ preferredDuringSchedulingIgnoreddDuringExecution

Regle soft : si on a la capacité d'appliquer la regle car on trouve les ressource c'est parfait sinon on schedule quand même.

Un des bons use case pour le pod affinity est le co-located pods : on va vouloir que deux pods soient sur le même node.
ex: un serveur cache de type redis devra être hébergé sur le node qui host le pod de l'appli utilisant ce cache.
Idem pour la gestion geographique : certains pods doivent être dans une même zone.

Quand on defini des pod affinity /antiaffinity on doit definir des topology domain : topologyKey
qui fait reference à un node label.

Si une rule d'affinity match alors le nouveau pod sera schedule sur le node ayant la meme topologyKey que le pod running

ex : si on a notre redis a deployer et qu'on a trois node dans notre cluster : 

nos regles vont indiquer que le deployment devra se faire sur un ou des nodes particulier répondant aux normes :
> ce / ces nodes doivent héberger le pod ayant le label app=pod-affinity-1 et un node ayant un label kub interne de type topologyKey: "kubernetes.io/hostname" 

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pod-affinity-2
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: pod-affinity-2
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                    - pod-affinity-1
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis
        image: redis
        ports:
        - name: redis-port
          containerPort: 6379


- anti-affinity : 

on va pouvoir utiliser ces antiaffinity pour nous assurer par exemple que notre pod n'est déployé qu'une seule fois sur un node.
ex : 3 nodes et 2 pods
on va pouvoir définir que notre nouveau pod ne sera pas déployer sur un node comportant un pod matchant avaec un certain label.

ex :  on va vouloir déployer un pod sur un node qui ne contient pas app=pod-affinity-1 ou app=pod-affinity-3 

    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                    - pod-affinity-1
                    - pod-affinity-3
              topologyKey: "kubernetes.io/hostname"

biensur si on a un cluster de 2 nodes alors on aura un pb, il sera possible de mettre une regle soft pour assurer quand même le déployment sur un node même si celui ci contient un pod ayant un label app: pod-affinity-1 ou app: pod-affinity-3
cat affinity/pod-anti-affinity-5.yaml                                                   [☸ minikube:default]
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pod-affinity-5
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: pod-affinity-5
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                    - pod-affinity-1
                    - pod-affinity-3
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: k8s-demo
        image: wardviaene/k8s-demo
        ports:
        - name: nodejs-port
          containerPort: 3000


- Ecriture des rules :

on peut utiliser :
In / NotIn
Exists / DoesNotExist

/!\ Attention les affinity / antiaffinity demande beaucoup de ressources. Il faut bien sizer le cluster en conséquence.


= taints - tolerations

Contrairement aux affinity / antiaffinity les tolerations vont empecher le schedule sur un node
les taints vont creer des regles qui seront affectées sur le node. Les tolerations vont empecher le schedule de pod sur un node.

- taints :

un des use case standart est de s'assurer que quand on crée des pods ils ne soient pas schedule sur le master.
la master a un taint : node-role.kubernetes.io/master:NoSchedule
ex : en examinant un master on va avoir classiquement ce taint de flaggué : 
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master



kubectl taint node1 key=value:NoSchedule

Dans ce cas aucun pod ne pourra etre schedule sur le node1 tant qu'une toleration ne soit settée.

on pourra donc bypasser le no schedule en définissant un toleration dans notre deployment /pod .. 

  replicas: 3
  template:
    metadata:
      labels:
        app: tolerations-2
    spec:
      tolerations:
      - key: "type"
        operator: "Equal"
        value: "specialnode"
        effect: "NoSchedule"

on peut avoir des operateurs :
Equal :  key + value
Exists : s'assure que la clé doit exister


Comme pour les affinity on peut avoir des taints qu sont hard ou soft 

Noschedule est hard : et aucun pod ne pourra etre schédule sauf si une toleration est posée
PreferedNoschedule: kube va essayer de ne pas scheduel de pod sur le node mais ce n'est n'est pas exclusif.

Si un taint est posé et que des pods sont déja running : ils ne seront pas ejectés sauf si la clé :
NoExecute: evict est posé : les pods qui n'auront pas de tolération posé seront ejecter du node.

Quand on utilise un taint NoExecute , il est possible de definir dans notre tolération la durée pendant laquelle le pod pourra tourner avant d'être ejecté.

    spec:
      tolerations:
      - key: "type"
        operator: "Equal"
        value: "specialnode"
        effect: "NoSchedule"
        tolerationSeconds: 3600

Si le flag tolerationSeconds n'est pas sette alors le pod n'a pas de limite de temps.       

on va avoir des cas ou biensur on ne veut pas schedule :
le node pouvant avoir différents etats :
not ready, memory pressure, network pb, espace disque full etc ....


# Taint a node
```
kubectl taint nodes NODE-NAME type=specialnode:NoSchedule
```

# Taint a node with NoExecute
```
kubectl taint nodes NODE-NAME testkey=testvalue:NoExecute



pour supprimer un taint d'un node : on place un "-" apres le nom de notre type  

kubectl taint nodes NODE-NAME type-
kubectl taint nodes NODE-NAME testkey-


= CRD - custom resource definition : 

n va pouvoir ajouter des ressources , objects personalisés que l'on va injecter dans notre cluster. Ce sont des adddons que l'on crée. une extension de l'api kube.

Les customs ressources sont decrites en yaml comme les objects kube.


= operators : 

c'est une methode de packaging, deployment et management d'une application.
Un fois deployé l'operator pourra être managé par un crd : type permettant l'extension de l'api kube.
C'est un bon moyen de déployer des applications comlplexes ( ex statefull ) en  masquant la complexité aux users.

Toutes les application externes peuvent créer un opérator : prometheurs, rook, postgresql, mysql ...

ex: si on deploy un container postgreql on peut juste démarrer le service 
en utilisant un operator on va pouvoir créer une replication ,gérer les backups etc ...

on va donc utiliser des outils qui vont nous permetter de manager la db via les operators.

ex :  utilisation operator : 
https://github.com/CrunchyData/crunchy-containers



== Administration ==

= quotat / ressources :

il va être important de  gérer les ressources de notre cluster , et de bien gérer celles qui sont liées à différentes ressources, équipes.

on va classiquement separer notre cluster en différents namespace au sein desquels nous allons definir des quotas.

Nous avons des ressources kube dédiées pour cela : Ressourcequota et Objectquota

- Ressourcequota :
chaque container va specifier ses requests capacity et capacity limits 

-> request capacity : va être en quelque sorte le minimum de ressource dont le pod a besoin pour tourner 
le scheduler va se servir de ses indications pour dipatcher le pod a créer sur le node disposant des ressources nécéssaires.

-> ressource limit : le pod ne pourra pas obtenir plus de ressource que ce qui est défini ici . 


Attention : si l'admin kube a defini des quotas sur le cluster alors chaque definition de pod, deployment devra comporter des ressource dans le manifest.
Si on en defini pas alors on a une erreur.

ceci est valable pour les ressources et limits quotat.

Si une demande de ressource depassant celle autorisée est faite alors kube renvoi une 403 et kubectl renvoi lui une erreur.

l'admin kube peut definir un set de quota pour les namespaces :
requests.cpu  : somme des toutes les valeurs dispos pour tous les pods en cpu 
requests.mem  : idem en mem 
requets.storage : idem en storage
limits.cpu : limit sur notre cluster en cpu allouable
limits.mem : idem en memory 

on va aussi pour voir definir des limits sur les objets kube suivant :
configmap
secret
persistantvolumeclaim
pod : ex 10 pod max par nodes
replicationcontroller
ressourcequota
services
services.loadbalancer
services.nodeports

= namespaces : 

le namespace va nous permettre de créer un cluster virtual dans notre cluster kube.
on va s'en servir pour separer les ressources de maniere logique dans notre cluster

si on ne specifie rien on est de base dans le namespace default
plusieurs namespaces existent de base : default, kube-system.
Le nom des ressources doivent être unique au sein de notre namespace mais pas forcement au sein de tous les namespaces.

on va pouvoir definir des limites par namespaces :
ex : marketing aura le droit max a : 
10Gb de memoire, 2 lb, 2 core cpu

- creation de namepace :
kubectl create namespace my_ns

- liste des ns ( namespaces )
kubectl get ns

- definition de notre namespace par default :
1/ on recupere le ns actuel

export CONTEXT=$(kubectl config view |awk '/current-context/ {print $2 }')

2/ on defini comme context notre context :

kubectl config set-context $CONTEXT --namespace=my_ns


On va pouvoir definir nos limites par namespaces :

boogie$ cat resourcequotas/resourcequota.yml                           [☸ N/A:N/A]
apiVersion: v1
kind: Namespace
metadata:
  name: myspace            <<<<<< on defini les namespaces.
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: myspace
spec:
  hard:                    <<<<< on defini des limites hard sur des ressources system
    requests.cpu: "1"      <<<<< 1 core cpu 
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota
  namespace: myspace
spec:
  hard:                     <<<<< ici on defini des limites hard sur des objects kube 
    configmaps: "10"
    persistentvolumeclaims: "4"
    replicationcontrollers: "20"
    secrets: "10"
    services: "10"
    services.loadbalancers: "2"


= users :=


on a deux types de users :

-> normal : qui accede au cluster de l'exterrieur via l'api kube.
ex kubectl 

ce user n'est pas managé via des objects

-> user system : 
managés en temps qu'object kube.
ce type d'utilisateur est utilisé pour s'authentifier au sein du cluster kube.
ex : pods, services 

ces users sont managés comme les secrets.

- Users normaux :

pour accéder au cluster ces users vont utiliser : 

-> clients certificates
-> tokens bearers
-> proxy auth
-> http basic auth
-> openid
-> webhooks

Les users normaux ont de plus des specificité :

username ( user123 , user@bob.com ..)
uid 
groups
extra fields 

- services users :
utilisent des tokens de services account
Ils sont stockés en credentials en temps que secret 
ces secrets sont montés dans les pods pour autoriser les communications entre les services.
Ces services users sont spécifiquess aux namespaces.

Ils sont crées via l'api ou manuellement en utilisant des objets.
Les calls d'api non authentifiés sont considérés effectués en tant qu'anonymous.



Quand un user nomal est authentifié, il peut acceder à tout
pour limiter les acces il faut definir des authorizations.

Alwaysallow / Alwaysdeny
ABAC ( attribute base access control)
RBAC ( role base access control)
webhook ( authorization par service remote)

Les RBAC sont les methodes à privilégier.

ex: on va créer un user qui pourra se connecter à l'api kube en utilisant un cert qui aura ete signé par la ca du cluster : 

minikube ssh 

-> on cree notre clé priv : 
$ openssl genrsa -out boogie.pem 2048
Generating RSA private key, 2048 bit long modulus
......................................+++
...+++
e is 65537 (0x10001)
$ ls
boogie.pem

-> on crée notre demande de certificat 
$ openssl req -new -key boogie.pem -out boogie-csr.pem -subj "/CN=boogie/O=boogie-team/"
$ ls
boogie-csr.pem	boogie.pem

-> on signe notre certificat avec la clé de le ca de notre kube :
$ sudo openssl x509 -req -in boogie-csr.pem -CA /var/lib/minikube/certs/ca.crt -CAkey /var/lib/minikube/certs/ca.key -CAcreateserial -out boogie.crt -days 1000000
Signature ok
subject=/CN=boogie/O=boogie-team
Getting CA Private Key
$ ls
boogie-csr.pem	boogie.crt  boogie.pem

on a notre certificat :
$ cat boogie.crt 
-----BEGIN CERTIFICATE-----
MIICujCCAaICCQD3S4/laQkyFDANBgkqhkiG9w0BAQUFADAVMRMwEQYDVQQDEwpt
aW5pa3ViZUNBMCAXDTE5MDkyNzE3MTg0MloYDzQ3NTcwODI0MTcxODQyWjAnMQ8w
DQYDVQQDDAZib29naWUxFDASBgNVBAoMC2Jvb2dpZS10ZWFtMIIBIjANBgkqhkiG
9w0BAQEFAAOCAQ8AMIIBCgKCAQEAnhdNjL6Qj/ITKe7viaEBRoUqWKxSki31I/ax
VgnvBklYvGkxsyD6+WvDObtzJ003somx7GfG1KNyqSMKFiOI6ibmja8aMHJCQqVT
oe1OxzOPgOFH5btVC2cgWO9bOrzWwWYNHMUaSJUbDwo57rwDov75/CkWZGe+zD/H
afEHxDuXJRyT2NF7mNzi+fXMUYgK7G0VawZGjTEnuN9fOpky0sqfrReDPoAHw+YU
K/VKFijWFLNr488CmcFeSgjwFeCT5/4v/nC1KMGoGUT1FUuZdNOHT4SHgmStV7iH
w5xT+AONzroYRPZBcGXuDJ7+7CtNW72rMyS6znR2nhN43yORZQIDAQABMA0GCSqG
SIb3DQEBBQUAA4IBAQAcs2ubafGrkBu1wzuxfsGC24cFhYS20GtLItkdHFhm4uWQ
N00fkvWuFNx2ZrXSvquQJJp1iBjQL88DfX5Fuo9H7NfOMmyRlyjsozkrxwQh5rjd
89hpzIae+L6t+oYHf9MMy1J4GccDPz8u65E1zq3DJHxEXeybuZAbZVno9vZvcbG6
mvdkS0WecQgTYM7SPTVv8B0HGf2GoPrLF9wXiY5ElKKp//wZKO0e3RkV+Pa4URoS
szCYXzb+Ou5CG2xHlhEPycKBLiSIJDiD6XX0BwoBAculrTUdq4WFKA6LeFA0L9Vv
IP1qftdJZ/oSovhbo3W921oreV+ykxYP3/tE4dxT
-----END CERTIFICATE-----

notre clé :
$ cat boogie.pem 
-----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEAnhdNjL6Qj/ITKe7viaEBRoUqWKxSki31I/axVgnvBklYvGkx
syD6+WvDObtzJ003somx7GfG1KNyqSMKFiOI6ibmja8aMHJCQqVToe1OxzOPgOFH
5btVC2cgWO9bOrzWwWYNHMUaSJUbDwo57rwDov75/CkWZGe+zD/HafEHxDuXJRyT
2NF7mNzi+fXMUYgK7G0VawZGjTEnuN9fOpky0sqfrReDPoAHw+YUK/VKFijWFLNr
488CmcFeSgjwFeCT5/4v/nC1KMGoGUT1FUuZdNOHT4SHgmStV7iHw5xT+AONzroY
RPZBcGXuDJ7+7CtNW72rMyS6znR2nhN43yORZQIDAQABAoIBAGV1dyO4uXZoWbwz
yC9/0R29IOw/y85dCFLIZA9f2LMkJ3rj2C2qIOgqLOTEVKcIe5JtpG6gO48ERvYd
sr96lsgPpy+PDTPCYJv9Fqt5bXGSuapw6n+Ztn7W9H+fPeF4iCsen5OzO6cRaohA
e+WnbH4TPcSDageOPXlABW8MbXuV8Hm2G8Ux0LlOTl4C/LiAH53LEtcqvK0DMzKJ
+hIpQdhJ+38t2oUEAPBZd1ndn471SYuP1CuuzonMESwjjk8A3mO2l/0Xryo1eUZH
hshi8J/Um5s4lGpLXqPCmpdn43fcTEHZqHX/AHKCOR4i1rVX97A0okjiKwyvYXq7
RQcIHIECgYEA0fJFVzQcClYMcP2HOVUbsEW0Bphnb8SzLkIVSxmxHLRHoxrwMasc
JQ3Lly99igOSZYrhRSXz9Kqpx/hXihidpkDkBauepk2A8Vt68vMnisvS2+TUUyAN
groGgzvHjZtcnwPGIId6rD08HdeNX37/OvCzJVZStbZeqeAq8aMT/akCgYEAwMUM
U63ab9qxCmpe+doKlERZFNjRsviywXUNu6EeHEswHRDkPA+EFWwZzvCCBATsnzqp
wz3l/ybzZATHarJm50t7SLWeijs4tQOoYTrj8pi1IHlkas4AJcbCgpU02gkAA8Qf
2Y2Sn68RsENPxf1btgw19zWI+1aXYZsWtuPx810CgYAc0/6PfdlH1g75SObTiYs2
xit1KGIzp7fO50YTPODkByE7FUzcFRnVQLKu/NdpJxpoFQzfuNlpxDFyAvjKPp0P
IiPtX2Z2gXfuLFiSma7ZUSmQBjjdfS49wBt606+QE0BBmXwQJWpD0li55EpGTaLw
TfLbcqICz5rqCpz69yCcmQKBgFiKqYzN7+Usl+Dm1tUAN5e+hQQ0Sq1f2gtNzo7+
BCyOxHy1Q3/qyIpmarxIcA+Ui3LfqRYKmlfF9klcgJQbeXSHkQrmkPmBFhzenzp5
qDXHuno+B8ee8yZm0vTTv8DeHWmdbdupUU2TRhal9EBGjRdjKG5x4oc/UKz+ZUKA
DDDDDDAoGBAMubU7nxPuc9ugzn1BAzes6osQRetTzkQZcQBCc44IlHMSALxaeqH+9r
3J8oFUx4bmCT3Mty7ya5ibOgSKpZt1ArEo0oyha/vWrLt5BSWaD/fne08C+ZwZNY
0ZnTPmBRUfcaMCXcDoCpueICyEKYyIIhvNfHmVdhENjHEQ44E1gY
-----END RSA PRIVATE KEY-----

on va pouvoir renseigner ce datas dans notre kube config apres avoir mis les fichiers dans le path de notre minikube et on pourra tenter de se connecter au kube avec ce couple cert /clé.

boogie@boogie-stuff:~$ ls .minikube/
addons         boogie.crt  ca.key    client.crt  key.pem   proxy-client-ca.crt
apiserver.crt  boogie.key  ca.pem    client.key  logs      proxy-client-ca.key
apiserver.key  cache       cert.pem  config      machines  proxy-client.crt
bin            ca.crt      certs     files       profiles  proxy-client.key

apiVersion: v1
clusters:
- cluster:
    certificate-authority: /home/boogie/.minikube/ca.crt
    server: https://192.168.39.8:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /home/boogie/.minikube/boogie.crt
    client-key: /home/boogie/.minikube/boogie.key


Attention cette phase n'a pas fonctionné qu'une fois notre user créer et les rbac setté correctement voir tout de suite apres.

= rbac : 

apres l'authentification, on va devoir décider ce que notre user a le droit de faire.
les authorisations se font au niveau de l'api.
ex: quand on fait un kubectl get node : l'api va vérifier si le user qui execute cette commande a le droit de le faire.

rbac : role base access controle va permettre de 
reguler les acces via des roles
administrer dynamiquement les ressources.

webhook : on va envoyer des requette d'authorisation un une api rest
si on veut gérer un server d'authorization : on peut parser le payload en json et repondre si l'acces est granté ou pas.

Les vielles versions de kube nécéssitait de demarrer le cluster en precisant le mode d'authorisation rbac

--authorisation-mode=RBAC 

pour minikube invoquer en cli au boot :
minikube start --extra-config=apiserver.Authorization.Mode=RBAC 

on peut biensur definir nos rbac dans un fichier :
1/ on defini un role
2/ on associe un user / group a ce role 
on peut créer des roles limités a un namespace : Role
ou alors valable pour tous les namespaces : ClusterRole

on associera donc ensuite ces role / user avec des RoleBinding ( limité à un namespace ) ou alors ClusterRole Binding : pas de limite de namespace 


ex : role limité a un namespace :

on crée un role qui permet de lire, examiner les pods et secrets : 

boogie@boogie-stuff:~/Documents/learn/kube/learn-devops-the-complete-kubernetes-course/kubernetes-course$ cat users/user.yaml 
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods", "secrets"]
  verbs: ["get", "watch", "list"]

Maintenant on va faire matcher ce role à un user créer au prealable : 
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: boogie     <<< notre user        
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader  <<<<<          on accroche le role défini auparavant 
  apiGroup: rbac.authorization.k8s.io


maintenant cluster wide : on ne precise pas de namespace car c'est pour notre cluster entier 


boogie@boogie-stuff:~/Documents/learn/kube/learn-devops-the-complete-kubernetes-course/kubernetes-course$ cat users/user.yaml 
kind: ClusterRole           <<<<< on defini ici l'objet ClusterRole 
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pod-reader-clusterwide  <<< on ne precise pasde namespace : puisqu'ils sont tous accessibles mais on donne un nom explicite.
rules:
- apiGroups: [""]
  resources: ["pods", "secrets"]
  verbs: ["get", "watch", "list"]

Maintenant on va faire matcher ce role à un user créee au prealable : 
---
kind: ClusterRoleBinding   <<<< on défini un clusterolbinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: read-pods
subjects:
- kind: User
  name: bob     <<< notre user        
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader-clusterwide  <<<<<          on accroche le role défini auparavant 
  apiGroup: rbac.authorization.k8s.io

boogie@boogie-stuff:~/Documents/learn/kube/learn-devops-the-complete-kubernetes-course/kubernetes-course$ cat users/admin-user.yaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: User
  name: "boogie"
  apiGroup: rbac.authorization.k8s.io


ex: on va créer un user puis on pourr tester ses acces en fonctions des rbac définis pour ce user :

- creation de users :

on va pouvoir créer un user de plusieurs manieres .
Il faudra qu'il s'authentifie au cluster kube

## Create new user

on va créer un nouveau user , donc générer a l'aide d'openssl une nouvelle clé :
sudo apt install openssl
openssl genrsa -out bob.pem 2048

on va générer une demande de signature de cert (cert sign request) csr  :

openssl req -new -key bob.pem -out bob-csr.pem -subj "/CN=bob/O=myteam/"

on signe ensuite la demande csr avec la clé et le cert de notre CA :
openssl x509 -req -in bob-csr.pem -CA ca.crt -CAkey ca.key -CAcreateserial -out bob.crt -days 10000
```

On va maintenant pouvoir ajouter notre user à notre conf kube :
## add new context
```
kubectl config set-credentials bob --client-certificate=bob.crt --client-key=bob.pem
kubectl config set-context bob --cluster=kubernetes-prod --user bob

on verra en fonction des droits que le user ne pourra pas lister les nodes, les pods en dehors de son namespace via les rolebinding , ou pourra au contraire lister les pods de tous les namespaces avec un clusterrolebinding.

== network : ==

dans kube les pods sont routables : pod to pod communication : les pods doivent tous pouvoir communiquer indépendamment des nodes sur lesquels ils se trouvent.
Chaque pod a son ip 

Pour la gestion de kube dans le cloud les providers (aws, gcp, azure ) fournissent la couche reseau 
ex: aws : kubenet networking ( dans kops ) les pods ont une ip utilisant un VPC (pour amazon : aws private network ) 
kube fourni un /24 pour tous les nodes.
Mais attention on est limité a 50 nodes : aprss souci de perf 

On a sinon la possibilité d'utiliser un CNI : container network interface ex : calico / weave  
ou un network overlay : flannel 

Flannel par exemple va servir de gateway entre les nodes : 
une ip 10.3.x.x sera donné par flannel aux pods sur un node . pour communiquer sur un pod d'au autre node : l'ip du pod initial et destination seront encapsulée et transmise a l'iface du premier node ( celui qui a le pod client) . Le node envoit la paquet au node hébergeant le pod destination : le paquet recu est désencapsuler et le flannel du node destination l'envoi au pod de destination.

= node maintenance : =

le node controller va etre responsable de la gestion des objects du node
il va aussi monitorer la santé du node.
Quand on ajoute un nouveau node kubelet va essayer de l'enregistrer lui même  :self-registration

cela permet de gérer sans intervention manuelle dans l'api . Un objet node sera créer avec les metadata (ip, hstname) des labels ( region , taille d'instance ..) 
Un node a aussi un status de condition : running, outofdisk  ...

Quand on veut sortir un node on va donc s'assurer qu'on reparti les pods sur d'autre noeud pour cela on drain le node

kubectl drain node_name 
on peut ajouter un delai 
kubectl drain node_name --grace-period=600s

si notre node héberge des pod non gérés par le controller on peut forcés :
kubectl drain node_name --force

on peut voir que le node a un status de schedulingdisable quand les pods sont ejectés et qu"il passe en maintenance.

=  kube high availability : =

quand on veut assurer une ha o, doit assurer au minimum :
3 noeuds etcd 
l'api kube repliquée avec un lb 
avoir plusieurs instance de controller et scheduler . Un seul sera master les autres en stand-by



== chapitre 5 : software install / package - deploy : ==

= helm : =

helm est la meilleure solution pour rechercher et installer des applis pour kube.
c'est un paquet manager pour kube : il va nous aider a paquager des applis . Ce projet est maintenu par la cncf ( container native cloud foundation) 

- helm install :
on va recupérer le binaire helm ( voir helm.sh) : puis lancer un 
helm init 
> cela va installer tiller sur le cluster kube qui est le server de helm
le binaire helm est le client de tiller

On doit pour installer tiller setter des regles de RBAC et un service account 

ex: 

oogie$ cat helm-rbac.yaml                                                                                                                           [☸ kube-infra:fso]
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system


helm utilise un format de packge appellé charts.
un chart est une collection de ressource qui decrivent des ressources kube.
un simple fichier peut installer une app, une db ....
on peut avoir des dépendance un chart wordpress peut dependre d'un chart mysql ...
on peut biensur ecrire nos propres charts pour ecrire notre application.

Les charts utilisent des templates développés par des packages maintainers
ils vont générés des fichiers yaml que kube  comprend.

ces templates vont contenir de la logique et des variables : 
on va biensur pouvoir overrider les valeurs de ces variables pour quelles matchent nos besoin 
ex :
boogie$ cat service.yaml                                                                                                                             [☸ kube-infra:fso]
apiVersion: v1
kind: Service
metadata:
  name: {{ template "demo-chart.fullname" . }}
  labels:
    app: {{ template "demo-chart.name" . }}
    chart: {{ template "demo-chart.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}                <<<<< exemple ici on va overrider les valeurs de notre port 
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: {{ template "demo-chart.name" . }}
    release: {{ .Release.Name }}



- commandes :

helm init 
helm reset ( desintalle tiller )
helm install une app
helm search pour chercher une app
helm list pour lister les charts installés
helm upgrade ( pour upgrader une release )
helm rollback  pour rollback notre release

ch5 v96


helm install -n redis stable/redis

= helm charts creation :

la creation de charts helm est la methode recommandée pour le déployment d'applis dans kube.
on peut packager puis déployer l'app en une commande. on peut aussi avec helm gérer les upgrade et rollback.
le chart est versionné.
on va utiliser la commande create qui crée un skel pour notre app :

boogie$ helm create mychart                                     [☸ kube-infra:fso]
Creating mychart
boogie$ tree                                                    [☸ kube-infra:fso]
.
├── mychart
│   ├── charts
│   ├── Chart.yaml   <<< contient les metadata ( version,nom du chart ..)
│   ├── templates
│   │   ├── deployment.yaml   <<<<< 
│   │   ├── _helpers.tpl
│   │   ├── ingress.yaml      <<<<<< 
│   │   ├── NOTES.txt
│   │   ├── service.yaml      <<<<<       ici on a des template d'object kube 
│   │   └── tests
│   │       └── test-connection.yaml
│   └── values.yaml    <<<< ici on definie les valeur des variables de nos templates key:value en yaml


boogie$ cat Chart.yaml                                          [☸ kube-infra:fso]
apiVersion: v1
appVersion: "1.0"
description: A Helm chart for Kubernetes
name: mychart
version: 0.1.0


cat values.yaml                                         [☸ kube-infra:fso]
# Default values for mychart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: nginx
  tag: stable
  pullPolicy: IfNotPresent

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

service:
  type: ClusterIP
  port: 80

boogie$ cat templates/service.yaml                              [☸ kube-infra:fso]
apiVersion: v1
kind: Service
metadata:
  name: {{ include "mychart.fullname" . }}
  labels:
{{ include "mychart.labels" . | indent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: {{ include "mychart.name" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}


Lors de la creation de notre chart on a aussi un fichier NOTES.txt qui va permettre d'alimenter les valeursqu'on a definies dans notre fichier values pour permettre aux users de se connecter à notre app
ex :

boogie$ cat templates/NOTES.txt                                 [☸ kube-infra:fso]
1. Get the application URL by running these commands:
{{- if .Values.ingress.enabled }}
{{- range $host := .Values.ingress.hosts }}
  {{- range .paths }}
  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}{{ . }}
  {{- end }}
{{- end }}
{{- else if contains "NodePort" .Values.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "mychart.fullname" . }})
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" .Values.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include "mychart.fullname" . }}'
  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include "mychart.fullname" . }} -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
  echo http://$SERVICE_IP:{{ .Values.service.port }}
{{- else if contains "ClusterIP" .Values.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "mychart.name" . }},app.kubernetes.io/instance={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl port-forward $POD_NAME 8080:80
{{- end }}

ch5 v99

exemple appli complete :
une appli qui va avoir une partie en node js en front et une dbmysql : l'appli va afficher un message a chaque fois qu'une personne fait un hit sur une url , une base de données est egalement créee. chaque connexion incremente un compteur en bdd et le message d'acceuil donne le nbre de visiteur du site 

boogie$ tree demo-chart                                                                                                                              [☸ kube-infra:fso]
demo-chart
├── Chart.yaml
├── README.md
├── requirements.lock
├── requirements.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   └── service.yaml
└── values.yaml

1 directory, 10 files


- fichier values.yaml :

boogie$ cat  values.yaml                                                                                                                             [☸ kube-infra:fso]
# Default values for demo-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 3

image:
  repository: boogie/node-demo-app            <<< l'appli en node js 
  tag: v0.0.1
  pullPolicy: IfNotPresent

service:
  type: LoadBalancer
  port: 80

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  path: /
  hosts:
    - node-demo-app.boogie
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #  cpu: 100m
  #  memory: 128Mi
  # requests:
  #  cpu: 100m
  #  memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

##
## MariaDB chart configuration
##
mariadb:
  enabled: true                         <<<<< definition de notre chart mysql 
  replication:
    enabled: false
  db:
    name: app
    user: app-user
  master:
    persistence:
      enabled: true
      accessMode: ReadWriteOnce
      size: 8Gi


- fichier requirement :

ce fichier va indiquer les dependances de notre chart : qui sera donc une image depuis la registry google et sera recupérée uniquement si la condition mariadb.enable est remplie : ce qui est le cas quand on examine la section mysql dans le values.yaml  
boogie$ cat requirements.yaml                                                                                                                        [☸ kube-infra:fso]
dependencies:
- name: mariadb
  version: 4.x.x
  repository: https://kubernetes-charts.storage.googleapis.com/
  condition: mariadb.enabled
  tags:
    - node-app-database


- les fichiers de templates :

on a par exemple le deployment , quelques modif sont faites : ex le port de l'appli est setté en dur ( port 3000 )
l'essentiel porte sur la definition de variables mariadb 

certains blocs contiennent le keyword template : 
ex :
          env:
          - name: MYSQL_HOST
          {{- if .Values.mariadb.enabled }}
            value: {{ template "mariadb.fullname" . }}
          {{- else }}

le lien de la construction pourra être fait en examinant le fichier _helpers.tpl 

dans notre cas la contruction de mariadb.fullname est faite grâce à cette indication dans le bloc helpers.tpl 

Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
*/}}
{{- define "mariadb.fullname" -}}
{{- printf "%s-%s" .Release.Name "mariadb" | trunc 63 | trimSuffix "-" -}}
{{- end -}}


boogie$ cat templates/deployment.yaml                                                                                                                [☸ kube-infra:fso]
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: {{ template "demo-chart.fullname" . }}
  labels:
    app: {{ template "demo-chart.name" . }}
    chart: {{ template "demo-chart.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ template "demo-chart.name" . }}
      release: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app: {{ template "demo-chart.name" . }}
        release: {{ .Release.Name }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          env:
          - name: MYSQL_HOST
          {{- if .Values.mariadb.enabled }}
            value: {{ template "mariadb.fullname" . }}
          {{- else }}
            value: unknown
          {{- end }}
          - name: MYSQL_USER
          {{- if .Values.mariadb.enabled }}
            value: {{ .Values.mariadb.db.user | quote }}
          {{- else }}
            value: unknown
          {{- end }}
          - name: MYSQL_DATABASE
          {{- if .Values.mariadb.enabled }}
            value: {{ .Values.mariadb.db.name | quote }}
          {{- else }}
            value: unknown
          {{- end }}
          - name: MYSQL_PASSWORD
            valueFrom:
              secretKeyRef:
              {{- if .Values.mariadb.enabled }}
                name: {{ template "mariadb.fullname" . }}
                key: mariadb-password
              {{- else }}
                name: unknown
                key: db-password
              {{- end }}
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
{{ toYaml .Values.resources | indent 12 }}
    {{- with .Values.nodeSelector }}
      nodeSelector:
{{ toYaml . | indent 8 }}
    {{- end }}
    {{- with .Values.affinity }}
      affinity:
{{ toYaml . | indent 8 }}
    {{- end }}
    {{- with .Values.tolerations }}
      tolerations:
{{ toYaml . | indent 8 }}
    {{- end }}



le fichier helpers.tpl va nous permettre de comprendre comment les infos du template sont alimentées :

boogie$ cat templates/_helpers.tpl                                                                                                                   [☸ kube-infra:fso]
{{/* vim: set filetype=mustache: */}}
{{/*
Expand the name of the chart.
*/}}
{{- define "demo-chart.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" -}}
{{- end -}}

{{/*
Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
If release name contains chart name it will be used as a full name.
*/}}
{{- define "demo-chart.fullname" -}}
{{- if .Values.fullnameOverride -}}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" -}}
{{- else -}}
{{- $name := default .Chart.Name .Values.nameOverride -}}
{{- if contains $name .Release.Name -}}
{{- .Release.Name | trunc 63 | trimSuffix "-" -}}
{{- else -}}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" -}}
{{- end -}}
{{- end -}}
{{- end -}}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "demo-chart.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" -}}
{{- end -}}

{{/*
Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
*/}}
{{- define "mariadb.fullname" -}}
{{- printf "%s-%s" .Release.Name "mariadb" | trunc 63 | trimSuffix "-" -}}
{{- end -}}


on va pouvoir poser un fichier readme qui contient les infos pour la creation du chart :

Chart.yaml  README.md  requirements.lock  requirements.yaml  templates  values.yaml
 ~/Documents/learn/kubernetes/learn-devops-the-complete-kubernetes-course/kubernetes-course/helm/demo-chart (master*) [10:33:07]
boogie$ cat README.md                                                                                                                                [☸ kube-infra:fso]
# Node demo app Chart

## Download dependencies
```
helm dependency update
```

## Install Chart
```
helm install .
```

## Upgrade Chart
```
helm upgrade --set image.tag=v0.0.2,mariadb.db.password=$DB_APP_PASS RELEASE .
```


on voit  qu'on commence  a charger les dependances :

## Download dependencies
```
helm dependency update


>>> ce qui va se baser sur le fichier de requirement et donc downloader le chart mariadb-4-3.1.gz defini dans notre fichier de requirement ( puisqu'on a vu que cette action est faite si mariadb enable est defini dans notre chart /values.yaml 


on lance l'install maintenant puisque nous n'avons plus de dependances : 

helm install 

on va biensur pouvoir upgrader notre appli :

## Upgrade Chart
```
helm upgrade --set image.tag=v0.0.2,mariadb.db.password=$DB_APP_PASS RELEASE .


on peut rollback 
on va examiner la version qu'on a :
helm history 


on selectionne la version qu'on veutr utiliser pour notre rollback 

ex :
helm rollback mon_app num_version 
help rollback my-app 1

helm list 
helm delete ourapp

attention si on a un statefull set : les volumes ne sont pas deleted 

ch5 -v 100


- Build et deployment de chart avec jenkins : 

un chart jenkins existe : on va donc s'en servir pour installer jenkins.
on cree un service account dédié : 

boogie$ cat serviceaccount.yaml                                                                                                                      [☸ kube-infra:fso]
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jenkins-helm
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: jenkins-helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: jenkins-helm
  namespace: default

# install jenkins
```
kubectl create -f serviceaccount.yaml

on lance l'install via helm en settant les rbac(si vieille version de kube) et en definissant le user jenkins avec un uid a 1000 ( pour ne pas executer en root )
helm install --name jenkins --set rbac.install=true,Master.RunAsUser=1000,Master.FsGroup=1000 stable/jenkins

on lance l'install .
Le plugin kubernetes de jenkins est installé dans notre deployment : on pourra lancer des pods depuis jenkins.

on se loggue sur jenkins : admin/ xxxx apres avoir recupérer les infos apres le deployment des conf kube.

1/ on update les plugins :

manage plugins > update

2/ on va créer un job de build pour pousser notre conf dans un bucket s3 amazon :

on s'assure de permettre au pod de pouvoir ecrire dans le bucket amazon : on recupere les id nécéssaire pour les connex aws

creation de job 
> type pipeline 

on set un nom 
on peut definir dans la section pipeline le definition d'un script / ou un script de repo  : en url ( au cas ou on a forké le repo)

on va inserer le script suivant dans la fenetre :

boogie$ cat Jenkinsfile.build                                                                                                                        [☸ kube-infra:fso]
pipeline {
  agent {
    kubernetes {
      label 'helm-pod'
      containerTemplate {
        name 'helm'
        image 'boogie/helm-s3'
        ttyEnabled true
        command 'cat'
      }
    }
  }
  stages {
    stage('Run helm') {
      steps {
        container('helm') {
          git url: 'git://github.com/boogie/kubernetes-course.git', branch: 'master'
          sh '''
          HELM_BUCKET=helm-rytcufor
          PACKAGE=demo-chart
          export AWS_REGION=eu-west-1

          cp -r /home/helm/.helm ~
          helm repo add my-charts s3://${HELM_BUCKET}/charts
          cd helm/${PACKAGE}
          helm dependency update
          helm package .
          helm s3 push --force ${PACKAGE}-*.tgz my-charts
          '''
        }
      }
    }
  }
}

on lance le build 
ceci va creer un pod qui va pousser les charts dans s3
on lance ensuite le deploy :
on va donc lancer la creation du pod ..qui va deployer l'appli dans notre cluster

on va créer un job de type pipeline en utilisant idem un script : 

ici nous allons deployer la conf helm dans notre cluster kube via le service account Jenkins-helm créé 

cat Jenkinsfile.deploy 

pipeline {
  agent {
    kubernetes {
      label 'helm-pod'
      serviceAccount 'jenkins-helm'
      containerTemplate {
        name 'helm-pod'
        image 'boogie/helm-s3'
        ttyEnabled true
        command 'cat'                          <<< ici la commande cat est lancé le pod tourne en background. 
      }
    }
  }
  stages {                               
    stage('Run helm') {               <<< la con commence les operations 
      steps {
        container('helm-pod') {
          git url: 'git://github.com/boogie/kubernetes-course.git', branch: 'master'
          sh '''
          HELM_BUCKET=helm-rytcufor
          PACKAGE=demo-chart
          export AWS_REGION=eu-west-1

          cp -r /home/helm/.helm ~                  <<< on copie les fichiers helms dans notre homedir
          helm repo add my-charts s3://${HELM_BUCKET}/charts    <<<< on ajoute le repo helm 
          DEPLOYED=$(helm list |grep -E "^${PACKAGE}" |grep DEPLOYED |wc -l)       <<< on compte les lignes et on controlle si la paquet est déja installé          
          if [ $DEPLOYED == 0 ] ; then
            helm install --name ${PACKAGE} my-charts/${PACKAGE}          <<< si le paquet n'est pas installé on l'installe 
          else
            helm upgrade ${PACKAGE} my-charts/${PACKAGE}                 <<<< sinon on upgrade.
          fi
          echo "deployed!"
          '''
        }
      }
    }
  }
}

on va donc injecter ce script dans un nouveau script jenkins, on lance et au premier run l'installation se fait.

On va pouvoir upgrader en chnageant la version .
On pourra donc pluguer jenkins a notre repo et lancer un deployment a chaque upgrade dans notre repo.
ex: on selectionne poll depuis jenkins pour recup les nouvelles versions de code.


= serverless in kubernetes = 

= fonctions dans kubernetes
les services de cloud permettent d'utiliser des composants serverless afin de  deployer des fonctions à la place d'instances ou de containers.
ex: azure functions, aws lambda, google cloud functions
dans ces cas nous n'avons ps à nous soucier des composants d'infrastructure .
Les functions n'ont pas à etre systematiquement mis à "on" contrairement aux containers : on peut donc réduire les couts d'utilisation.
on ne fait que payer pour l'utilisation que l'on fait de notre function.
on a juste à coder et le runtime d'execution est fourni par le cloud provider.
on doit faire un peu de setup : ex on doit utiliser une api aws pour indiquer une url qui lorsque qu'elle sera appellée executera le code de notre fonction.

Dans kube , au lieu d'utiliser des containers dans certains cas on peut utiliser des fonctions.
ex : de projets permettant l'utilisation de fonctions : kubeless, openfaas 
En tant qu'admin kube on va devoir gérer l'infra sous jacente mais en tant que dev on a juste a deployer nos fonctions.
Il va être important de bien examiner les differents projets serverless afin de choisir celui qui correspond le mieux a nos besoins ..et qui est bien mature et suivi.


= kubeless presentation : 

c'est un framework natif à kube.
https://github.com/kubeless/kubeless
il permet d'ameliorer les ressources de kube en fournissant de l'autoscaling, du routage , du monitoring.
Il utilise des customs ressources definition pour implementer ces fonctions.
completement opensource il offre une ui pour permettre aux devs son amélioration.
Avec kubeless ont deploy une fonction developpée dans le language de notre choix ex de runtime supportés : python /nodejs /ruby /php /golang ....
une fois qu'on a deployer notre fonction, on doit déterminer commment elle sera triggered.

Les fonctions http sont supportés : ex on peut permettre l'execution d'une fonction des qu'un endpoint est contacté.
ex: on ecrit une fonction qui renvoi du texte dans le navigateur des que l'url est contactée.
kube utilise dans ce cas un object ingress

des fonctions de pub / sub comme kafka sont egalement supportées.

= kubeless in action : 

- installation :

https://github.com/kubeless/kubeless

-> releases : 
on download la version qui nous interresse en fonction de notre os .
ex: 
# Install CLI
```
wget https://github.com/kubeless/kubeless/releases/download/v1.0.2/kubeless_linux-amd64.zip
unzip kubeless_linux-amd64.zip
sudo mv bundles/kubeless_linux-amd64/kubeless /usr/local/bin
rm -r bundles/
```

on peut tester la bonne installation du binaire en lancant l'executable kubeless depuis notre prompt

# Deploy kubeless
```
kubectl create ns kubeless
kubectl create -f https://github.com/kubeless/kubeless/releases/download/v1.0.2/kubeless-v1.0.2.yaml

# Example function

exemple on créee une petite fonction pythpn :

boogie$ cat example.py                                                                                                                               [☸ kube-infra:fso]
def hello(event, context):
  print event
  return event['data']

 pour creer nos fonctions on va definir le rutime que l'on veut utiliser :
 
## python
```
kubeless function deploy hello --runtime python2.7 \           <<<<< on defini notre runtime 
                               --from-file python-example/example.py \        <<<< on défini l'endroit du code contenant notre fonction;
                               --handler test.hello          <<< ici on a un handler de défini

on peut definir une fonction en ndejs qui sera similaire : 

boogie$ cat node-example/example.js                                                                                                                  [☸ kube-infra:fso]
module.exports = {
  myfunction: function (event, context) {
    console.log(event);
    return "Hello world!";
  }
}


on peut definir un fichier package.json de dépendance dans lequel on alimentera nos dependances en fcontions de nos besoins :
ex : ici la section dependance est vide : 
boogie$ cat node-example/package.json                                                                                                                [☸ kube-infra:fso]
{
  "name": "node-example",
  "version": "0.0.1",
  "scripts": {
    "start": "node example.js"
  },
  "engines": {
    "node": "^6.14.4"
  },
  "dependencies": {
  }
}

## NodeJS
```
kubeless function deploy myfunction --runtime nodejs6 \
                                --dependencies node-example/package.json \             <<<< on peut en fonction du runtime definir des dependences 
                                --handler test.myfunction \
                                --from-file node-example/example.js
```

# Commands
on peut passer des commandes kubeless pour examiner nos fonctions : 

## List Function

on va voir les fonctions que l'on a déployé.
kubeless function ls

## Call Function

on peut executer nos fonctions 
ex ici on va lancer la fonction nodejs en lui donnant en argument --data 'This is some data ' ..ce qui ne sert pas a grand chose puisque la fonction nodejs ne fait que retourner la chaine "Hello world!"

kubeless function call myfunction --data 'This is some data'

on peut examiner les logs de notre pod puisque la fonction nodejs log dans  la console 

kubectl  logs my-fonction-7766656-dfdfg

## Expose function

on va pouvoir exposer notre fonction de maniere à pouvoir executer notre fonction via l'appel a une url ( ici on va utiliser une ressource de type ingress et un lb aws ) : 

on créer notre conf : 

boogie$ cat nginx-ingress-controller-with-elb.yml                                                                                                    [☸ kube-infra:fso]
---

apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: default-http-backend
  labels:
    app.kubernetes.io/name: default-http-backend
    app.kubernetes.io/part-of: ingress-nginx
  namespace: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: default-http-backend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: default-http-backend
        app.kubernetes.io/part-of: ingress-nginx
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: default-http-backend
        # Any image is permissible as long as:
        # 1. It serves a 404 page at /
        # 2. It serves 200 on a /healthz endpoint
        image: gcr.io/google_containers/defaultbackend:1.4
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        ports:
        - containerPort: 8080
        resources:
          limits:
            cpu: 10m
            memory: 20Mi
          requests:
            cpu: 10m
            memory: 20Mi
---

apiVersion: v1
kind: Service
metadata:
  name: default-http-backend
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: default-http-backend
    app.kubernetes.io/part-of: ingress-nginx
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app.kubernetes.io/name: default-http-backend
---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
data:
  use-proxy-protocol: "true"
---

kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---

kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
        - events
    verbs:
        - create
        - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx
---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
      annotations:
        prometheus.io/port: '10254'
        prometheus.io/scrape: 'true'
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.18.0
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            capabilities:
                drop:
                - ALL
                add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
          - name: http
            containerPort: 80
          - name: https
            containerPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
---
kind: Service
apiVersion: v1
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
  annotations:
    # Enable PROXY protocol
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'
    # Increase the ELB idle timeout to avoid issues with WebSockets or Server-Sent Events.
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '3600'
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: ingress-nginx
  ports:
  - name: http
    port: 80
    targetPort: http
  - name: https
    port: 443
    targetPort: https


et on l'applique : 

kubectl create -f nginx-ingress-controller-with-elb.yml


maintenant on va s'asssurer que les regles ingress pointant vers cette fonction sont créées. 

on va donc créer une fonction qui s'appellera myfunction et qui sera executée quand l'url myfunction.kubernetes.newtech.academy sera appellée.

on crée un enregistrement dns chez aws pour notre sous domain myfunction de kubernetes.newtech.academy et au bout de quelques minutes un appel a myfunction.kubernetes.newtech.academy nous renvoi hello world ! le resultat de notre function.

kubeless trigger http create myfunction --function-name myfunction --hostname myfunction.kubernetes.newtech.academy

ch6 v105

= pub sub exemple avec kafka : =

on install kafka et zookeeper : 
# PubSub
## Kafka Installation
export RELEASE=$(curl -s https://api.github.com/repos/kubeless/kafka-trigger/releases/latest | grep tag_name | cut -d '"' -f 4)
kubectl create -f https://github.com/kubeless/kafka-trigger/releases/download/$RELEASE/kafka-zookeeper-$RELEASE.yaml
```

on va créer et deployer une fonction qui converti les données en entrées en majuscules  :

boogie$ cat  node-example/uppercase.js                                                                                                               [☸ kube-infra:fso]
module.exports = {
  uppercase: function (event, context) {
    str = event['data'].toUpperCase()
    console.log(str);
    return str
  }
}

## Deploy function
kubeless function deploy uppercase --runtime nodejs6 \
                                --dependencies node-example/package.json \
                                --handler test.uppercase \
                                --from-file node-example/uppercase.js


dans l'exemple en http on linkait la fonction avec un objet ingress, ici on agit différemment : 

on exécute la commande suivante qui va déclencher la creation de notre fonction via kubeless ( --function-selector created-by=kubeless ) et un topic uppercase sera triggered : 

## Trigger and publish
kubeless trigger kafka create test --function-selector created-by=kubeless,function=uppercase --trigger-topic uppercase

le declenchement de cette fonction va se faire en publiant des data dans le topic : on envoit les data au topic : la fonction de conversion se déclenche.

on peut se passer de kubeless pour pousser dans notre topic kafka : n'importe quel outil adapté peut faire le job.
Dans notre cas nous ne disposons que de kubeless on va donc l'utiliser en passant les argument nécéssaire à la publication de data dans notre topic.

kubeless topic publish --topic uppercase --data "this message will be converted to uppercase"

en examinant les logs de notre pod hébergant kafka on va voir dans les logs le message posté en cli ..mais converti en majuscule...


kubectl get logs uppercase-dfssddd543-dfdjsd

....
THIS MESSAGE WILL BE CONVERTED TO UPPERCASE
...


boogie$ cat  node-example/uppercase.js                                                                                                               [☸ kube-infra:fso]
module.exports = {
  uppercase: function (event, context) {       <<< on a donc la fonction qui va recupérer les flux en entrée
    str = event['data'].toUpperCase()          <<< le flux d'entrée event contient les data ( dans notre cas le message en minuscule) puis ces data sont converties.
    console.log(str);                          <<< on envoi le tout recupérer dans une variable str dans notre console de log
    return str
  }
}


biensur  on pourra stocker nos data converties dans une db, recupérer nos data pour les renvoyer dans kafka en creant un produceur etc .....


ch7 to do 

== Microservices : 




== kubeadm : ==

ch8 : to do  git clone repo "on perm or cloud"  et examiner le rep script.

kubeadm va nous permettre de build notre cluster et nous permet d'installer kube sur toute type de distro linux.

kubeadm peut utiliser des tokens de bootstrap : simple token qui permettent de booter un cluster ou de permettre de  joindre un node plus tard.
on va utiliser les tokens pour permettre à un node de joindre notre cluster de manière secure.

kubeadm permet l'upgrade et le downgrade de cluster.
Attention il faut nous même installer un CNI (container network interface) : kubadm ne le gère pas.

mini 2 Gb ram / 2 cpu .
s'assurer des connexions entre les differents nodes du cluster ( private network ou reseau pub routé et filtré via un fw) 





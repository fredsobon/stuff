====     notes cours kube admin certif : ===


== Core concepts : ==

=  architecture générale :
kubernetes est un systeme qui va gérer le de containers qui vont embarquer des applications. Il va entre autre permettre le déploiment de containers.

- les workers nodes : sont les serveurs qui vont héberger les containers contenant les applications.
- le master  est le cerveau qui va gerer les containers, les identifier, sur les workers, les monitorer etc ...: plan; schedule, monitor les nodes 

- etcd : est le composant qui va enregister la configuration sous forme clé valeur de nos containers, config etc ...
- kube-scheduler : est le composant du master qui va permettre de gérer le bon dispatch des containers en fonctions des ressourcers nécéssaires et différentes config inhérantes au cluster.
- le controller manager du master est le composant qui s'assure que tous les evenements sont corrects et que la situation actuelle est bien celle qui est défini dans les confs : que la flotte de container est cohérente  : il existe plusieurs sous composants du controller manager ( chaqun dédié à un périmetre précis. ex :
 node controller : va s'assurer de la bonne santé des containers, s'assurer qu'ils répondent bien ...
 replication controller : va s'assurer que le nombre de container défini et nécéssaire est bien correctement lancé 
 .... il ya toute une liste de controller qui seront etudier précisement.
- kube-api : est le composant central de kube qui va gérer l'orchestration de tous les composants : les composants internes au master, les communications des nodes vers le master et les communication de l'exterieur du cluster vers les composants. C'est le point d'entrée global.
- container runtime : toutes les applications vont être hébergées sous forme de container sur nos nodes. Il est possible d'avoir les composants du master sous forme de container aussi , ex dns etc ..donc pour faire tourner des containers il va falloir avoir le runtime dédié. 
Dans le cas le plus courant à l'heure actuelle il s'agit de docker.
-kubelet : ce composant est un agent qui est hébergé sur les nodes et qui va communiquer en permanence avec le kube-api server afin de recevoir les confs et tous les ordres a appliquer sur les containers.
- kube-proxy est le composant qui va permettre aux differentes applications hébergées sur les nodes de communiquer entre elles ( ex un serveur web d'un container hébergé sur un node voulant communiquer avec une database hébergée sur un autre node... )

= etcd : 

- presentation : 
etcd def : "etcd is a distributed  reliable key value store that is simple, secure and fast"
un systeme clé valeur permet de stocker les infos sous forme de document (contrairement aux bdd transactionnelles qui stockent en tables)
chaque document (comportant donc des couples clés valeurs) peuventt être modifiés indépendamment (contrairement aux tables habituelles dans lesquelles un ajout de colonnes ou de valeurs affectent toutes les données des tables)
Nous n'avons pas a updater les autres documents quand nous ajoutons une info dans un document précis.
Le format est assez simple : yaml ou json 

- install etcd : 
sudo apt install etcd 
ou recupérer le binaire sur github

Le serveur etcd écoute sur le port 2379 

boogie@satellite:~/Documents/stuff/kube$ ps fauxw |grep etcd
boogie    5102  0.0  0.0  17480   892 pts/1    S+   19:11   0:00              |           \_ grep --color=auto etcd
etcd      4984  0.9  0.2 11548340 17532 ?      Ssl  19:10   0:00 /usr/bin/etcd
boogie@satellite:~/Documents/stuff/kube$ ss -atln |grep 2379
LISTEN   0         128               127.0.0.1:2379             0.0.0.0:*    

- ecrire des données en base : 

on peut utiliser le client etcdctl (fourni dans le paquet pour injecter /retrouver des data ) :
boogie@satellite:~/Documents/stuff/kube$ etcdctl set key1 value1
value1
boogie@satellite:~/Documents/stuff/kube$ etcdctl set key2  boogie
boogie
boogie@satellite:~/Documents/stuff/kube$ etcdctl set bingo  bongo
bongo

- lire les données en base : 
boogie@satellite:~/Documents/stuff/kube$ etcdctl get key1
value1
boogie@satellite:~/Documents/stuff/kube$ etcdctl get key2
boogie
boogie@satellite:~/Documents/stuff/kube$ etcdctl get bingo
bongo

le detail des options est valable avec etcdctl --help

- Etcd -  role dans kubernetes : 

le role d'etcd dans le cluster kube est de stocker les valeurs des nodes, pods, configs, secrets, accounts, roles, bindings ....
chaque modification dans notre cluster est enregistrée dans le cluster etcd 

Nous pouvons déployer notre cluster etcd de plusieurs méthodes : ex : from scratch , via kubeadm ...il est important de comprendre les différences.
 
- from scratch :
on va downlader le binaire ou installer via notre distro 
definir un fichier de service (si ce n'est fait automatiquement via le package)
boogie@satellite:~/Documents/stuff/kube$ cat /lib/systemd/system/etcd.service
[Unit]
Description=etcd - highly-available key value store
Documentation=https://github.com/coreos/etcd
Documentation=man:etcd
After=network.target
Wants=network-online.target

[Service]
Environment=DAEMON_ARGS=
Environment=ETCD_NAME=%H
Environment=ETCD_DATA_DIR=/var/lib/etcd/default
EnvironmentFile=-/etc/default/%p
Type=notify
User=etcd
PermissionsStartOnly=true
#ExecStart=/bin/sh -c "GOMAXPROCS=$(nproc) /usr/bin/etcd $DAEMON_ARGS"
ExecStart=/usr/bin/etcd $DAEMON_ARGS
Restart=on-abnormal
#RestartSec=10s
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
Alias=etcd2.service

toute une partie de configuration n'est pas présente de base et est fondamentale pour notre serveur kubernetes : la gestion de certif , les config cluster etc ...
notamment la ligne : --advertise-clients-urls https://${INTERNAL_IP}::2379 

- kubeadm :
en deployant le cluster kube via kubeadm celui ci cree un cluster etcd sous forme de container que l'on peut voir avec :
kubectl get pods -n kube-system 
..
kube-system etcd-master

- pour examiner les clés enregistrées dans le cluster etcd on peut faire simplement :

kubectl exec etcd-master -n kube-system etcdctl get / --prefix -keys-only 

kubernetes stocke ses datas dans une arbo précise de sa registry : "/" 
sous "/" on a les differentes constructions : repertoires : minions, pods,nodes, replicasets ...

dans un cluster kube haute dispo on a plusieurs master kube ayant chacun des server etcd master : il faut s'assurer dans ce cas que chaque serveur etcd puisse communiquer avec les autres en adaptant la conf dédiée ( examen de la conf en pratique plus tard. )
--initial-cluster controller-0=https://${CONTROLLER0_IP}:2380,controller-1=https://${CONTROLLER1_IP}:2380

--> Chap2-video4 done

= kube apiserver : 

c'est le composant principal de managment dans kubernetes.
quand on utilise kubectl pour consulter par exemple les données stockées dans etcd , kubectl  va contacter le kube apiserver qui va authentifier la requette ( examiner les droits du user qui se connecte ) , puis interroger etcd qui retourne les valeurs a kube apiserver qui les renvoi au client ayant initié la demande.
on peut aussi directement communiquer avec l'api sans utiliser kubectl .

ex : grande etapes pour créer un pod : 
curl -X POST /api/v1/namespaces/default/pods ...[other] 
kube apiserver va :
1- authentifier le user 
2- valider la requette
3- recupérer les data 
4- mettre a jour etcd
5 -informer le user que le pod est crée
ensuite 
6- le scheduler qui observe en permanence le kube api server va realiser qu'il y a un nouveau pod non assigné sur un node.
7- le scheduleur identifie grace aux specificités du pods et des ressources des nodes un node qui va heberger le nouveau pod
8 - il informe le kube apiserver qu'un node a un nouveau pod attribué
9 - kube apiserver met a jour la base etcd avec les infos du node ébergeant le pod
10 - kube apiserver va ensuite envoyer l'information à l'agent kubelet présent sur le node qui va héberger le nouveau pod. 
11 - le kubelet va creer le pod et donner l'instruction au runtime de container de deployer l'image 
12 - kubelet informe ensuite kube apiserver que les opérations sont en place
13 - kubeapiserver update la base etcd

Ce type d'opérations est appliqué a chaque fois que des modifications sont a appliquer sur le cluster. Le kube apiserver est au centre de toutes les communications.

L'installation de kube apiserver peut se faire aisement via kubeadm mais il est important de connaitre les différents éléments de configuration présents pour une installation manuelle.
Il est fondamental de prendre en compte que tous les composants du cluster doivent pouvoir communiquer et que des certificats ssl sont à mettre en place . Le kube apiserver est le seul élement a communiquer directement avec etcd server . Sa presence dans la confi de apiserver n'est donc pas surprenante. 
Nous pouvons examiner la conf de kubeapiserver en downloadant le binaire sur le site de kube.
En fonction du mode d'installation ( kubadm/ scratch )on peut voir la conf dans :

/etc/kubernetes/manifeest/kube-apiserver.yaml
ou en service 
/etc/systemd/system/kube-apiserver.service

ch2 v5 -> done

= kube controller manager : 

il manage différents controllers dans kube.
Le role essentiel est de controller en permanence l'etat des ressources et remédier à un pb quand il y en a. puisque son role est s'assurer que l'etat de fonctionnement est totalement equivalent a la configuration et l'etat désiré du system.

- node controller : va s'occuper de monitorer les nodes et s'assurer que les applications fonctionnent correctement sur les nodes .Toutes les actions transitent par le kube apiserver comme on l'a vu.
le node controller recupere l'etat des nodes toutes les 5 secondes.
Si le node controler recoit une alerte de heartbeat d'un node : il flaggue celui ci comme unreachable pendant 40 secondes.
Si le node est toujours ko au bout de 5 minutes , le node controller va ejecter les pods et les héberger sur d'autre nodes si ces pods font partis d'un replicaset.

- replication controller : il est responsable des replicasets et doit s'assurer que le nombre de pods on line définis dans les confs de replicasets est bien corrects
si un pod crash alors le replication controller en recrée un.

Il y a a beaucoup de controller au sein de kube et du controller manager :
deployement controller, namespace controller , job controller , service account controller , endpoints controller ......

Tous ces controllers sont embarqués dans l'installation du pacquet de kube controller manager
si on download le paquet depuis le site de kube et qu'on lance le service : on peut examiner la conf quand on regarde le kube-controller-manager.service 
On peut configurer les conf de heartbeat, check etc ..dans cette config , de meme on peut définir l'aaplication des services controller manager que l'on veut activer dans cette section.

pour examiner le kube-controller-manager : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
la conf est comme d'habitude dans ce cas dans :
/etc/kubernetes/manifests/kube-controller-manager.yaml

sinon on peut examiner la conf dans le service gérer par systemctl.

on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v6 -> done.

= kube-scheduler : 

il est responsable de la planification des pods sur les nodes.Il va gérer le dispatch des pods sur les nodes. Quel pod va sur quel nodes en fonction des besoins du pods et des ressources des nodes.
Le sheduler examine la conf des pods et va s'assurer que le node qui va acceuillir ce pod en fonction de cette conf est correctment sizer en terme de ressource et de config specifique par exemple.
exemple : si notre pod a besoin de 10 cpu et qu'on est fasse a 4 nodes : deux nodes ont 4 cpu dispos , 1 a 12 cpu , le dernier a 16 cpu.
Les deux premiers nodes sont filtrés par le sheduler : pas assez de ressources.
Pour les deux autres nodes : le sheduler va faire un classement en donnant des scores de 0 a 10 aux nodes .
le scheduller va calculer le nombre de ressource dispo apres le set up de notre pod
ici : 2 et 6 . Le dernier node a donc une meilleur note.

L'installation du process se fait comme d'habitude en downloadant le binaire et l'exxecutant .

pour examiner le kube-scheduler : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
la conf est comme d'habitude dans ce cas dans :
/etc/kubernetes/manifests/kube-controller-manager.yaml

sinon on peut examiner la conf dans le service gérer par systemctl.

on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v7 -> done

= kubelet : 

Ce process est hébergé sur les nodes .c'est le process qui va être responsable de toutes les interractions avec le master.
iL va s'occuper de charger / décharger les containers.
Le kubelet va enregister le nodes, declencher la creation du pod via le runtime container et il va monitorer les pods 

kubeadm ne deploit pas automatiquement le conf du kubelet.
On doit toujours faire les installations manuelles des kubelet sur nos nodes.
on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v8 -> done

= kube-proxy : 

dans un cluster kube chaque pod peut contacter tous les autres pods.Ceci est possible grace a un reseau dédié pour les pods. Il y a beaucoup de solution dispos pour gérer la conf reseau des pods.

On peut avoir un pod qui heberge un server web ( 10.1.1.1 ) et un pod qui heberge une db ( 10.1.1.2)
le web peut contacter la db via son adresse ip de pod ...mais on est jamais sur que le pod ne va pas changer d'ip ou que le pod ne va pas crasher.
Il est plus sage de créer un service qui va expposer notre db dans le cluster : service db : 10.96.0.12 
On va donc contacter ce service qui va forwarder le traffic vers le backend. ( notre db ) , le service doit etre accessible depuis tous les nodes.
Le service est virtuel ..qui n'existe que dans la memoire de notre cluster kube.

kube-proxy est un process qui tourne sur tous les nodes du cluster .Son role est de surveiller toutes les nouvelles creations de services et d'ecrire les regles qui vont forwarder le service vers le backend correspondant .
Ceci est fait par des regles iptables.
on va avoir une regle iptable sur tous les nodes qui va forwarder le traffic du service db ( 10.96.0.12 ) vers le pod de backend ( 10.1.1.2 )

L'installation se fait comme d'habitude.

pour examiner le kube-proxy : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
Il est present sur tous les nodes en tant que daemonset :
kubectl get daemonset -n kube-system 

v2 ch9 -> done

= pods : 

on s'assure que le pod existe : un cluster existe, une image docker existe et kube peut la puller depuis une registry

un pod est l'objet le plus petit qu'on peut créer dans kube.
le plus petit est donc un container , d'une application contenu dans un pod.
En cas de charge ..on ne va pas creer un nouveau container de notre appli dans le pod ..on va créer un nouveau pod 
Si on est en cas de grosse charge on va créer des nouveaux pods sur un nouveau node.

On peut biensur avoir plusieurs container dans notre pod mais ils doivent chacun héberger une appli différente.
On est obliger de creer des pods dans kube ...meme si on a une tres simple appli ..mais c'est tres bien car on peut modifier nos archi sans pb

Pour lancer un pod :
on  va invoquer kubectl qui va demarrer un container nginx en downloadant l'image depuis la registry configurée ( docker hub ou une registry privée) :
kubectl run nginx --image nginx 
on va pouvoir lister les pods présents avec :
kubectl get pods 

on va pouvoir definir un objet pod puis creer le pod qui sera ici de cette conf 

cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels: 
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container 
      image: nginx

on instancie la creation de l'objet avec : 
kubectl create -f myapp-pod.yaml

si on modifie un param dans notre objet on pourra reappliquer le changement avec :
kubectl apply -f myapp-pod.yaml

v2 ch10 -> done

= kubernetes controllers : Replication controller : 

les controllers sont les cerveaux. Ce sont les process qui monitorent les objets kube et s'assurent qu'ils répondent correctement.

- replicat : 
on va assurer le principe de HA high availibility en assurant que si un pod crash le service est rendu au user. 
Même si on a un seul pod , le replication controller va nous aider a assurer le fonctionnement de notre appli.
Il s'assure que le nombre de pod défini dans la conf est toujours équivalent a ce qu'il y a dans le cluster kube.
Le replication controller va naturellement nous aider à assurer les montée en charge en créant les pods nécéssaires pour assurer le bon maintien de l'appli.
et ce même sur différents nodes.

On a deux termes qui ont le même sens :
replication controller 
replica set 

mais attention ils sont diférents.
replication controller  est l'ancienne techno qui a été remplacée par le replicat set. 
! Le replica set est à privilégier !

v11 no lesson 

- replication controller :

on va utiliser une structure qui sera présente dans toutes les fichiers de définition  de nos objets kube :
la premiere partie de notre definitoin est assez similaire a celle de la creation d'un pod 
la seconde partie est cruciale et c'est elle qui va gérer les specificité de notre objet ( section spec: )
vi rc-definition.yaml 

apiVersion: v1 ( -> cela va dependre de l'objet que nous creons: pour un replication controller c'est l'api v1 qui supporte cet objet.)
kind: ReplicationController (->  ici on renseigne le type d'objet que l'on veut creer : ici ReplicationController)
metadata:  on va rajouter ici différentes information specifique 
  name: myapp-rc
  labels: 
    app: my-app
    type: front-end
spec: on sait ici que dans l'object replication controller la definition va gérer la creation de plusieurs instances de pods
  - template: ( -> on va creer un template de pod qui sera utiliser par le replication controller pour gérer nos réplicas pour cela on va juste inserer la definition d'un pod au préalable créer par exemple : on va imbriquer les définitions d'objects. On reprend l'integralite de notre defintion d'objet pod SAUF la section apiVersion et kind ))
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas : 3 ( -> c'est dans cette section (qui doit se situer au même niveau que le spec du replication-controller que l'on doit setté le nombre de pod que l'on veut afin d'assurer la HA de notre appli. )
    
nous avons donc deux objets imbriqués avec deux sections metadata et specs.
Attention à l'indentation comme dans tout fichier yaml on doit bien s'assurer de la structure de notre fichier 
vi rc-definition.yaml 

apiVersion: v1 
kind: ReplicationController 
metadata:  
  name: myapp-rc
  labels: 
    app: my-app
    type: front-end
spec: 
  - template: 
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas: 3 

on va ensuite creer notre replication-controller :

kubectl create -f rc-definition.yaml

on va pouvoir examiner les replication-controller :
kubectl get replicationcontroller

NAME   DESIRED  CURRENT READY AGE
my-app       3        3     3  5s
on voit donc les différents status : etat attendu, l'etat actuel et le status du rc

kubectl get pods va nous montrer les trois pods differents crées via notre rc.

- replica set :

vi replicaset-definition.yaml 

apiVersion: apps/v1   --> ici on est obligé ded saisir apps/v1 pour l'apiversion : sous peine d'avoir une erreur de kube
kind: ReplicaSet      --> on renseigne ici le replicatset comme type 
metadata:  
  name: myapp-replicaset  
  labels: 
    app: my-app
    type: front-end
spec: 
  - template: 
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas: 3 
selector:         --> on est obligé ici de définir le pod qui va être gérer par le replicaset. 
  matchLabels: 
    type: front-end  --> le matchlabels va reprendre le type defini dans notre définition de pod.
    
Ceci est nécéssaire car le replicaset peut gérer d'autre pods que ceux inclus dans la definition même du replicaset. C'est la difference majeure entre les replication controller et les replicaset.
on voit qu'on est obliger de setter une mention matchLabels qui va recupérer les  infos settées dans notre section labels précédente.

kubectl create -f replicatset-definition.yaml

on va pouvoir examiner les replication-controller :
kubectl get replicaset

NAME   DESIRED  CURRENT READY AGE
my-app       3        3     3  5s
on voit donc les différents status : etat attendu, l'etat actuel et le status du rc

kubectl get pods va nous montrer les trois pods differents crées via notre rc.


- Labels et selectors : 

on a vu que le replicaset /controller s'occupe de gérer les pods crées et s'assure qu'ils sont up and running.
ce sont donc des process qui monitorent les pods.
Comment le replicat set connait les pods qu'il doit surveiller ..il peut y avoir des centaines de pods faisant tourner plusieurs applications.
C'est la que donner un label à notre pod lors de sa creation peut être vraiment utile..
on peut donc maintenant donner ce label comme filtre pour notre réplicatset.
Le replicaset sait donc maintenant quel pods il doit monitorer.
Le replicaset monitore l'etat des pods crées et s'assure que le nombre en prod en cohérent avec la définition.


on retrouve les concepts de labels dans diffrents objets gérés dans kubernetes.


Attention si on doit gérer un replicatset de pods déja crées ..il faut s'assurer que le bon template est charger dans notre definition. Le rc ne créera pas de nouveau pod si ceux existant avant sa creation comporte les bonnes informations.

- scaling :

il peut nous arriver de definit un replicat a 3 puis avoir une montée en charge et definir a 6 notre réplicat .
On peut agir de plusieurs manieres :

- update le champ replicat dans notre définition : le passer de 3 à 6.
ensuite on met a jour notre rc :

kubectl replace -f replicaset-definition.yaml

- on peut utiliser la commande scale en donnant nos arguments :

kubectl scale --replicas=6 -f replicaset-definition.yaml

on peut utiliser cette commande en utilisant le type name format : 
kubectl scale --replicas=6  replicaset myapp-replicaset

Attention dans ces cas le fichier de definition ne sera pas updater de 3 à 6 réplicats : les pods seront créer mais la conf inchangée.

cmds quick recap : 

kubectl create -f def.yaml
kubectl get replicaset
kubectl delete replicaset myapp-replicaset
kubectl replace -fdef.yaml
kubectl scale --replicas=6 -f def.yaml 
kubectl edit replicasets new-replica-set

v2 ch12 ->done


-> tp replicatset -> done

kubectl create -f /root/replicaset-definition-1.yaml
master $ kubectl delete replicasets replicaset-1 replicaset-2
kubectl edit replicasets new-replica-set
master $ kubectl describe replicasets new-replica-set
Name:         new-replica-set
Namespace:    default
Selector:     name=busybox-pod
Labels:       name=busybox-pod
Annotations:  <none>
Replicas:     4 current / 4 desired
Pods Status:  0 Running / 4 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  name=busybox-pod
  Containers:
   busybox-container:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      echo Hello Kubernetes! && sleep 3600
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-b4dzk
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-5hs7t
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-w6pcw
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-vtfv9
  Normal  SuccessfulCreate  16m   replicaset-controller  Created pod: new-replica-set-j2pkm
  Normal  SuccessfulCreate  15m   replicaset-controller  Created pod: new-replica-set-9sx4x

= deployment : 

Le deployment  est un objet kube qui va permettre le deploiement / mise à jour, rollback d'application sans interruption de service pour le client : seamless deployment / rolling update ( mise à jour séquentielle des pods ) sans interruption de service.
On va pouvoir mettre a jour une nouvelle version d'une appli dispo dans le docker hub pod par pod , sans devoir relancer tous les pods comme cela se passerai avec le replicaset.
on va pouvoir rolbacker sur la version précedente de notre appli, on va pouvoir tester le comportement de notre nouvelle appli sur un pod avant de déployer sur tout notre parc ..toutes ses possibilités sont offertes par l'objet deployment.

Tous les containers sont encapsulés dans des pods qui sont deployés par des replicaset ou replication controller : le deployment encapsule tout ces objets dans la hierarchie kube.

La définition d'un deployment se fait comme pour les replicasets :


vi deployment-definition.yaml

apiVersion: apps/v1   --> ici on est obligé ded saisir apps/v1 pour l'apiversion : comme pour le replicaset.
kind: Deployment      --> on renseigne ici le deployment  comme type
metadata:
  name: myapp-deployment
  labels:
    app: my-app
    type: front-end
spec:
  - template:
     metadata:
       name: myapp-pod
       labels:
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container
           image: nginx
replicas: 3
selector:        
  matchLabels:
    type: front-end 


On retrouve le template de pod et le nombre de replicas désiré.

kubectl create -f deployment-definition.yaml
on verifie la creation : 
kubectl get deployments
le deployment crée automatiquement le replicaset 
kubectl get replicaset
et les pods sont aussi crées automatiquement :
kubectl get pods

on peut voir la création globale de nos objets avec :

kubectl get all 

ch2 v13 -> done


- tp deployment -> done

ex deployment :

master $ kubectl describe deployments
Name:                   frontend-deployment
Namespace:              default
CreationTimestamp:      Wed, 01 May 2019 17:57:33 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision=1
Selector:               name=busybox-pod
Replicas:               4 desired | 4 updated | 4 total | 0 available | 4 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=busybox-pod
  Containers:
   busybox-container:
    Image:      busybox888
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      echo Hello Kubernetes! && sleep 3600
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   frontend-deployment-b9b7c6cb6 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set frontend-deployment-b9b7c6cb6 to 4



creation de deploiement :
avec comme nom httpd-frontend  / replicas 3 et image : httpd:2.4-alpine
master $ cat my-deploy.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      name: httpd-pod
  template:
    metadata:
      labels:
        name: httpd-pod
    spec:
      containers:
      - name: httpd-container
        image: httpd:2.4-alpine
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600



= Namespaces : 

Les namespaces vont permettre de cloisonner nos resssources dans des espaces complétements distincts et hermétique. on pourra interragir diectement dans un namespace si il est accessible depuis notre namespace  actuel . 
On peut interroger cependant des objets appartenant a différents namespaces.

De base tous les objets que l'on crée et avec lesquels on interréagis sont dans le "default" namespace de kube.
Kube pour ses besoins internes crées des objets, pods services , dns, network etc ... dans un namespace privé afin de le protéger des users : c'est le namespace appellé kube-system

Un troisieme namespace est crée automatiquement c'est le kube-public : dans lequel on devrait avoir tous les objets qu'on l'on veut rendre accessible, dispo pour les users.

Bien evidemment pour des tests on peut continuer a travailler dans le namespace par default mais on doit impérativement créer des namespace dans les environmments devant important et en production.

on va pouvoir utiliser le même cluster pour différents env ( dev, preprod )mais on va isoler leur ressources avec des namespaces .

On va pouvoir attribuer des quotats par namespaces ( nombre de pods, cpu , memoire ) : afin d'assurer la garantie de ne pas dépasser des ressources physiques .


On va pouvoir au sein d'un namespace appellé directement un service :
ex : dans le namespace default :

- un web-pod
- un web-deployment
- un service-db

pour se connecter a la db on pourra simplement faire :

ex : mysql_connect("db.service")

Si on doit atteindre un service dans un autre namespace ex dev on devra alors utiliser un nom complet créer et diffusé dans le service dns a la création du service par kub lui même :
ex : mysql_connect("db.service.dev.svc.cluster.local") 

on retrouve le nom du service , le namespace , le type d'objet : svc ici et le domain kube natif cluster.local

- lister les pods d'un autre namespace :

kubectl get pods --namespace="notre_namespace"

- creer un pod dans un namespace particulier : 
kubectl create -f pod-definition.yaml --namespace="my_namespace"

On peut rajouter directement le namespace concerné par notre pod a la creation de celui ci en rajoutant :


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod

metadata:
  name: dev
  namespace: my-namespace  <<<< on ajoute ici le namespace de notre object.
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

- creation de namespace :

on peut ecrire un objet namespace :

cat myapp-namespace.yaml
---
apiVersion: v1
kind: Namespace

metadata:
  name: dev

puis :
kubectl create namespace -f myapp-namespace.yaml

on peut creer a la volée le namespace aussi :

kubectl create namespace  dev 

- Switching de namespace : 

on va pouvoir pour plus de facilité quand on travaille longtemps sur un namespace le définir par défaut : de maniere a ne pas devoir systematiqueme ntutiliser le flag : --namespace

kubectl config set-context $(kubectl config current-context) --namespace=dev 

- listing de tous les pods de tous les namespaces : 

pour voir tous les pods de tous les namespaces :

kubectl get pods --all-namespaces


= Quota :
on va pouvoir definir des quota de ressources pour nos namespaces :


cat myapp-quota.yaml
---
apiVersion: v1
kind: ResourceQuota

metadata:
  name: compute-quota
  namespace: dev 
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limit.cpu: "10"
    limit.memory: 10Gi


kubectl create  -f myapp-quota.yaml

ch2 v14 -> done

tp namespace ->  done

- creation d'un pod pour le namespace finance : 
master $ cat fin-pod.yaml
apiVersion: v1
kind: Pod

metadata:
  name: redis

metadata:
  name: redis
  namespace: finance
  labels:
    app: redis
    type: cache
spec:
  containers:
    - name: redis-container
      image: redis
master $ kubectl create -f fin-pod.yaml --namespace=finance
pod/redis created


= services : 
Les services vont nous permettre la communication interne et externe à notre cluster avec notre application.
On va donc pouvoir connecter nos applications et nos users entre eux.
par exemple : les services vont permettre à nos users de communiquer avec nos front-end, nos frontend avec nos backends et nos backends avec les db.

ex : 

un user sur un laptop en 192.168.0.2 
un node sur le même reseau que le user : 192.168.0.5
un pod sur le node en 10.244.0.2 qui héberge un service web en 80
un reseau pour nos pods en 10.244.0.0 

1/ de base : 
le user ne ping pas le pod
2/ en se connectant en ssh sur le node le user peut communiquer avec le pod : curl http://10.244.0.2 > ok
biensur on est donc dans ce cas dans le cluster kube ....ce n'est pas vraiment ce que l'on veut.

on veut pouvoir atteindre l'appli de notre pod via notre laptop: c'est la qu'interviennent les services.
Le role du service est d'ecouter un port sur le node et de forwarder les connexions sur ce port au port de notre application sur le pod : c'es tce qu'on appelle un NodePort service.

Il y a plusieurs type de service :

NodePort c'est le cas le plus simple que nous venons de décrire brièvement.
ClusterIp : qui sera un service virtuel crée au sein du cluster qui permettra la commnunication entre différents services ( ex: frontend avec backend via ce service )
LoadBalancer : qui va servir à repartir la charge entre nos différents pods hébergeant une appli frontend par exemple.

- NodePort : 
dans notre cas le laptop accedera a l'appli en appellant l'ip pod suivi d'un port .Cette connexion sera forwardée par le service sur le pod qui hébergera l'application.

sur le pod
on a le "target port" du pod qui sert l'application -> 10.244.0.2:80
sur le service 
on a le port 80  qui sera le même que celui de notre appli , le service a une ClusterIp qui permettra de recevoir les flux du client passant par le node puis envoyé ce flux vers le pod et le service final : ex: 10.106.1.12:80 
sur le node :
on a un port qui permettra d'acceder a ce node et au service  : c'est le NodePort ex: 192.169.0.5:3008

/!\ les NodePorts sont assignés dans un range bien dédié et obligatoire compri entre 30000 et 32767

On va créer un objet service comme nos différents objects :


master $ cat service-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: myapp-service

spec:
  type: NodePort  <<< c'est le type qu'on vient de voir;
  ports: 
    - targetPort: 80 <<< c'est le port final qui sera sur notre pod
      port: 80 <<< ici il s'agit du port de notre service vers lequel on enverra les flux vers le pod ( c'est le même port )
      nodeport: 30008 <<<< c'est le port qui sera sur notre node et qui sera le point d'entrée par lequel les appels pourront joindre notre appli
master $ kubectl create -f service-definition.yaml 

Attention les ports sont sous forme de tableau 

On va maintenant devoir mapper vers quel node on doit envoyer nos requettes ..il peut y avoir enormement de nodes .

On va pour cela utiliser les selectors et labels qu'on a utiliser avec la creation de pod.

master $ cat service-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: myapp-service

spec:
  type: NodePort  
  ports: 
    - targetPort: 80 
      port: 80 
      nodeport: 30008 
  selector: 
      app: myapp     <<<< on va ici ajouter les deux params que l'on a créer dans nos pods : afin de bien matcher les bons pods 
      type: frontend 

-commandes : 

kubectl get services

On va donc pouvoir maintenant atteindre notre application depuis notre laptop en utilisant l'ip du node et le nodeport défini :

curl http://192.168.0.5:30008







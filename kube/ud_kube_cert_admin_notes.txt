====     notes cours kube admin certif : ===


== Core concepts : ==

=  architecture générale :
kubernetes est un systeme qui va gérer le de containers qui vont embarquer des applications. Il va entre autre permettre le déploiment de containers.

- les workers nodes : sont les serveurs qui vont héberger les containers contenant les applications.
- le master  est le cerveau qui va gerer les containers, les identifier, sur les workers, les monitorer etc ...: plan; schedule, monitor les nodes 

- etcd : est le composant qui va enregister la configuration sous forme clé valeur de nos containers, config etc ...
- kube-scheduler : est le composant du master qui va permettre de gérer le bon dispatch des containers en fonctions des ressourcers nécéssaires et différentes config inhérantes au cluster.
- le controller manager du master est le composant qui s'assure que tous les evenements sont corrects et que la situation actuelle est bien celle qui est défini dans les confs : que la flotte de container est cohérente  : il existe plusieurs sous composants du controller manager ( chaqun dédié à un périmetre précis. ex :
 node controller : va s'assurer de la bonne santé des containers, s'assurer qu'ils répondent bien ...
 replication controller : va s'assurer que le nombre de container défini et nécéssaire est bien correctement lancé 
 .... il ya toute une liste de controller qui seront etudier précisement.
- kube-api : est le composant central de kube qui va gérer l'orchestration de tous les composants : les composants internes au master, les communications des nodes vers le master et les communication de l'exterieur du cluster vers les composants. C'est le point d'entrée global.
- container runtime : toutes les applications vont être hébergées sous forme de container sur nos nodes. Il est possible d'avoir les composants du master sous forme de container aussi , ex dns etc ..donc pour faire tourner des containers il va falloir avoir le runtime dédié. 
Dans le cas le plus courant à l'heure actuelle il s'agit de docker.
-kubelet : ce composant est un agent qui est hébergé sur les nodes et qui va communiquer en permanence avec le kube-api server afin de recevoir les confs et tous les ordres a appliquer sur les containers.
- kube-proxy est le composant qui va permettre aux differentes applications hébergées sur les nodes de communiquer entre elles ( ex un serveur web d'un container hébergé sur un node voulant communiquer avec une database hébergée sur un autre node... )

= etcd : 

- presentation : 
etcd def : "etcd is a distributed  reliable key value store that is simple, secure and fast"
un systeme clé valeur permet de stocker les infos sous forme de document (contrairement aux bdd transactionnelles qui stockent en tables)
chaque document (comportant donc des couples clés valeurs) peuventt être modifiés indépendamment (contrairement aux tables habituelles dans lesquelles un ajout de colonnes ou de valeurs affectent toutes les données des tables)
Nous n'avons pas a updater les autres documents quand nous ajoutons une info dans un document précis.
Le format est assez simple : yaml ou json 

- install etcd : 
sudo apt install etcd 
ou recupérer le binaire sur github

Le serveur etcd écoute sur le port 2379 

boogie@satellite:~/Documents/stuff/kube$ ps fauxw |grep etcd
boogie    5102  0.0  0.0  17480   892 pts/1    S+   19:11   0:00              |           \_ grep --color=auto etcd
etcd      4984  0.9  0.2 11548340 17532 ?      Ssl  19:10   0:00 /usr/bin/etcd
boogie@satellite:~/Documents/stuff/kube$ ss -atln |grep 2379
LISTEN   0         128               127.0.0.1:2379             0.0.0.0:*    

- ecrire des données en base : 

on peut utiliser le client etcdctl (fourni dans le paquet pour injecter /retrouver des data ) :
boogie@satellite:~/Documents/stuff/kube$ etcdctl set key1 value1
value1
boogie@satellite:~/Documents/stuff/kube$ etcdctl set key2  boogie
boogie
boogie@satellite:~/Documents/stuff/kube$ etcdctl set bingo  bongo
bongo

- lire les données en base : 
boogie@satellite:~/Documents/stuff/kube$ etcdctl get key1
value1
boogie@satellite:~/Documents/stuff/kube$ etcdctl get key2
boogie
boogie@satellite:~/Documents/stuff/kube$ etcdctl get bingo
bongo

le detail des options est valable avec etcdctl --help

- Etcd -  role dans kubernetes : 

le role d'etcd dans le cluster kube est de stocker les valeurs des nodes, pods, configs, secrets, accounts, roles, bindings ....
chaque modification dans notre cluster est enregistrée dans le cluster etcd 

Nous pouvons déployer notre cluster etcd de plusieurs méthodes : ex : from scratch , via kubeadm ...il est important de comprendre les différences.
 
- from scratch :
on va downlader le binaire ou installer via notre distro 
definir un fichier de service (si ce n'est fait automatiquement via le package)
boogie@satellite:~/Documents/stuff/kube$ cat /lib/systemd/system/etcd.service
[Unit]
Description=etcd - highly-available key value store
Documentation=https://github.com/coreos/etcd
Documentation=man:etcd
After=network.target
Wants=network-online.target

[Service]
Environment=DAEMON_ARGS=
Environment=ETCD_NAME=%H
Environment=ETCD_DATA_DIR=/var/lib/etcd/default
EnvironmentFile=-/etc/default/%p
Type=notify
User=etcd
PermissionsStartOnly=true
#ExecStart=/bin/sh -c "GOMAXPROCS=$(nproc) /usr/bin/etcd $DAEMON_ARGS"
ExecStart=/usr/bin/etcd $DAEMON_ARGS
Restart=on-abnormal
#RestartSec=10s
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
Alias=etcd2.service

toute une partie de configuration n'est pas présente de base et est fondamentale pour notre serveur kubernetes : la gestion de certif , les config cluster etc ...
notamment la ligne : --advertise-clients-urls https://${INTERNAL_IP}::2379 

- kubeadm :
en deployant le cluster kube via kubeadm celui ci cree un cluster etcd sous forme de container que l'on peut voir avec :
kubectl get pods -n kube-system 
..
kube-system etcd-master

- pour examiner les clés enregistrées dans le cluster etcd on peut faire simplement :

kubectl exec etcd-master -n kube-system etcdctl get / --prefix -keys-only 

kubernetes stocke ses datas dans une arbo précise de sa registry : "/" 
sous "/" on a les differentes constructions : repertoires : minions, pods,nodes, replicasets ...

dans un cluster kube haute dispo on a plusieurs master kube ayant chacun des server etcd master : il faut s'assurer dans ce cas que chaque serveur etcd puisse communiquer avec les autres en adaptant la conf dédiée ( examen de la conf en pratique plus tard. )
--initial-cluster controller-0=https://${CONTROLLER0_IP}:2380,controller-1=https://${CONTROLLER1_IP}:2380

--> Chap2-video4 done

= kube apiserver : 

c'est le composant principal de managment dans kubernetes.
quand on utilise kubectl pour consulter par exemple les données stockées dans etcd , kubectl  va contacter le kube apiserver qui va authentifier la requette ( examiner les droits du user qui se connecte ) , puis interroger etcd qui retourne les valeurs a kube apiserver qui les renvoi au client ayant initié la demande.
on peut aussi directement communiquer avec l'api sans utiliser kubectl .

ex : grande etapes pour créer un pod : 
curl -X POST /api/v1/namespaces/default/pods ...[other] 
kube apiserver va :
1- authentifier le user 
2- valider la requette
3- recupérer les data 
4- mettre a jour etcd
5 -informer le user que le pod est crée
ensuite 
6- le scheduler qui observe en permanence le kube api server va realiser qu'il y a un nouveau pod non assigné sur un node.
7- le scheduleur identifie grace aux specificités du pods et des ressources des nodes un node qui va heberger le nouveau pod
8 - il informe le kube apiserver qu'un node a un nouveau pod attribué
9 - kube apiserver met a jour la base etcd avec les infos du node ébergeant le pod
10 - kube apiserver va ensuite envoyer l'information à l'agent kubelet présent sur le node qui va héberger le nouveau pod. 
11 - le kubelet va creer le pod et donner l'instruction au runtime de container de deployer l'image 
12 - kubelet informe ensuite kube apiserver que les opérations sont en place
13 - kubeapiserver update la base etcd

Ce type d'opérations est appliqué a chaque fois que des modifications sont a appliquer sur le cluster. Le kube apiserver est au centre de toutes les communications.

L'installation de kube apiserver peut se faire aisement via kubeadm mais il est important de connaitre les différents éléments de configuration présents pour une installation manuelle.
Il est fondamental de prendre en compte que tous les composants du cluster doivent pouvoir communiquer et que des certificats ssl sont à mettre en place . Le kube apiserver est le seul élement a communiquer directement avec etcd server . Sa presence dans la confi de apiserver n'est donc pas surprenante. 
Nous pouvons examiner la conf de kubeapiserver en downloadant le binaire sur le site de kube.
En fonction du mode d'installation ( kubadm/ scratch )on peut voir la conf dans :

/etc/kubernetes/manifeest/kube-apiserver.yaml
ou en service 
/etc/systemd/system/kube-apiserver.service

ch2 v5 -> done

= kube controller manager : 

il manage différents controllers dans kube.
Le role essentiel est de controller en permanence l'etat des ressources et remédier à un pb quand il y en a. puisque son role est s'assurer que l'etat de fonctionnement est totalement equivalent a la configuration et l'etat désiré du system.

- node controller : va s'occuper de monitorer les nodes et s'assurer que les applications fonctionnent correctement sur les nodes .Toutes les actions transitent par le kube apiserver comme on l'a vu.
le node controller recupere l'etat des nodes toutes les 5 secondes.
Si le node controler recoit une alerte de heartbeat d'un node : il flaggue celui ci comme unreachable pendant 40 secondes.
Si le node est toujours ko au bout de 5 minutes , le node controller va ejecter les pods et les héberger sur d'autre nodes si ces pods font partis d'un replicaset.

- replication controller : il est responsable des replicasets et doit s'assurer que le nombre de pods on line définis dans les confs de replicasets est bien corrects
si un pod crash alors le replication controller en recrée un.

Il y a a beaucoup de controller au sein de kube et du controller manager :
deployement controller, namespace controller , job controller , service account controller , endpoints controller ......

Tous ces controllers sont embarqués dans l'installation du pacquet de kube controller manager
si on download le paquet depuis le site de kube et qu'on lance le service : on peut examiner la conf quand on regarde le kube-controller-manager.service 
On peut configurer les conf de heartbeat, check etc ..dans cette config , de meme on peut définir l'aaplication des services controller manager que l'on veut activer dans cette section.

pour examiner le kube-controller-manager : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
la conf est comme d'habitude dans ce cas dans :
/etc/kubernetes/manifests/kube-controller-manager.yaml

sinon on peut examiner la conf dans le service gérer par systemctl.

on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v6 -> done.

= kube-scheduler : 

il est responsable de la planification des pods sur les nodes.Il va gérer le dispatch des pods sur les nodes. Quel pod va sur quel nodes en fonction des besoins du pods et des ressources des nodes.
Le sheduler examine la conf des pods et va s'assurer que le node qui va acceuillir ce pod en fonction de cette conf est correctment sizer en terme de ressource et de config specifique par exemple.
exemple : si notre pod a besoin de 10 cpu et qu'on est fasse a 4 nodes : deux nodes ont 4 cpu dispos , 1 a 12 cpu , le dernier a 16 cpu.
Les deux premiers nodes sont filtrés par le sheduler : pas assez de ressources.
Pour les deux autres nodes : le sheduler va faire un classement en donnant des scores de 0 a 10 aux nodes .
le scheduller va calculer le nombre de ressource dispo apres le set up de notre pod
ici : 2 et 6 . Le dernier node a donc une meilleur note.

L'installation du process se fait comme d'habitude en downloadant le binaire et l'exxecutant .

pour examiner le kube-scheduler : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
la conf est comme d'habitude dans ce cas dans :
/etc/kubernetes/manifests/kube-controller-manager.yaml

sinon on peut examiner la conf dans le service gérer par systemctl.

on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v7 -> done

= kubelet : 

Ce process est hébergé sur les nodes .c'est le process qui va être responsable de toutes les interractions avec le master.
iL va s'occuper de charger / décharger les containers.
Le kubelet va enregister le nodes, declencher la creation du pod via le runtime container et il va monitorer les pods 

kubeadm ne deploit pas automatiquement le conf du kubelet.
On doit toujours faire les installations manuelles des kubelet sur nos nodes.
on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v8 -> done

= kube-proxy : 

dans un cluster kube chaque pod peut contacter tous les autres pods.Ceci est possible grace a un reseau dédié pour les pods. Il y a beaucoup de solution dispos pour gérer la conf reseau des pods.

On peut avoir un pod qui heberge un server web ( 10.1.1.1 ) et un pod qui heberge une db ( 10.1.1.2)
le web peut contacter la db via son adresse ip de pod ...mais on est jamais sur que le pod ne va pas changer d'ip ou que le pod ne va pas crasher.
Il est plus sage de créer un service qui va expposer notre db dans le cluster : service db : 10.96.0.12 
On va donc contacter ce service qui va forwarder le traffic vers le backend. ( notre db ) , le service doit etre accessible depuis tous les nodes.
Le service est virtuel ..qui n'existe que dans la memoire de notre cluster kube.

kube-proxy est un process qui tourne sur tous les nodes du cluster .Son role est de surveiller toutes les nouvelles creations de services et d'ecrire les regles qui vont forwarder le service vers le backend correspondant .
Ceci est fait par des regles iptables.
on va avoir une regle iptable sur tous les nodes qui va forwarder le traffic du service db ( 10.96.0.12 ) vers le pod de backend ( 10.1.1.2 )

L'installation se fait comme d'habitude.

pour examiner le kube-proxy : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
Il est present sur tous les nodes en tant que daemonset :
kubectl get daemonset -n kube-system 

v2 ch9 -> done

= pods : 

on s'assure que le pod existe : un cluster existe, une image docker existe et kube peut la puller depuis une registry

un pod est l'objet le plus petit qu'on peut créer dans kube.
le plus petit est donc un container , d'une application contenu dans un pod.
En cas de charge ..on ne va pas creer un nouveau container de notre appli dans le pod ..on va créer un nouveau pod 
Si on est en cas de grosse charge on va créer des nouveaux pods sur un nouveau node.

On peut biensur avoir plusieurs container dans notre pod mais ils doivent chacun héberger une appli différente.
On est obliger de creer des pods dans kube ...meme si on a une tres simple appli ..mais c'est tres bien car on peut modifier nos archi sans pb

Pour lancer un pod :
on  va invoquer kubectl qui va demarrer un container nginx en downloadant l'image depuis la registry configurée ( docker hub ou une registry privée) :
kubectl run nginx --image nginx 
on va pouvoir lister les pods présents avec :
kubectl get pods 

on va pouvoir definir un objet pod puis creer le pod qui sera ici de cette conf 

cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels: 
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container 
      image: nginx

on instancie la creation de l'objet avec : 
kubectl create -f myapp-pod.yaml

si on modifie un param dans notre objet on pourra reappliquer le changement avec :
kubectl apply -f myapp-pod.yaml

v2 ch10 -> done

= kubernetes controllers : Replication controller : 

les controllers sont les cerveaux. Ce sont les process qui monitorent les objets kube et s'assurent qu'ils répondent correctement.

- replicat : 
on va assurer le principe de HA high availibility en assurant que si un pod crash le service est rendu au user. 
Même si on a un seul pod , le replication controller va nous aider a assurer le fonctionnement de notre appli.
Il s'assure que le nombre de pod défini dans la conf est toujours équivalent a ce qu'il y a dans le cluster kube.
Le replication controller va naturellement nous aider à assurer les montée en charge en créant les pods nécéssaires pour assurer le bon maintien de l'appli.
et ce même sur différents nodes.

On a deux termes qui ont le même sens :
replication controller 
replica set 

mais attention ils sont diférents.
replication controller  est l'ancienne techno qui a été remplacée par le replicat set. 
! Le replica set est à privilégier !

v11 no lesson 

- replication controller :

on va utiliser une structure qui sera présente dans toutes les fichiers de définition  de nos objets kube :
la premiere partie de notre definitoin est assez similaire a celle de la creation d'un pod 
la seconde partie est cruciale et c'est elle qui va gérer les specificité de notre objet ( section spec: )
vi rc-definition.yaml 

apiVersion: v1 ( -> cela va dependre de l'objet que nous creons: pour un replication controller c'est l'api v1 qui supporte cet objet.)
kind: ReplicationController (->  ici on renseigne le type d'objet que l'on veut creer : ici ReplicationController)
metadata:  on va rajouter ici différentes information specifique 
  name: myapp-rc
  labels: 
    app: my-app
    type: front-end
spec: on sait ici que dans l'object replication controller la definition va gérer la creation de plusieurs instances de pods
  - template: ( -> on va creer un template de pod qui sera utiliser par le replication controller pour gérer nos réplicas pour cela on va juste inserer la definition d'un pod au préalable créer par exemple : on va imbriquer les définitions d'objects. On reprend l'integralite de notre defintion d'objet pod SAUF la section apiVersion et kind ))
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas : 3 ( -> c'est dans cette section (qui doit se situer au même niveau que le spec du replication-controller que l'on doit setté le nombre de pod que l'on veut afin d'assurer la HA de notre appli. )
    
nous avons donc deux objets imbriqués avec deux sections metadata et specs.
Attention à l'indentation comme dans tout fichier yaml on doit bien s'assurer de la structure de notre fichier 
vi rc-definition.yaml 

apiVersion: v1 
kind: ReplicationController 
metadata:  
  name: myapp-rc
  labels: 
    app: my-app
    type: front-end
spec: 
  - template: 
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas: 3 

on va ensuite creer notre replication-controller :

kubectl create -f rc-definition.yaml

on va pouvoir examiner les replication-controller :
kubectl get replicationcontroller

NAME   DESIRED  CURRENT READY AGE
my-app       3        3     3  5s
on voit donc les différents status : etat attendu, l'etat actuel et le status du rc

kubectl get pods va nous montrer les trois pods differents crées via notre rc.

- replica set :

vi replicaset-definition.yaml 

apiVersion: apps/v1   --> ici on est obligé ded saisir apps/v1 pour l'apiversion : sous peine d'avoir une erreur de kube
kind: ReplicaSet      --> on renseigne ici le replicatset comme type 
metadata:  
  name: myapp-replicaset  
  labels: 
    app: my-app
    type: front-end
spec: 
  - template: 
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas: 3 
selector:         --> on est obligé ici de définir le pod qui va être gérer par le replicaset. 
  matchLabels: 
    type: front-end  --> le matchlabels va reprendre le type defini dans notre définition de pod.
    
Ceci est nécéssaire car le replicaset peut gérer d'autre pods que ceux inclus dans la definition même du replicaset. C'est la difference majeure entre les replication controller et les replicaset.
on voit qu'on est obliger de setter une mention matchLabels qui va recupérer les  infos settées dans notre section labels précédente.

kubectl create -f replicatset-definition.yaml

on va pouvoir examiner les replication-controller :
kubectl get replicaset

NAME   DESIRED  CURRENT READY AGE
my-app       3        3     3  5s
on voit donc les différents status : etat attendu, l'etat actuel et le status du rc

kubectl get pods va nous montrer les trois pods differents crées via notre rc.


- Labels et selectors : 

on a vu que le replicaset /controller s'occupe de gérer les pods crées et s'assure qu'ils sont up and running.
ce sont donc des process qui monitorent les pods.
Comment le replicat set connait les pods qu'il doit surveiller ..il peut y avoir des centaines de pods faisant tourner plusieurs applications.
C'est la que donner un label à notre pod lors de sa creation peut être vraiment utile..
on peut donc maintenant donner ce label comme filtre pour notre réplicatset.
Le replicaset sait donc maintenant quel pods il doit monitorer.
Le replicaset monitore l'etat des pods crées et s'assure que le nombre en prod en cohérent avec la définition.


on retrouve les concepts de labels dans diffrents objets gérés dans kubernetes.


Attention si on doit gérer un replicatset de pods déja crées ..il faut s'assurer que le bon template est charger dans notre definition. Le rc ne créera pas de nouveau pod si ceux existant avant sa creation comporte les bonnes informations.

- scaling :

il peut nous arriver de definit un replicat a 3 puis avoir une montée en charge et definir a 6 notre réplicat .
On peut agir de plusieurs manieres :

- update le champ replicat dans notre définition : le passer de 3 à 6.
ensuite on met a jour notre rc :

kubectl replace -f replicaset-definition.yaml

- on peut utiliser la commande scale en donnant nos arguments :

kubectl scale --replicas=6 -f replicaset-definition.yaml

on peut utiliser cette commande en utilisant le type name format : 
kubectl scale --replicas=6  replicaset myapp-replicaset

Attention dans ces cas le fichier de definition ne sera pas updater de 3 à 6 réplicats : les pods seront créer mais la conf inchangée.

cmds quick recap : 

kubectl create -f def.yaml
kubectl get replicaset
kubectl delete replicaset myapp-replicaset
kubectl replace -f def.yaml
kubectl scale --replicas=6 -f def.yaml 
kubectl edit replicasets new-replica-set

v2 ch12 ->done


-> tp replicatset -> done

kubectl create -f /root/replicaset-definition-1.yaml
master $ kubectl delete replicasets replicaset-1 replicaset-2
kubectl edit replicasets new-replica-set
master $ kubectl describe replicasets new-replica-set
Name:         new-replica-set
Namespace:    default
Selector:     name=busybox-pod
Labels:       name=busybox-pod
Annotations:  <none>
Replicas:     4 current / 4 desired
Pods Status:  0 Running / 4 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  name=busybox-pod
  Containers:
   busybox-container:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      echo Hello Kubernetes! && sleep 3600
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-b4dzk
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-5hs7t
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-w6pcw
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-vtfv9
  Normal  SuccessfulCreate  16m   replicaset-controller  Created pod: new-replica-set-j2pkm
  Normal  SuccessfulCreate  15m   replicaset-controller  Created pod: new-replica-set-9sx4x

= deployment : 

Le deployment  est un objet kube qui va permettre le deploiement / mise à jour, rollback d'application sans interruption de service pour le client : seamless deployment / rolling update ( mise à jour séquentielle des pods ) sans interruption de service.
On va pouvoir mettre a jour une nouvelle version d'une appli dispo dans le docker hub pod par pod , sans devoir relancer tous les pods comme cela se passerai avec le replicaset.
on va pouvoir rolbacker sur la version précedente de notre appli, on va pouvoir tester le comportement de notre nouvelle appli sur un pod avant de déployer sur tout notre parc ..toutes ses possibilités sont offertes par l'objet deployment.

Tous les containers sont encapsulés dans des pods qui sont deployés par des replicaset ou replication controller : le deployment encapsule tout ces objets dans la hierarchie kube.

La définition d'un deployment se fait comme pour les replicasets :


vi deployment-definition.yaml

apiVersion: apps/v1   --> ici on est obligé ded saisir apps/v1 pour l'apiversion : comme pour le replicaset.
kind: Deployment      --> on renseigne ici le deployment  comme type
metadata:
  name: myapp-deployment
  labels:
    app: my-app
    type: front-end
spec:
  - template:
     metadata:
       name: myapp-pod
       labels:
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container
           image: nginx
replicas: 3
selector:        
  matchLabels:
    type: front-end 


On retrouve le template de pod et le nombre de replicas désiré.

kubectl create -f deployment-definition.yaml
on verifie la creation : 
kubectl get deployments
le deployment crée automatiquement le replicaset 
kubectl get replicaset
et les pods sont aussi crées automatiquement :
kubectl get pods

on peut voir la création globale de nos objets avec :

kubectl get all 

ch2 v13 -> done


- tp deployment -> done

ex deployment :

master $ kubectl describe deployments
Name:                   frontend-deployment
Namespace:              default
CreationTimestamp:      Wed, 01 May 2019 17:57:33 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision=1
Selector:               name=busybox-pod
Replicas:               4 desired | 4 updated | 4 total | 0 available | 4 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=busybox-pod
  Containers:
   busybox-container:
    Image:      busybox888
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      echo Hello Kubernetes! && sleep 3600
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   frontend-deployment-b9b7c6cb6 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set frontend-deployment-b9b7c6cb6 to 4



creation de deploiement :
avec comme nom httpd-frontend  / replicas 3 et image : httpd:2.4-alpine
master $ cat my-deploy.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      name: httpd-pod
  template:
    metadata:
      labels:
        name: httpd-pod
    spec:
      containers:
      - name: httpd-container
        image: httpd:2.4-alpine
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600



= Namespaces : 

Les namespaces vont permettre de cloisonner nos resssources dans des espaces complétements distincts et hermétique. on pourra interragir diectement dans un namespace si il est accessible depuis notre namespace  actuel . 
On peut interroger cependant des objets appartenant a différents namespaces.

De base tous les objets que l'on crée et avec lesquels on interréagis sont dans le "default" namespace de kube.
Kube pour ses besoins internes crées des objets, pods services , dns, network etc ... dans un namespace privé afin de le protéger des users : c'est le namespace appellé kube-system

Un troisieme namespace est crée automatiquement c'est le kube-public : dans lequel on devrait avoir tous les objets qu'on l'on veut rendre accessible, dispo pour les users.

Bien evidemment pour des tests on peut continuer a travailler dans le namespace par default mais on doit impérativement créer des namespace dans les environmments devant important et en production.

on va pouvoir utiliser le même cluster pour différents env ( dev, preprod )mais on va isoler leur ressources avec des namespaces .

On va pouvoir attribuer des quotats par namespaces ( nombre de pods, cpu , memoire ) : afin d'assurer la garantie de ne pas dépasser des ressources physiques .


On va pouvoir au sein d'un namespace appellé directement un service :
ex : dans le namespace default :

- un web-pod
- un web-deployment
- un service-db

pour se connecter a la db on pourra simplement faire :

ex : mysql_connect("db.service")

Si on doit atteindre un service dans un autre namespace ex dev on devra alors utiliser un nom complet créer et diffusé dans le service dns a la création du service par kub lui même :
ex : mysql_connect("db.service.dev.svc.cluster.local") 

on retrouve le nom du service , le namespace , le type d'objet : svc ici et le domain kube natif cluster.local

- lister les pods d'un autre namespace :

kubectl get pods --namespace="notre_namespace"

- creer un pod dans un namespace particulier : 
kubectl create -f pod-definition.yaml --namespace="my_namespace"

On peut rajouter directement le namespace concerné par notre pod a la creation de celui ci en rajoutant :


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod

metadata:
  name: dev
  namespace: my-namespace  <<<< on ajoute ici le namespace de notre object.
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

- creation de namespace :

on peut ecrire un objet namespace :

cat myapp-namespace.yaml
---
apiVersion: v1
kind: Namespace

metadata:
  name: dev

puis :
kubectl create namespace -f myapp-namespace.yaml

on peut creer a la volée le namespace aussi :

kubectl create namespace  dev 

- Switching de namespace : 

on va pouvoir pour plus de facilité quand on travaille longtemps sur un namespace le définir par défaut : de maniere a ne pas devoir systematiqueme ntutiliser le flag : --namespace

kubectl config set-context $(kubectl config current-context) --namespace=dev 

- listing de tous les pods de tous les namespaces : 

pour voir tous les pods de tous les namespaces :

kubectl get pods --all-namespaces


= Quota :
on va pouvoir definir des quota de ressources pour nos namespaces :


cat myapp-quota.yaml
---
apiVersion: v1
kind: ResourceQuota

metadata:
  name: compute-quota
  namespace: dev 
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limit.cpu: "10"
    limit.memory: 10Gi


kubectl create  -f myapp-quota.yaml

ch2 v14 -> done

tp namespace ->  done

- creation d'un pod pour le namespace finance : 
master $ cat fin-pod.yaml
apiVersion: v1
kind: Pod

metadata:
  name: redis

metadata:
  name: redis
  namespace: finance
  labels:
    app: redis
    type: cache
spec:
  containers:
    - name: redis-container
      image: redis
master $ kubectl create -f fin-pod.yaml --namespace=finance
pod/redis created


= services : 
Les services vont nous permettre la communication interne et externe à notre cluster avec notre application.
On va donc pouvoir connecter nos applications et nos users entre eux.
par exemple : les services vont permettre à nos users de communiquer avec nos front-end, nos frontend avec nos backends et nos backends avec les db.

ex : 

un user sur un laptop en 192.168.0.2 
un node sur le même reseau que le user : 192.168.0.5
un pod sur le node en 10.244.0.2 qui héberge un service web en 80
un reseau pour nos pods en 10.244.0.0 

1/ de base : 
le user ne ping pas le pod
2/ en se connectant en ssh sur le node le user peut communiquer avec le pod : curl http://10.244.0.2 > ok
biensur on est donc dans ce cas dans le cluster kube ....ce n'est pas vraiment ce que l'on veut.

on veut pouvoir atteindre l'appli de notre pod via notre laptop: c'est la qu'interviennent les services.
Le role du service est d'ecouter un port sur le node et de forwarder les connexions sur ce port au port de notre application sur le pod : c'es tce qu'on appelle un NodePort service.

Il y a plusieurs type de service :

NodePort c'est le cas le plus simple que nous venons de décrire brièvement.
ClusterIp : qui sera un service virtuel crée au sein du cluster qui permettra la commnunication entre différents services ( ex: frontend avec backend via ce service )
LoadBalancer : qui va servir à repartir la charge entre nos différents pods hébergeant une appli frontend par exemple.

- NodePort : 
dans notre cas le laptop accedera a l'appli en appellant l'ip pod suivi d'un port .Cette connexion sera forwardée par le service sur le pod qui hébergera l'application.

sur le pod
on a le "target port" du pod qui sert l'application -> 10.244.0.2:80
sur le service 
on a le port 80  qui sera le même que celui de notre appli , le service a une ClusterIp qui permettra de recevoir les flux du client passant par le node puis envoyé ce flux vers le pod et le service final : ex: 10.106.1.12:80 
sur le node :
on a un port qui permettra d'acceder a ce node et au service  : c'est le NodePort ex: 192.169.0.5:3008

/!\ les NodePorts sont assignés dans un range bien dédié et obligatoire compri entre 30000 et 32767

On va créer un objet service comme nos différents objects :


master $ cat service-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: myapp-service

spec:
  type: NodePort  <<< c'est le type qu'on vient de voir;
  ports: 
    - targetPort: 80 <<< c'est le port final qui sera sur notre pod
      port: 80 <<< ici il s'agit du port de notre service vers lequel on enverra les flux vers le pod ( c'est le même port )
      nodeport: 30008 <<<< c'est le port qui sera sur notre node et qui sera le point d'entrée par lequel les appels pourront joindre notre appli
master $ kubectl create -f service-definition.yaml 

Attention les ports sont sous forme de tableau 

On va maintenant devoir mapper vers quel node on doit envoyer nos requettes ..il peut y avoir enormement de nodes .

On va pour cela utiliser les selectors et labels qu'on a utiliser avec la creation de pod.

master $ cat service-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: myapp-service

spec:
  type: NodePort  
  ports: 
    - targetPort: 80 
      port: 80 
      nodeport: 30008 
  selector: 
      app: myapp     <<<< on va ici ajouter les deux params que l'on a créer dans nos pods : afin de bien matcher les bons pods 
      type: frontend 

-commandes : 

kubectl get services

On va donc pouvoir maintenant atteindre notre application depuis notre laptop en utilisant l'ip du node et le nodeport défini :

curl http://192.168.0.5:30008


Nous avons vu ici l'utilisation d'un service permettant l'acces a une app hébergée sur un pod.

Biensur il est possible sur un node 'avoir une multitude de pod hébergeant la même application pour assurer la high availibility et le load balancing .Comment faire ? 

Tous ces pods vont avoir le même label ex : myapp  à la creation du service on va utiliser en selector la même valeur que le label : on aura donc potentiellement un service qui redirigera vers une multitude de pods ayant tous une ip dédié.
Le service  selectionnera donc tous ces pods comme endpoint pour forward les connexions des users.
Le loadbalancing est fait automatiquement entre tous les pods via l'algorithme "random" 
Le service  héberge donc nativement un loadbalancer en build in pour distribuer la charge sur les pods.
Aucune actions de configuration supplémentaire à faire de notre coté.


Il est egalement possible d'avoir plusieurs nodes hébergeant les pods de notre appplication. Comment permettre l'acces via le service ? 
Kubernetes va automatiquement créer un service qui va distribuer le traffic en asssignant le traget port sur les pods de nos nodes.
ex: si on a 3 nodes :
192.168.0.1
192.168.0.2
192.168.0.3

on pourra acceder naturellement à l'appli ecoutant sur le port 80 de nos pods hébergés sur chaque nodes en interrogant n'importe qu'ell ip suivi du NodePort défini :

curl http://192.168.0.1:30008
curl http://192.168.0.2:30008
curl http://192.168.0.3:30008

Donc on voit l'extreme fluidité du service qui est automatiquement opérationnel :
pour une app hébergée sur un pod d'un node
pour une app hébergée sur plusieurs pods d'un node
pour une app hébergée sur plusieurs pods de plusieurs nodes

ch2 v15 -> done

- ClusterIP :

Dans une appli classique on peut avoir des app de frontend qui doivent communiquées avec des app de backend qui doivent communiquer avec des db 
Comme on le sait chaque pod a une ip mais celle ci n'est pas a prendre en compte puisque potentiellement volatile.

Un service kube va permettre de grouper différentes ressource de pods afin de les rendre accessibles aux autres composants du cluster.
Chaque service  une ip et un nom associé :on pourra donc scale notre cluster sans souci.
Ce nom sera utilisé par les autres pods pour attenidre notre service :

ex les pods frontend appelleront le service backend . les pods du service backend appelleront le service db : derriere chaque service on retrouve les pods qui recevront de manière random les requetes des clients.

master $ cat service-clusterip-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: backend

spec:
  type: ClusterIp  <<< c'est le type qu'on vient de voir;
  ports:
    - targetPort: 80 <<< c'est le port final qui sera sur notre pod
      port: 80 <<< ici il s'agit du port de notre service vers lequel on enverra les flux vers le pod ( c'est le même port )
      
      selector: 
        app: myapp   <<<< on va linker notre service aux label des pods hebergeant le service .
        type: backend         
master $ kubectl create -f service-definition.yaml

on peut controller la creation de notre service avec 
kubectl get service


chap2 v16 -> done

- 
tp services ->  done.

master $ kubectl get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   6m    <<<< ce service est crée par default au lancement e kube.

master $ kubectl describe service
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP:                10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.17.0.79:6443
Session Affinity:  None
Events:            <none>


- creation de service de type NodePort pour atteindre depuis le browser via le port 30080 l'appli simple-webapp qui ecoute sur le port 8080
master $ cat service-definition-1.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 8080
      port: 8080
      nodePort: 30080
  selector:
    name: simple-webapp


== chapitre 3 : Scheduling : 

ch3 v2

= Fonctionnement du scheduling :

sur chaque definition de pod un champ : nodeName n'est pas à remplir quand on crée le pod mais  Kube renseigne se champ automatiquement à la creation
Le scheduleur va examiner toutes les definitions de pods et quand il voit le champ nodeName vide : il va alors examiner la conf et déterminer sur quel node il va falloir créer ce pod . 
Le nom du node va donc etre associer au nodeName de la definition de notre pod 
ex : 
nodeName : node2
S'il n'y a pas de sheduleur pour gérer les nouveaux pods ceux restent toujours en etat de pending : dans l'attente de l'assignation a un node.

Dans ce cas nous avons plusieurs solutions : 

= Manual scheduling : 

on va pouvoir assigner manuellement un pod a un node.
dans ce cas nous avons juste à renseigner la section nodeName dans la création de notre pod

ex : 


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
    - Ports: 
        containerPort : 8080

nodeName : node2

Par contre Kube NE nous laissera pas modifier ce champ si l'objet existe déja.
Dans ce cas nous allons devoir créer un objet qui referencera notre pod et ira donc sur le node que nous désirons : ce nouvel object est de type Binding.

cat pod-bind-definition.yaml

apiVersion: v1
kind: Binding

metadata:
  name: nginx
target: 
  apiVersion: v1
  kind: Node
  name: node2

on va ensuite envoyer en requete POST a l'api de binding notre définition d'objet :
on va convertir notre yaml en json : 
curl -H "Content-type: application/json" -X POST -D '{"apiVersion: v1, kind: Binding ......}' https://server/api/v1/namespaces/default/pods/$podname/binding

tp manual shedule -> done 

master $ cat nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginxmaster
master $ kubectl create -f nginx.yaml
pod/nginx created
master $ kubectl get pods
NAME      READY     STATUS    RESTARTS   AGE
nginx     0/1       Pending   0          3s

master $ kubectl get pods --all-namespaces
NAMESPACE     NAME                             READY     STATUS    RESTARTS   AGE
default       nginx                            0/1       Pending   0          1m
kube-system   coredns-78fcdf6894-7zbqs         1/1       Running   0          6m
kube-system   coredns-78fcdf6894-c9zll         1/1       Running   0          6m
kube-system   etcd-master                      1/1       Running   0          5m
kube-system   kube-apiserver-master            1/1       Running   0          5m
kube-system   kube-controller-manager-master   1/1       Running   0          5m
kube-system   kube-proxy-fdlnz                 1/1       Running   0          6m
kube-system   kube-proxy-wgccj                 1/1       Running   0          6m
kube-system   weave-net-9mhgm                  2/2       Running   1          6m
kube-system   weave-net-wqfwn                  2/2       Running   1          6m

on voit qu'il n'y a pas de scheduler 
on va detruire le pod puis le recreer en inserant dans la defintion le nodeName que l'on veut : 

cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01



master $ vi nginx.yaml
master $ kubectl create -f nginx.yaml
pod/nginx created
master $ kubectl get pod
NAME      READY     STATUS    RESTARTS   AGE
nginx     1/1       Running   0          11s
master $ kubectl describe pod nginx
Name:               nginx
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               node01/172.17.0.73
Start Time:         Sun, 05 May 2019 17:57:28 +0000
Labels:             <none>
Annotations:        <none>
Status:             Running
IP:                 10.32.0.2
Containers:
  nginx:
    Container ID:   docker://e990f90c76b0712ddfcba7632c047b3158e24c2e2b23de406b593b8cfd925234
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:e71b1bf4281f25533cf15e6e5f9be4dac74d2328152edf7ecde23abc54e16c1c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Sun, 05 May 2019 17:57:37 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-89rxh (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-89rxh:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-89rxh
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason   Age   From             Message
  ----    ------   ----  ----             -------
  Normal  Pulling  22s   kubelet, node01  pulling image "nginx"
  Normal  Pulled   15s   kubelet, node01  Successfully pulled image "nginx"
  Normal  Created  14s   kubelet, node01  Created container
  Normal  Started  14s   kubelet, node01  Started container


= Labels et selectors : 

 Les labels et les selectors sont des methodes pour grouper les choses entre elles.
 On veut etre capable de filtrer en fonctions de group mais également en fonction de certains critères.
 ex : tous les animaux verts
 tous les animaux de type oiseaux verts
 Les labels sont alors ideaux . Les labels sont des propriétés attachees a des items 
ex : 
class : mammal
kind : domestic 
color : green

class : reptile
kind : wild 
color : purple

on trouve les labels dans plein de contexte web : sur les blogs, dans les boutiques on line ...
  
- utilisation des labels et selectors dans kube : 

on crée plein de differents objects : pods, deployment, replicaset, service ....
avec le temps on peu donc avoir une multitude d'objet dans notre cluster.
on doit donc etre capable de voir nos différents objects par catégories.
on peut grouper les objects par leur type ( pods ..) , leur application ( my-app1 ), leut fonctionnalité ( frontend )
chaque object attache les labels selon nos besoins.

On va donc créer dansla section metadata les labels que l'on veut sans limitation de nombre :

cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
    function: web 
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01


Le selector va utiliser des labels pour filtrer nos objects : 
selectors :
  app: app1
  function: front-end 

le selector sera ici app=app1 par exemple.

Un fois l'object créer on va pourvoir selectionner en fonction de critère passés en arguments au selector :


kubectl get pods --selector app=app1 

kube utilise les labels et selectors en interne pour grouper les ressources.

ex dans un replicaset on va definir les pods utilisés via leur labels 

Attention on a dans ce cas 2 labels :

le premier en debut de definition concerne le replicaset lui meme alors que le second defini dans la section du template concerne les pods.  --> Il faut bien faire attention c'est une erreur classique.
Dans notre cas actuellement les labels de replicaset  ne sont pas utilisés : nous verrons plus tard que ceci peut être utile pour la decouverte auto de replicaset.

Afin de connecter le replicaset au pod : on va definir dans la conf de notre replicaset un matchingLabels qui va comporter une entrée qui va correspondre a une definition de label dans la section de notre pod :


definition de replicaset a revoir pour l'exemple de correspondance matchlabels repliscaset et label pod sur entrée app: app1

kind: ReplicaSet      
metadata:
  name: simple-webapp
  labels:
    app: app1
    type: front-end
spec:
  replicas: 3
  selector:        
    matchLabels:
      type: app1
  - template:
     metadata:
       name: myapp-pod
       labels:
         app: app1
         type: front-end
     spec:
       containers:
         - name: simple-webapp
           image: simple-webapp


C'est le même principe pour les differents objects kube

ex : pour les services :
quand un service est créer il va utiliser  l'entrée defini dans le selector pour matcher avec l'entree du label du pod defini dans le replicaset.

On peut rajouter une autre caracteristique en plus des labels et selectors : les annotations .
Cela va anous permettre de donner des infos supplementaires dans la description de nos pods ( numero de version , adresse mail ...)


kind: ReplicaSet      
metadata:
  name: simple-webapp
  labels:
    app: app1
    type: front-end
  annotations:
    buildversion: 1.34

ch 3 v3 -> done


ch3 tp labels /selector -> done 

aster $ kubectl get pods --selector env=dev
NAME          READY     STATUS    RESTARTS   AGE
app-1-g9k8r   1/1       Running   0          1m
app-1-t7qcs   1/1       Running   0          1m
app-1-zhbdm   1/1       Running   0          1m
db-1-2vgkv    1/1       Running   0          1m
db-1-7678l    1/1       Running   0          1m
db-1-fj5rm    1/1       Running   0          1m
db-1-z6hbq    1/1       Running   0          1m
master $ kubectl get pods --selector bu=finance
NAME          READY     STATUS    RESTARTS   AGE
app-1-g9k8r   1/1       Running   0          1m
app-1-t7qcs   1/1       Running   0          1m
app-1-zhbdm   1/1       Running   0          1m
app-1-zzxdf   1/1       Running   0          1m
auth          1/1       Running   0          1m
db-2-sdnz9    1/1       Running   0          1m
master $ kubectl get all --selector env=prod
NAME              READY     STATUS    RESTARTS   AGE
pod/app-1-zzxdf   1/1       Running   0          2m
pod/app-2-cfnth   1/1       Running   0          2m
pod/auth          1/1       Running   0          2m
pod/db-2-sdnz9    1/1       Running   0          2m

kubectl get all --selector env=prod,bu=finance,tier=frontend
NAME              READY     STATUS    RESTARTS   AGE
pod/app-1-zzxdf   1/1       Running   0          3m

cat replicaset-definition-1.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend   <<<< il faut faire matcher le label de notre replicatset avec celui des pods
  template:
    metadata:
      labels:
        tier: frontend   <<<< label des pods
    spec:
      containers:
      - name: nginx
        image: nginx



= Ressources limits : 

comme on l'a vu le scheduleur va s'occuper de la creation et la repartition des pods sur les différents nodes, en fonction des ressources nécéssaires pour le pod et des ressources dispos sur les nodes.

Si un pod demande des ressources non dispos alors il reste en status pending et on a une info spécifique au pb : ..insufficiant cpu  ....etc .

kube de base considère qu'un container  de base a besoin de :

0.5 cpu 
256M ram 
Si de base on sait que notre appli va consommer plus , il suffit de déclarer dans la définition les valeurs nécéssaires.


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
    function: web 
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01

  ressources: 
    requests:
      memory: "1Gb"
      cpu: 1

- cpu :
L'unite 1 cpu correspond a :
1 vcpu
1 thread
on peut mettre 100m et decendre jusqu'a 0.1m qui veut dire milli.
on n'est pas obligé de mettre des multiples de 0.5 ou 100m 

- memory :
on peut mettre les valeurs que l'on veut avec le suffixes qu'on veut mais  il s'agit de Mi Gi ayant pour base 1k = 1024bytes 
contrairement a 1K = 1000bytes pour Mo et Go

Dans le monde de docker ..il n'y a pas de limite et le container peut consumer les ressources qu'il veut sur le node.
De base kube fixe des limites pour les containers afin de preserver les nodes :

1vcpu par container
512mi de ram 

On va pouvoir definir des limits à nos container dans la definition de notre pod 

le container ne pourra pas depasser le cpu defini mais il pourra depasser la memoire jusqu'a crasher ....


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
    function: web 
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01

  ressources:                    <<<<< definition d'une section de resources: on defini le besoin de notre pod 
    requests:
      memory: "1Gi"
      cpu: 1
    limits:                     <<<<<< definition des limites de notre container : limites fixées manuellement pour contenir les eventuels debordements du container
      memory: "2Gi"
      cpu: 2


ch 3 v4 -> done


tp ressources -> done 

creation d'un pod avec ressources cpu : 


Attention à l'indentation -> le yaml ici est correct.

master $ cat lion.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: lion
spec:
  containers:
  - name: lion
    image: vish/stress
    resources:
       requests:
        cpu: 2
      

cat elephant.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: elephant
spec:
  containers:
  - name: elephant
    image: polinux/stress
    resources:
       limits:
        memory: "20Mi"
       requests:
        memory: "15Mi"
        cpu: 2

= Daemon sets : 

Le daemonset va permettre comme le replicaset de s'assurer du deploiement correct des pods définis.
A la différence du replicaset , le daemonset va déployer un pod apres sa creation  sur CHAQUE node du cluster.
Il s'assure qu'une copie de chaque pods et toujours presentes sur tous les nodes du cluster.
Chaque nouveau node va donc recevoir les pods définis dans notre daemonset.

Un exemple parfait d'utilisation est l'ajout de monitoring et de logger pour le cluster.
On va deployer ses agents sous formes de pod et déployer sur chaque node du cluster via un daemonset pour s'assurer du bon fonctionnement de notre cluster.
Dans ce cas on n'a pas besoin de s'occuper de retirer / rajouter des agents de log ou monito sur un node qui sort du cluster ou un nouveau node qui rentre dans le cluster.
Un second exemple d'utilisation du daemonset est le kube-proxy qui va être deployer sous forme de pod sur chaque noeud de notre cluster en daemonset.
Une solution de networking comme weave nécéssite le deployment de conf sur chaque node : le daemonset est la encore conseillé.



La definition du daemonset est identique au replicaset :
on a les pods du daemonset definis et imbriquer dans la section template, on a un selector défini pour matcher ces pods.

cat daemonset-definition.yaml 
apiVersion: apps/v1  
kind: DeaemonSet      
metadata:
  name: monitoring-daemon
spec:
  selector:         
    matchLabels:
      app: monitoring-agent
  - template:
      metadata:
        labels:
          app: monitoring-agent
      spec:
        containers:
          - name: monitoring-agent
            image: monitoring-agent

on doit s'assurer que le matchLabels de notre selector correspond au label defini dans le template de nos pods.            


kubectl create -f daemonset-definition.yaml

kubectl get daemonsets

kubectl describe daemonsets our_daemonset_name

Fonctionnement :

on peut definir manuellement comme on l'a vu l'affectation de pod à un node  

-> c'etait comme cela avant kubernetes v1.12

Depuis le daemonsets utilise le scheduleur par defaut et les nodes affinity.


ch 3 v5 -> done


tp daemonsets -> done 

master $ kubectl get daemonsets --all-namespaces
NAMESPACE     NAME         DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                   AGE
kube-system   kube-proxy   2         2         2         2            2           beta.kubernetes.io/arch=amd64   3m
kube-system   weave-net    2         2         2         2            2           <none>

master $ kubectl describe daemonset kube-proxy --namespace=kube-system
Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  beta.kubernetes.io/arch=amd64
Labels:         k8s-app=kube-proxy
Annotations:    <none>
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy-amd64:v1.11.3
    Port:       <none>
    Host Port:  <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
    Environment:  <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  5m    daemonset-controller  Created pod: kube-proxy-q2csm
  Normal  SuccessfulCreate  5m    daemonset-controller  Created pod: kube-proxy-dssqz

creation d'un daemonset dont on défini le nom "elasticsearch" , le namespace dans le quel il evolue : kube-system et avec une image : k8s.gcr.io/elasticsearch:1.20

master $ cat fluend-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
     metadata:
       labels:
         app: elasticsearch
     spec:
       containers:
         - name: elasticsearch
           image: k8s.gcr.io/elasticsearch:1.20



= multiple schedulers : =

On a vu que le sheduling va dependre de plusieurs criteres que l'on peut avoir défini : ( node affinity , ressources ..).
Il est possible d'avoir des besoins complexes avec par exemple une appli qui devra avoir ses pods placés sur des nodes après des tests additionnels (??)
Que ce passe il quand aucune des specificité ne sont disponibles ?

On va pouvoir définir notre propre sheduleur.
Dans ce cas les applications normales utiliseront le scheduleur par defaut et notre application scpecifique elle sera gérée par notre scheduleur personel.: 
le cluster kube peut utiliser en même temps plusieurs scheduleurs.

Quand on crée un pod ou un deployement on va pouvoir definir le scheduleur qui va être utilisé.
On peut downloader le binaire du sheduleur sur le site de kube.
Si on ne precise rien : se sera un sheduleur par defaut qui se lancera  comme un service (linux)

  --sheduleur-name=default
on peut déployer un scheduleur perso en utilisant le même bianaire mais en specifiant un nom :

 ... 
  --sheduleur-name=my-own-scheduler

Il va être nécéssaire de bien identifié le nom du scheduleur

ex : kubeadm deploi le scheduleur sous forme de pod ( pas comme un service ) 
on peut s'inspirer de la conf de kubeadm pour créer la definition de notre scheduler 

il faudra biensur s'assurer de bien renseigner son nom et aussi de rajouter la section : --leader-elect: true
qui sert a s'assurer qu'un seul master est le sheduler dans le cas d'un cluster multi master ( HA )

Un seul scheduleur doit être actif sur un master : il faut faire une election de leader.
ex : 
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: custom-scheduler
    namespace: kube-system
spec:
  containers:
    - command:
    - /usr/local/bin/kube-scheduler
    - --address=127.0.0.1
    - --leader-elect=true
    - --scheduler-name=custom-scheduler
    image: g1g1/custom-kube-scheduler:1.0
    name: kube-second-scheduler

On va pouvoir examiner une fois que l'on lance notre pods qu'il tourne bien . Une fois que notre sheduleur est up , on va definir les pods qui vont l'utiliser.
dans la définition du pod on va definir le scheduleur crée au prealable: 
cat elephant.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: elephant
spec:
  containers:
  - name: elephant
    image: polinux/stress
    resources:
       limits:
        memory: "20Mi"
       requests:
        memory: "15Mi"
        cpu: 2
  schedulerName: custom-scheduler    <<< on defini explicitement le scheduleur qui va être utilisé pour notre pod.

  On crée notre popd et on le lance : il doit être en runnning 


- Comment identifier quel sheduler est  utilisé ?

on va utiliser la gestion des events : 

kubectl get events : va lister les evenement s dans le namespace courant.
on peut voir dans notre cas apres la creation du pods qu'il y a une section SOURCE qui indique les scheduleurs en charge de l'event : ici la creation du pod est bien dépendante de notre scheduleur personalisé.


on va egalement pouvoir examiner les logs :

kubectl logs custom-scheduler --namespace=kube-system

ch3 v6 ->  done 

tp scheduler -> done

creation d'un scheduler :

nom :my-sheduler 


master $ cat /var/answers/my-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  creationTimestamp: null
  labels:
    component: my-scheduler
    tier: control-plane
  name: my-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=false
    - --port=10282
    - --scheduler-name=my-scheduler
    image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10282
        scheme: HTTP
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
status: {}


creation d'un pod rattaché au sheduler crée :

master $ cat nginx-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginx
  schedulerName: my-scheduler


Pour résumer :
on peut installer / gérer un scheduleur :

-> en service linux
-> en pod comme kubeadm

references supplémentaires pour scheduling .

ch3 v7 -> en cours

== Logging /monitoring kube cluster : ==


= monitoring : 

-> nodes level
-> pods level 
etc ...

des solutions existent pour monitorer le cluster kube : prometheus, elasticstac , datadog ..
Une solution native existait mais est maintenant déprecated : heapster 

on va pouvoir cependant recupérer les métriques des nodes et des pods , les aggréger et les stocker en mémoire.
On devra cependant utiliser une solution externe pour stocker les data.

Les metrics sont récoltées par un sous composants de kubelet sur les nodes. cet agent se nomme CAdvisor ( container advisor ) : cet agent est responsable de la récupération des métriques et de leur envoi à kubelet qui va les transférer au server de metrique final.

on peut tester sur minikube :

minkube addons enable metrics-server
sinon on peut recupérer le code :

git clone https://github.com/kubernetes-incubator/metrics-server


https://github.com/kodekloudhub/kubernetes-metrics-server.git

kubectl create -f deploy/1.8+/

kubectl top node
kubectl top pod

tp monitoring -> done

= logging : 

comme pour docker on va pouvoir examiner les logs de nos pods :

kubectl logs my_pod
kubectl logs -f my_pod   <<<< -f pour voir les logs défiler en continu.

Si on a plusieurs container dans notre pod on va devoir nommer explicitement le container dont on veut voir les logs .
ex: 

kubectl logs -f my_pod my_container1_name 

tp logging -> done


== Application life cycle managment : ==


= rolling update and rollback : =

- rollout :

quand on crée un deployment , cela trigger un nouveau rollout.
Ce rollout crée un nouveau num de revision pour le deploiement.
ex : 
on cree un deploiement pour des images nginx:1.7.0 , cela trigger un rollout qui va générer un num de revision pour le deploiement: ex :revision 1
Dans le futur si on upgrade une nouvelle version de container avec des images updatées nginx:1.7.1 : on aura apres le deploiement un rollout qui créera un num de recision : ex : revision 2
Cela permet de garder un historique des déploiements et de faciliter le rollback si besoin.

On peut voir le status de notre rollout avec :
kubectl rollout status deployment/my-app-deploy

on peut voir l'historique de nos déployments avec :
kubectl rollout history deployment/my-app-deploy

- strategy de deployement :

il y a deux types de stratégie de déploiement :

- recreate :
dans cette strategie : on va detruire tous les pods de notre deploiement puis les recreer avec la nouvelle version.
Biensur : il y a une interruption de service pendant ce temps.

- rolling-update :
dans cette deuxieme strategie , on va detruire et recréer tous les pods un par un :
si on a un deployement de 4 pods nginx :
on detruit le premier pod puis on créer le premier pod nouvelle version, on fait pareil avec tous les autres pods.
Dans ce cas il n'y a aucune coupure de service et le deploiement est seamless.
Cette stratégie est celle utilisée par default dans kubernetes.
Si on ne precise rien dans notre deployement : cette stratégie sera utilisée par defaut.


update : on peut avoir beaucoup de parametres qu'on peut updaté : num d'application, numbre de replicat, images docker etc ...

on peut renseigner les changements dans notre fichier de déploiement et mettre a jour avec :
kubectl apply -f definition-deployment.yaml

on peut editer notre deployment : il sera pris à chaud 

on peut faire un update a chaud :
ex: 
kubectl set image deployment/my-app-deploy nginx=nginx:1.9.1 
Attention on aura donc une différence avec le fichier de déploiment qui aura la version de l'image inchangée.

On peut voir le detail du deploiment avec :

kubectl describe deployment my-app-deploy 
on peut bien voir le champ :
StrategyType: Recreate ou RollingUpdate 

on peut voir les différences de deploiement :
dans un cas tous est detruit puis recréer , dans le second les operations se font une a une pour chaque pod.

upgrade : 
fonctionnement interne :
dans le cas d'un nouveau deployment :
le deployment va crée un replicat set avec le nombre de pod necessaire
dans le cas d'une modif de deployment :
un nouveau replicatset est créer et le premier pod du premier replicaset est detruit, le premier pod du nouveau replicat set est crée etc ..

on peut voir le  resultat en consultant :
kubectl get replicasets
on voit le premier replicatset avec 0 pod et le second avec le nombre de pod definit dans notre deployement.

- Rollback :
on va pouvoir revenir a une version anterieure :

kubectl rollout undo deployment/my-app-deploy




tp ch5 rolling-update rollback -> done


on examine un deployment , on modifie l'image utilisée ( kubectl edit deployment frontend)

on voit ensuite la modification : on observe le shutdown des anciens pods et l'allumage des nouveaux pods avec nouvelle image qui se fait de maniere incrémentale 


master $ kubectl describe deployments.
Name:                   frontend
Namespace:              default
CreationTimestamp:      Sun, 19 May 2019 18:00:33 +0000
Labels:                 name=webapp
Annotations:            deployment.kubernetes.io/revision=2
Selector:               name=webapp
Replicas:               4 desired | 4 updated | 5 total | 3 available | 2 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        20
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp
  Containers:
   simple-webapp:
    Image:        kodekloud/webapp-color:v2
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  frontend-7965b86db7 (1/1 replicas created)
NewReplicaSet:   frontend-65998dcfd8 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  10m   deployment-controller  Scaled up replica set frontend-7965b86db7 to 4
  Normal  ScalingReplicaSet  25s   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 1
  Normal  ScalingReplicaSet  25s   deployment-controller  Scaled down replica set frontend-7965b86db7 to 3
  Normal  ScalingReplicaSet  25s   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 2
  Normal  ScalingReplicaSet  0s    deployment-controller  Scaled down replica set frontend-7965b86db7 to 1
  Normal  ScalingReplicaSet  0s    deployment-controller  Scaled up replica set frontend-65998dcfd8 to 4
master $ kubectl describe deployments.
Name:                   frontend
Namespace:              default
CreationTimestamp:      Sun, 19 May 2019 18:00:33 +0000
Labels:                 name=webapp
Annotations:            deployment.kubernetes.io/revision=2
Selector:               name=webapp
Replicas:               4 desired | 4 updated | 5 total | 3 available | 2 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        20
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp
  Containers:
   simple-webapp:
    Image:        kodekloud/webapp-color:v2
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  frontend-7965b86db7 (1/1 replicas created)
NewReplicaSet:   frontend-65998dcfd8 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  10m   deployment-controller  Scaled up replica set frontend-7965b86db7 to 4
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 1
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled down replica set frontend-7965b86db7 to 3
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 2
  Normal  ScalingReplicaSet  3s    deployment-controller  Scaled down replica set frontend-7965b86db7 to 1
  Normal  ScalingReplicaSet  3s    deployment-controller  Scaled up replica set frontend-65998dcfd8 to 4

 
-script pour vérifier la version des pods déployés apres un update de deployment : 

master $ cat curl-test.sh
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""

Hello, Application Version: v1 ; Color: blue OK

master $ cat curl-test.sh
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done



en changeant la methode de déployement a recreate on a : 

master $ kubectl describe  deployments.
Name:               frontend
Namespace:          default
CreationTimestamp:  Sun, 19 May 2019 18:00:33 +0000
Labels:             name=webapp
Annotations:        deployment.kubernetes.io/revision=3
Selector:           name=webapp
Replicas:           4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:       Recreate
MinReadySeconds:    20
Pod Template:
  Labels:  name=webapp
  Containers:
   simple-webapp:
    Image:        kodekloud/webapp-color:v3
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   frontend-5c858cd557 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  34m   deployment-controller  Scaled up replica set frontend-7965b86db7 to 4
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 1
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled down replica set frontend-7965b86db7 to 3
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 2
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled down replica set frontend-7965b86db7 to 1
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 4
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set frontend-7965b86db7 to 0
  Normal  ScalingReplicaSet  7m    deployment-controller  Scaled down replica set frontend-65998dcfd8 to 0
  Normal  ScalingReplicaSet  6m    deployment-controller  Scaled up replica set frontend-5c858cd557 to 4


on va pouvoir voir les différents déployements : 

master $ kubectl rollout history deployment/frontend
deployments "frontend"
REVISION  CHANGE-CAUSE
1         <none>
2         <none>
3         <none>

on peut executer un rollback : 
master $ kubectl rollout undo deployment/frontend
deployment.extensions/frontend


= Commands and arguments dans une définition de pod :


- memo sur les commandes et arguments dans docker :

quand on lance un container simplement avec docker :
docker run ubuntu
et qu'on liste les container up 
docker container ls 

il n'y a pas de process actif : le container s'est lancé puis s'est arrêté.
avec un docker ps -a : on peut voir que le container est en status exited.
les containers sont fait pour faire tourner une tache ou un process . contrairement a une vm qui fait tourner un os.
un container est up uniquement tant que le process a l'interrieur de celui ci est up.
ex : si un server web crash alors le container qui l'héberge se ferme.
On peut facilement voir la commande qui est executée dans notre container : en editant le Dockerfile de celui-ci on voit le param :
ex : pour un container nginx :
# default command :
CMD ["nginx"]

ex: pour un container hébergeant mysql :
#default command :
CMD ["mysqld"]

Quand on lance un container faisant tourner ubuntu on peut voir que la commande est :

CMD ["bash"] 
la commande bash n'est pas comme un serveur web.
Bash attend une commande sur un terminal ..s'il ne trouve pas de terminal alors il se ferme.
par defaut docker n'attache pas de terminal a un container quand il est lancé.
donc bash ne trouve pas de terminal ..il se ferme donc 

On peut ajouter une commande qui va overrider  la commande spécifiée dans le dockerfile.

ex :
docker run ubuntu sleep 5 <<<< dans ce cas  la commande sleep sera prise en compte la commande bash sera remplacée et le container sera up pendant 5 secondes.

On peut overrider en ligne de commande mais biensur on peut rendre se changement permanent :

on peut modifier la commande bash et la remplacer par sleep dans le dockerfile.

CMD sleep 5
2 formes sont possibles :
- forme simple :CMD command, param1 -> CMD sleep 5
- format json CMD ["command", "param1"]  -> CMD ["sleep","5"]

attention dans le format json le premier element doit etre un executable et tous les élements doivent etre séparés dans la liste :
CMD ["sleep 5"] --<< ne fonctionne PAS

on peut maintenant builder notre nouvelle image :

docker build -t ubuntu-sleeper .
docker run ubuntu-sleeper

pour augmenter le delai passer de 5 à 10 secondes on peut modifier en cli : docker run ubuntu-sleeper sleep 10 mais comme son nom l'indique ubuntu-sleeper doit sleep ..;donc rajouter la commande en argumant n'est pas génial .

on voudrait juste passer en argument le nbr de secondes pendant lequel le container sleep :
docker run ubuntu-sleeper 10 

pour cela on va utiliser la directive "entrypoint" 
cette instruction ENTRYPOINT est commande CMD on va donc pouvoir juste saisir notre nombre de secondes.
CMD ENTRYPOINT ["sleep"]
docker run ubuntu-sleeper 10 

CMD peut être completement overrider par les parametres passés en cli 
ENTRYPOINT les arguments de la ligne de commande sont ajoutés à l'entrypoint 

si on lance le container avec l'entrypoint sleep sans argumant on a une erreur :
c'est normal car sleep a besoin d'argument.

On va donc pouvoir donner une valeur par default pour se faire on va cumuler les deux directives :


FROM ubuntu
...
...
ENTRYPOINT ["sleep"]
CMD ["5"]
dans se cas si on lance la commande docker run ubuntu-sleeper sans argumant par defaut le container prendra 5 secondes comme valeur
si on passe un argument alors celui-ci ovverridera la valeur contenu dans la directive CMD 

ex : docker run ubuntu-sleeper 10

Il est cependant si besoin possible d'overrider l'entrypoint en cli en utilisant l'argumant --entrypoint 

docker run ubuntu-sleeper --entrypoint sleep.2 5 

ch5 v3 -> done 

Todo tp ch5 -> commandes args

- commands et arguments dans kube :

on a vu qu'on pouvait créer avec docker un container qui se lancerait et pendant 5 secondes ne ferait rien
docker run --name ubuntu-sleeper ubuntu-sleeper
on a vu qu'on pouvait overrider les commandes et arguments pour le lancer 10 secondes : 
docker run --name ubuntu-sleeper ubuntu-sleeper 10

on va pouvoir créer un pod ayant le même comportement : 
et on va pouvoir passer les argumants overridant le comportement par defaut du pod dans la section args de notre container : 
pod-def.yaml

apiVersion: v1 
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
specs:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      args: ["10"]

kubectl create -f pod-def.yaml

 c'est comme si on overridait la partie CMD dans le dockerfile :

 From ubuntu
 ..
 ENTRYPOINT ["sleep"]
 CMD ["5"]

 Comment faire si on veut overrider l'entrypoint ? 
 dans ce cas on va renseigner le champ command dans la def de notre pod: 



pod-def.yaml

apiVersion: v1 
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
specs:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ["sleepv2"]
      args: ["10"]
Attention donc on a 

ENTRYPOINT : docker -> command: kubernetes
CMD: docker         -> args: kubernetes


ch5 v4 -> done

tp ch5 command args

master $ cat ubuntu-sleeper-2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-2
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command: ["sleep"]
    args: ["5000"]
master $


Il est possible de passer plusieurs commandes à la suite sous forme de tableau dans la directive commandes :

master $ cat ubuntu-sleeper-3.yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-3
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "1200"

      
ici on voit que la commande qui sera executé au demarrage du container sera:  "--color","green" : car la valeur déclarée dans le pod va ovverider ce qui est defini dans le dockerfile.

- dockerfile : 
master $ cat /root/webapp-color-2/Dockerfile2
FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]

CMD ["--color", "red"]

-pod : 
master $ cat /root/webapp-color-2/webapp-color-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["--color","green"]


= Variable d'environment dans kube : =

on va pouvoir specifier dans la definition de notre pod des variables.
La section commencera par env qui est lui même suivi de tableau :chaque élément est compris dans le tableau :
chaque item du tableau a un nom et une valeur

apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    ports: 
      - containerport: 8080
    command: ["watch -n2 "]
    env: 
      - name: APP_COLOR
        value: green

On va pouvoir configurer nos variables autrement qu'en associant des clés valeur dans notre pod.
On peut utiliser des configMap et des secrets 

- configMap :

    env: 
      - name: APP_COLOR
        valueFrom:
          configMapKeyRef:


- secret :           

    env: 
      - name: APP_COLOR
        valueFrom:
          secretKeyRef: 

-> ch5 v5 -> done

=  configMap :

on a vu qu'on pouvait setter des variables d'environment dans la defintion de notre pod. 
Quand on a beaucoup de definition de pod cela devient compliquer de gérer les variables au sein de multiples fichiers.

on va donc externaliser les differentes variables au sein d'un fichier pour centraliser nos données.
Ce fichier peut être un configMap 

configMap est un fichier comportant des clés /valeurs.
on va donc dans la definition de notre pod faire matcher les entrées de nos configmap dans les caracteriqtiques de notre pod .

On va donc créer notre configmap puis l'injecter dans notre pod.

ex: 
ConfigMap 

APP_COLOR: blue
APP_MODE: prod

- creation de configmap : 

on va avoir deux methodes pour créere notre configmap :
-> impératif : sans fichier de config :
kubectl create configmap  

-> declaratif : avec un fichier de definition.
kubectl create -f 


- imperatif :

kubectl create configmap  
  <config-name> --from-litteral=<key>=<value>
  app-config --from-litteral=APP_COLOR=blue

l'utilisation de --from-litteral permet de passer directement en cli les clés /valeurs
on peut ajouter autant de clés /valeurs que l'on veut en rajoutant une ligne -from-litteral suivie des clés/ valeurs: 



kubectl create configmap  
  app-config --from-litteral=APP_COLOR=blue
             --from-litteral=APP_MODE=prod

on peut également charger nos données depuis un fichier :

kubectl create configmap  
  app-config --from-file=app_config.properties

- declaratif :

kubectl create -f configmap-def.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:  <<<< a la place de spec que l'on trouve dans les objects kube habituels  ici on a la definition data
  APP_COLOR: blue
  APP_MODE: prod


kubectl create -f configmap-def.yaml

On peut biensur avoir plusieurs configmap :
mysql-config:

port: 3306
max_allowed-packets: 32M

etc ...

pour examiner les configmap :

kubectl get configmaps

kubectl describe  configmaps

- Declaration dans le pod :

on va maintenant definir notre configmap dans le pod :

on va utiliser des declaration specifique:
envFrom:
  configMapRef:
    name: notre_config_map 

pod-def.yaml

apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    ports: 
      - containerport: 8080
    command: ["watch -n2 "]
    envFrom:            <<<<<<<<  on va donc maintenant rattacher les valeurs definis dans notre configmap au pod 
      - configMapRef:
          name: app-config


kubectl create -f pod-def.yaml

ch5 v6 -> done


tp configmap : 

master $ kubectl get configmaps
NAME        DATA      AGE
db-config   3         14s
master $ kubectl describe configmaps db-config
Name:         db-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
DB_PORT:
----
3306
DB_HOST:
----
SQL01.example.com
DB_NAME:
----
SQL01
Events:  <none>
master $


master $ cat webapp-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config-map
data:
  APP_COLOR: darkblue

master $ kubectl create -f webapp-configmap.yaml
configmap/webapp-config-map created
master $ kubectl get configmaps
NAME                DATA      AGE
db-config           3         5m
webapp-config-map   1         5s


on va maintenant accrocher notre configmap a notre pod :

master $ cat webapp-color.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp-color
  labels:
      name: webapp-color
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    envFrom:
      - configMapRef:
          name: webapp-config-map


tp var / configmap -> done

= secrets : =

comme on l'a vu on peut charger des variables depuis un fichier de type configMap 
mais pour les données sensibles (mdp ..) il faut preserver une securité ..

https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

On va donc dans ce cas utiliser un secret qui est un object qui sert a stocker les données sensibles.

Les data seront hachées et invisibles pour les users.
comme pour les configmap on va créer dans un premier temps les secrets puis injecter la conf dans le pod.

ex: 

secret 

DB_HOST: mysql
DB_USER: root
DB_PASSWORD: psswd

- imperatif :

kubectl create secret generic
  <secret_name> --from-litteral=key=value
  app-secret --from-litteral=DB_HOST=mysql
             --from-litteral=DB_USER=root
             --from-litteral=DB_PASSWORD=psswd

biensur cela commence a etre compliqué quand on a beaucoup de secret a passer 
on va pouvoir passer l'option -from-file qui permetrra de gérer un fichier contenant toutes nos valeurs

kubectl create secret generic
  <secret-name> --from-file=<path-to-file>
  app-secret --from-file=fichier_contenant_nos_secrets


- declaratif :

on va biensur pouvoir utiliser un fichier pour créer notre object en mode déclaratif :

secret-data.yaml

apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data: 
  DB_HOST: mysql
  DB_USER: root
  DB_PASSWORD: psswd

kubectl create -f  secret-data.yaml
  
Biensur ici on voit que les données sont en clair se qui n'est biensur pas tolérable.
On va donc devoir présenter les valeurs dans un format hashé :

on pourra par exemple utiliser la base64 :
echo "mysql" |base64
bXlzcWwK
echo "root" |base64
cm9vdAo=
echo "psswd" |base64
cHNzd2QK

On pourra donc facilement injecter ces datas hashé dans la conf de notre object :


apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_HOST: bXlzcWwK
  DB_USER: cm9vdAo=
  DB_PASSWORD: cHNzd2QK

pour lister les secrets :

kubectl get secrets

Kube en crée naturellement pour le fonctionnement interne du cluster.

kubectl describe secrets permet la description mais les data sont cachées 

on peut voir les valeurs des secrets avec :

kubectl get secrets app-secret -o yaml

pour decoder les valeurs hachées on va utiliser la méthode inverse :

echo bXlzcWwK |base64 --decode
mysql
echo cm9vdAo= |base64 --decode
root
echo cHNzd2QK |base64 --decode
psswd

on va maintenant configurer notre secret dans la definition du pod 


pod-def.yaml

apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    ports: 
      - containerport: 8080
    command: ["watch -n2 "]
    envFrom:            <<<<<<<<  on va donc maintenant rattacher les valeurs definis dans notre configmap au pod 
      - secretRef:
        name: app-secret

on va pouvoir créer notre pod et les data seront donc présentes         
kubectl create -f pod-def.yaml


On peut créer des secrets :

-> en env

envFrom:
  - secretRef:
    name: app-secret

-> en single env

env:
  - name: DB_PASSWORD
    valueFrom:
      secretkeyRef:
        - name: app-secret
        key: DB_PASSWORD

-> dans un fichier qui sera monté dans un volume :

volumes:
  - name: volume-app-secret
    secret:
      secretName: app-secret 
      
ch 5 v7 -> done 


"notes : 
Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.
The concept of safety of the Secrets is a bit confusing in Kubernetes. The kubernetes documentation page and a lot of blogs out there refer to secrets as a "safer option" to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. In my opinion it's not the secret itself that is safe, it is the practices around it.
Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:
Not checking-in secret object definition files to source code repositories.
Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD.
Also the way kubernetes handles secrets. Such as:
A secret is only sent to a node if a pod on that node requires it.
Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.
Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.
Read about the protections and risks of using secrets here
Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault."


tp secrets :


master $ kubectl get secrets
NAME                  TYPE                                  DATA      AGE
default-token-mn8z6   kubernetes.io/service-account-token   3         7m

on a 3 secrets dans l'objet secret default-token-mn8z6

master $ kubectl describe secrets default-token-mn8z6
Name:         default-token-mn8z6
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=default
              kubernetes.io/service-account.uid=4d9de3b5-7ece-11e9-99f1-0242ac11004b

Type:  kubernetes.io/service-account-token      <<<<<<<< type de secret utilisé 

Data
====
ca.crt:     1025 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tbW44ejYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjRkOWRlM2I1LTdlY2UtMTFlOS05OWYxLTAyNDJhYzExMDA0YiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.FPP7-jwZsweSyn8tEps3PTJBgIGND4MliO4AwiP_Sj8yl2Xcb6mjO1eCpkD5hz-_taJNIVLvIrSm5W9oyNAzp1DL2_UChcFDb82Y8y-u0NZEFbCOvJTt1hcdOA_Idj4f8iELaCxxJwuuHTGWcIzjv952g7hB_09otZiRUxVcXKWvVQ0KObOvziOvelWRI14tOzosijebtywrUnt_6A41A5-xa1ULXaXfapOKrBzOPe5ntj1hfMzOQkq31WHkaLML5i5gee952klRelCAj69E9N16RMnO3qudC5qVGwpXKQKwUsXMPg4KEguqOPcaL9d3x8L6sye6OVLRK77n04Q4Ew

une appli est déployée : 

master $ kubectl get pods
NAME         READY     STATUS    RESTARTS   AGE
mysql        1/1       Running   0          32s
webapp-pod   1/1       Running   0          33s
master $ kubectl get service
NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes       ClusterIP   10.96.0.1       <none>        443/TCP          12m
sql01            ClusterIP   10.100.100.76   <none>        3306/TCP         36s
webapp-service   NodePort    10.96.171.216   <none>        8080:30080/TCP   36s
master $ kubectl get secrets
NAME                  TYPE                                  DATA      AGE
default-token-mn8z6   kubernetes.io/service-account-token   3         12m


creation d'un secret en litteral : 

master $ kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
secret/db-secret created
master $ kubectl get secrets db-secret -o yaml
apiVersion: v1
data:
  DB_Host: c3FsMDE=
  DB_Password: cGFzc3dvcmQxMjM=
  DB_User: cm9vdA==
kind: Secret
metadata:
  creationTimestamp: 2019-05-25T09:37:22Z
  name: db-secret
  namespace: default
  resourceVersion: "1814"
  selfLink: /api/v1/namespaces/default/secrets/db-secret
  uid: b24325ca-7ed0-11e9-99f1-0242ac11004b
type: Opaque



master $ cat webapp-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp-pod
spec:
  containers:
  - name: webapp-pod
    image: kodekloud/simple-webapp-mysql
    envFrom:
      - secretRef:
          name: db-secret


tp secret -> done          


== Readiness  probe / monitoring concepts : =

on a vu qu'un pod a un status et des conditions.

ex : 
-pending (quand le sheduler va s'occupé du dispatch ..)
- quand le pod est dispatché sur un node on est dans l'etat containercreating 
- quand le container est up le pod passe en running


les conditions competent le pod status 

ce sont des tableaux true /false sur le status des pods :

PodScheduled  -> true / false
Initialized   -> true /false 
ContainerReady -> true /false
Ready          -> true /false

on peut voir les conditions d'un pod avec un :

kubectl describe pod 


Le status ready indique que l'application dans le pod est running 
et prete a accepter du traffic user
l'appli peut etre un simple script, une db, un server web ...

Certaines applications peuvent mettre plusieurs minutes à démarrer ...

ex : quand jenkins démarre c'est tres long 
pourtant même si l'application n'est pas completement démarrée le status du pod peut etre en ready ce qui n'est pas complétement vrai.

Comment kube peut savoir si les applications d'un pod sont bien démarrées ou non ?

des que le container est créer kube pense que l'appli est up.

un pod s'il expose via un service un acces aux users sera donc vu up même si l'appli n'est pas démarrée completement.

On doit donc trouver un moyen de vraiment s'assurer que l'appli est dispo 

on va donc builder des tests :

readiness probes : ex http /api test ; tcp test port 3306 ...

On va pouvoir le faire au sein de notre pod :
on fait un set up de sonde qui ne permettra a kube de déclarer le pod en ready que quand la sonde sera ok : 

pod-def.yaml

apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    ports: 
      - containerport: 8080
    command: ["watch -n2 "]
    envFrom:            
      - secretRef:
        name: app-secret
    readinessProbe:   <<<<<< definition de notre sonde  
      httpGet:
        path: /api/ready
        port: 8080
        

on a plusieurs type de sondes :


    readinessProbe:   <<<<<< check http  
      httpGet: 
        path: /api/ready
        port: 8080
        

    readinessProbe:   <<<< check tcp 
      tcpSocket:
        port: 3306

    readinessProbe:   <<< execution de script 
      exec:
        command: 
          - cat 
          - /app/is_ready

on peut définir d'autres options :

initialDelaySeconds -> nombre de secondes nécéssaire à l'appli pour être up
periodSeconds -> nombre d'attente entre chaque sonde

readinessProbe:   <<<<<< check http  
  httpGet: 
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10 
  periodSeconds: 5

par défault trois tests seront fait pour assurer que l'appli est up et donc la passer en running mais on peut modifier ce nombre :
avec le param failureThreshold qui va permettre d'indique le nombre de test qui seront fait avant que le status ne passe soit en running soit en failure.
  failureThreshold: 8 

Il existe beaucoup de params : consulter la doc.

Les  readiness probes vont être très utiles dans un set up de multipods :

ex : une appli a deux pods exposant un service par lesquels se connectent les users : si on met un troisieme pod et qu'on l'injecte sans test de vie alors le traffic sera envoyer sur tous les pods même celui qui n'est pas pret ..on aura donc des failures coté users ...si on met une sonde readiness alors on pourra correctement mettre en prod le pod quand il sera vraiment pret.


ch5 v8 -> done 

pas de tp sur les sondes : pas requis sur certif kube admin mais requis sur kube dev application certif

== cluster maintenance ==


= operations d'upgrade sur notre cluster ( os, patch secu ...) : =

Si on perd un node ou qu'on fait une maintenance sur un node ...le service peut etre transparent pour les users sir le deploiement implique des multipod ..si on a qu'un seul pod déployé et qu'il  se trouve sur le noeud en erreur / maintenance le service ne sera pas rendu .

si on a juste une tres petite interruption : kubelet va remettre online les pods rapidement.
si la panne dure plus de 5 miniutes alors les pods de ce nodes passsent en status  terminated.
si les pods font partis d'un replicat set alors ils sont recrées sur un nouveau node.
Le delai pour lequel le pod est considéré ko est le timeeviction défini dans le controller-manager :
ex :
kube-controller-manager --pod-eviction-timeout=5m0s ...

pour assurer des maintenances propre on va drain le node :
les pods vont etre détruit du node sur lequel l'intervention va avoir lieu puis ils vont etre recréés sur les autres nodes dispos.

kubectl drain node1

le node va être marqué cordon egalement -> ce qui va empécher d'avoir des pods schedule dessus .
Quand le node revient en prod on va devoir permettre la creation de pods dessus : on va donc le "uncordon" :

kubectl uncordon node1

il est possible de simplement empecher le schedule de nouveaux pods sur le node en inter avec :

kubectl cordon node1 

dans ce cas les pods presents sur ce node ne sont pas éjectés ....

ch6 v2 -> done 

tp maintenance cluster 

on va mettre en maintenance le node01 : 

master $ kubectl get pods -o wide
NAME                   READY     STATUS    RESTARTS   AGE       IP          NODE      NOMINATED NODE
blue-684d969bd-dlr7q   1/1       Running   0          2m        10.38.0.2   node03    <none>
blue-684d969bd-h8qcl   1/1       Running   0          2m        10.44.0.2   node01    <none>
blue-684d969bd-mcfsn   1/1       Running   0          2m        10.32.0.2   node02    <none>
red-67465685c-27ncp    1/1       Running   0          2m        10.44.0.1   node01    <none>
red-67465685c-fjgj4    1/1       Running   0          2m        10.38.0.1   node03    <none>

Il est possible de drain un node tout en ignorant les daemonsets :

master $ kubectl drain node01 --ignore-daemonsets


WARNING: Ignoring DaemonSet-managed pods: kube-proxy-wf675, weave-net-89bh8
pod/blue-684d969bd-h8qcl evicted
pod/red-67465685c-27ncp evicted
master $ kubectl get pods -o wide
NAME                   READY     STATUS    RESTARTS   AGE       IP          NODE      NOMINATED NODE
blue-684d969bd-dlr7q   1/1       Running   0          3m        10.38.0.2   node03    <none>
blue-684d969bd-mcfsn   1/1       Running   0          3m        10.32.0.2   node02    <none>
blue-684d969bd-rc94x   1/1       Running   0          6s        10.32.0.3   node02    <none>
red-67465685c-fjgj4    1/1       Running   0          3m        10.38.0.1   node03    <none>
red-67465685c-w2wrk    1/1       Running   0          6s        10.32.0.4   node02    <none>

une fois que l'operation de maintenance sur le node01 est fait on permet de reschedule des pods dessus : 

master $ kubectl uncordon node01
node/node01 uncordoned


le master ne prend pas de pod car il est marqué :

Taints:             node-role.kubernetes.io/master:NoSchedule  quand on fait un kubectl describe nodes master.

on peut forcer un drain même si des pods ne font pas partis de replicatset ..dans ce cas ces pods sont detruits !

kubectl drain node02 --ignore-daemosents --force


pour forcer un node a ne plus prendre de nouveaux pods : 
master $ kubectl cordon node03
node/node03 cordoned



= kubernetes releases : =

on voit que la version de kube est dispo avec un kubectl get nodes 

ex : v1.11.3

1  -> version majeur
11 -> version mineure  : sortent regulierement avec des améliorations 
3  -> numero de patch  : sorent plus regulierement avec des bugfix ,secu ...

on a aussi comme d'habitude des alphas, betas 

sur le site de kube on peut downlader le package relatif à notre version : tous les composants du backplane sont present et a la meme version 
seuls etcd et coredns ne suivent pas la version car ils sont des projets à part.


exam de liens de version a faire cf slides

ch6 v3 -> done

= kubernetes upgrade version =

cluster upgrade process : on ne parle pas de coredns ni d'etcd 
tous les composants doivent etre à la même version 
aucun composant ne doit avoir de version supérieure à apiserver 

on peut avoir une version en moins pour controller-manager et kube-scheduleur
on peut avoir deux version en moins pour kubelet et kube-proxy 

le kubectl peut avoir une version sup a l'apiserver , la meme version ou une version en moins que l'apiserver

on peut  upgrader composants par composants.

- Quand upgrader ? 

!! Attention kubernetes ne peux pas supporter d'upgrade supérieure à 3 versions !! 

1.12 -- 1.11 -- 1.10 
l'upgrade de 1.9 a 1.12 ne passe pas.

Il est recommandé d'upgrader une version mineure à la fois.

Dans l'examen actuel nous allons étudier l'upgrade avec kubeadm :

1/ on upgrade d'abord le master
2/ les nodes apres.


1/ upgrade de master :

on met a jour le master : pendant ce temps les workers travaillent toujours
les users ne sont pas impactés.

pour les nodes ont a plusieurs stratégies :

- upgrade de tous les nodes :
dans ce cas plus aucuns service n'est dispo pour les users.

- upgrade node par node :

on update un node ..les pods partent sur les nodes dispos du cluster et on fait cela au fur et mesure


- injection d'un nouveau node avec la nouvelle version :
on injecte les pods dessus et on eteind le vieux node ..on poursuit pour tous les noeuds du cluster.


- kubeadm upgrade :

on va utiliser cette methode

- kubeadm upgrade plan 

-> va nous montrer les info de versions presentes et disponibles pour l'upgrade 

on peut voir egalement que l'upgrade de certains composants (kubelet) doit se faire manuellement sur chaque node.
puis on peut faire un kubadm apply num_version quand on est pret 

1- on va upgrade kubeadm :

apt upgrade -y kubeadm=1.14.0-00

2- upgrade 

kubeadm upgrade apply v1.14.0 : l'upgrade se fait naturellement.

quand on fait un kubectl get nodes on voit les num de versions inchangés -> c'est uniquement car c'est la version de kubelet qui est remontées sur les nodes.

en fonction de la maniere dont on a déployer notre cluster on peut avoir kubelet present sur notre master node. : si on a installer avec kubeadm.

on upgrade kubelet :
- sur le master en premier ( si présent ) 
apt upgrade -y kubelet=1.14.0-00

- sur les nodes :
on drain les nodes d'un node puis on upgrade ..on le fait partout
puis on uncordon 
kubeadm upgrade node config  --kubelet-version v1.14.0
systemctl restart kubelet


ch6 v4 -> done 


tp upgrade :

on est en version 1.11.3 on va donc pouvoir upgrader max jusqu'en 1.14 
on ne pourra le faire que de version mineure en mineure 
on va upgrader le master qui héberge aussi des pods :
kubectl drain master --ignore-daemonsets

on update kubeadm avec la version mineure du premier step : 
apt-get update && apt-get upgrade -y kubeadm=1.12.0-00

on lance l'update avec kubeadm : on verifie d'abord le status d'upgrade possible : 

master $ kubeadm upgrade plan
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.11.3
[upgrade/versions] kubeadm version: v1.12.0
[upgrade/versions] Latest stable version: v1.14.2
[upgrade/versions] Latest version in the v1.11 series: v1.11.10
[upgrade/versions] WARNING: No recommended etcd for requested kubernetes version (v1.14.2)

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     1 x v1.11.3   v1.11.10
            1 x v1.11.3   v1.11.10

Upgrade to the latest version in the v1.11 series:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.11.3   v1.11.10
Controller Manager   v1.11.3   v1.11.10
Scheduler            v1.11.3   v1.11.10
Kube Proxy           v1.11.3   v1.11.10
CoreDNS              1.1.3     1.2.2
Etcd                 3.2.18    3.2.18

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.11.10

_____________________________________________________________________

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     1 x v1.11.3   v1.14.2
            1 x v1.12.0   v1.14.2

Upgrade to the latest stable version:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.11.3   v1.14.2
Controller Manager   v1.11.3   v1.14.2
Scheduler            v1.11.3   v1.14.2
Kube Proxy           v1.11.3   v1.14.2
CoreDNS              1.1.3     1.2.2
Etcd                 3.2.18    N/A

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.14.2

Note: Before you can perform this upgrade, you have to update kubeadm to v1.14.2.

_____________________________________________________________________


master $ kubeadm upgrade apply v1.12.0
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[upgrade/apply] Respecting the --cri-socket flag that is set with higher priority than the config file.
[upgrade/version] You have chosen to change the cluster version to "v1.12.0"
[upgrade/versions] Cluster version: v1.11.3
[upgrade/versions] kubeadm version: v1.12.0
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y

....

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.12.0". Enjoy!

on remet le master en prod on permet d'acceuillir des pods :

kubectl uncordon master

on passe au node :

master $ kubectl drain node01 --ignore-daemonsets
node/node01 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-proxy-wcn2g, kube-system/weave-net-58ssj
evicting pod "coredns-576cbf47c7-z48rv"
evicting pod "red-67465685c-j5vcg"
evicting pod "coredns-576cbf47c7-2lzjk"
evicting pod "red-67465685c-8sxwc"
pod/red-67465685c-j5vcg evicted
pod/coredns-576cbf47c7-z48rv evicted
pod/red-67465685c-8sxwc evicted
pod/coredns-576cbf47c7-2lzjk evicted
node/node01 evicted

ssh node01
node01 $ apt-get update && apt-get upgrade -u kubeadm=1.12.0-00

node01 $ kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2)
[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.12" ConfigMap in the kube-system namespace
[kubelet] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[upgrade] The configuration for this node was successfully updated!
[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.


on update kubelet : 
node $ apt install kubelet=1.12.0-00

tp upgrade done 


= Backup and restore = 

que doit on backuper ?

les configurations de nos ressources 

on peut stocker tout nos fichiers de définition d'objects dans un repertoire dédié.
il nous faut donc une copie de sauvegarde de ces fichiers. --> on va habituellement les stocker dans un cvs : git / gitlab /github.

on peut pour s'assurer que toutes les ressources sont bien backupées intérroger l'apiserver et stocker l'intégralité de nos ressources dans un fichier de backup.

kubectl get all --all-namespaces -o yaml > all-deployed-services.yaml

certains outils dédiés existent.


Etcd stocke les états du cluster : les infos du cluster y sont stockées.

On va pouvoir backuper les data etcd 

>> on défini le datadir de etcd : l'endroit ou sont stockées les data.

Un outil de snap est fourni avec etcd :

Attention il faut préfixer avec l'api V3 de etcd

ETCDCTL_API=3 etcdctl snapshot save snapshot.db 
>>> ici on backup dans le repertoire courant nos data dans un fichier appellé snapshot.db
on peut biensur backupé ou on veut en précisant le path :

ETCDCTL_API=3 etcdctl snapshot save /home/boogie/backup/etcd_bck.db


On peut voir l'état de notre snap :

ETCDCTL_API=3 etcdctl snapshot status snapshot.db

Pour restaurer un backup :

1/ stopper le kubeapiserver : car on va devroir arreter le service etcd

service kube-apiserver stop

2/ restauration du snap sur l'intégralité des nodes du cluster etcd :

ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --data-dir /var/lib/etcd-for_backup \
  --initial-cluster etcd01=https://etcd01.boogie.net:2380,etcd02=https://etcd02.boogie.net:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls https://${internal_ip}:2380

!! /!\ Etcd restore les data depuis le backup et instancie une NOUVELLE configuration de cluster 
et configure les membres comme des nouveaux membres d'un nouveau cluster !!

C'est fait par prévention pour empecher un nouveau membre de s'integrer dans un cluster déja existant.
a ce moment la un nouveau datadir est créer dans notre exemple : /var/lib/etcd-from-backup

On va maintenant configurer notre service etcd pour qu'il prenne en compte notre nouveau data dir domain et notre nouveau token 

une fois notre service modifier on reload le service daemon et etcd :

systemctl daemon-reload
service etcd restart 

3/ on démarre le kubeapserver service : 

service kube-apiserver start

Attention pour le tls on doit indiquer a etcd les endpoint; ca; crt et key 


ch6 v5 -> done 

todo tp backup


== security ==


ch7 v6

rappel des fondamentaux sur le chiffrement tls /ssl 


-> CA (certification authorithy) pour signer les certificats
-> certificats servers : ( private key / cert (pub key )
-> clients certificats ( pour authentifier les users se connectant au server )

nomencalture :

clé pub -> *cert / .pem  : quand on a des fichier de type .cert ou pem se sont les clés pub ( ex : server.crt , client.crt ..)
clé prive -> .key / *-key.pem  : quand le nom du fichier contient key ..c'est la clé privée. ( ex: server.key , client.key ..)


Dans kube on a des master et worker dont les communications sont chiffrées.
un admin se connectant au cluster aura une connexion chiffrée et c'est la même chose pour les composants du cluster kube.
on a donc des certificats server et des certificats client 

- server : 
-> kube api : server ( on génere : apiserver.key / apiserver.cert ) : composant principal : les users et les composants s'y connectent
on essaye de nommer les fichiers clairement 

-> etcd cluster : server ( on genere : etcdserver.key /etcdserver.cert ) : stocke les infos du cluster.

-> kubelet : server ( on genere kubelet.key / kubelet.cert) : ce server est présent sur tous les nodes 

- clients:

-> admin : client ( on genere admin.key /admin.crt ) : on s'authentifie pour se connecter a l'apiserver  avec kubectl 
-> scheduler : client ( on genere scheduler.key /scheduler.cert) : le scheduler s'autentifie a l'apiserver 
-> controller-manager : client ( on genere controllermanager.key / controllermanager.cert) : le controller manager s'authentifie apures de l'apiserver
-> kube-proxy: client  ( on genere kube-proxy.key /kube-proxy.cert) le kube-proxy s'authentifie aupres de l'api server


Les servers communiquent aussi entre eux :

-> apiserver se connecte a etcd 
l'apiserver peut utiliser la meme paire de clé qu'il utilise pour servir ses clients. On peut en générer une paire de clé dédiée si on veut.
-> apiserver se connecte a kubelet : on peut utiliser la meme paire de clé ou en générer une nouvelle pour que l'apiserver s'authentifie a kubelet.

on va devoir avoir 2 CA :
-> une pour l'api  : ca.key / ca.cert
-> une pour etcd  etcd-ca.key / etcd-ca.cert


















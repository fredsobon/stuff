====     notes cours kube admin certif : ===


== Core concepts : ==

=  architecture générale :
kubernetes est un systeme qui va gérer le de containers qui vont embarquer des applications. Il va entre autre permettre le déploiment de containers.

- les workers nodes : sont les serveurs qui vont héberger les containers contenant les applications.
- le master  est le cerveau qui va gerer les containers, les identifier, sur les workers, les monitorer etc ...: plan; schedule, monitor les nodes 

- etcd : est le composant qui va enregister la configuration sous forme clé valeur de nos containers, config etc ...
- kube-scheduler : est le composant du master qui va permettre de gérer le bon dispatch des containers en fonctions des ressourcers nécéssaires et différentes config inhérantes au cluster.
- le controller manager du master est le composant qui s'assure que tous les evenements sont corrects et que la situation actuelle est bien celle qui est défini dans les confs : que la flotte de container est cohérente  : il existe plusieurs sous composants du controller manager ( chaqun dédié à un périmetre précis. ex :
 node controller : va s'assurer de la bonne santé des containers, s'assurer qu'ils répondent bien ...
 replication controller : va s'assurer que le nombre de container défini et nécéssaire est bien correctement lancé 
 .... il ya toute une liste de controller qui seront etudier précisement.
- kube-api : est le composant central de kube qui va gérer l'orchestration de tous les composants : les composants internes au master, les communications des nodes vers le master et les communication de l'exterieur du cluster vers les composants. C'est le point d'entrée global.
- container runtime : toutes les applications vont être hébergées sous forme de container sur nos nodes. Il est possible d'avoir les composants du master sous forme de container aussi , ex dns etc ..donc pour faire tourner des containers il va falloir avoir le runtime dédié. 
Dans le cas le plus courant à l'heure actuelle il s'agit de docker.
-kubelet : ce composant est un agent qui est hébergé sur les nodes et qui va communiquer en permanence avec le kube-api server afin de recevoir les confs et tous les ordres a appliquer sur les containers.
- kube-proxy est le composant qui va permettre aux differentes applications hébergées sur les nodes de communiquer entre elles ( ex un serveur web d'un container hébergé sur un node voulant communiquer avec une database hébergée sur un autre node... )

= etcd : 

- presentation : 
etcd def : "etcd is a distributed  reliable key value store that is simple, secure and fast"
un systeme clé valeur permet de stocker les infos sous forme de document (contrairement aux bdd transactionnelles qui stockent en tables)
chaque document (comportant donc des couples clés valeurs) peuventt être modifiés indépendamment (contrairement aux tables habituelles dans lesquelles un ajout de colonnes ou de valeurs affectent toutes les données des tables)
Nous n'avons pas a updater les autres documents quand nous ajoutons une info dans un document précis.
Le format est assez simple : yaml ou json 

- install etcd : 
sudo apt install etcd 
ou recupérer le binaire sur github

Le serveur etcd écoute sur le port 2379 

boogie@satellite:~/Documents/stuff/kube$ ps fauxw |grep etcd
boogie    5102  0.0  0.0  17480   892 pts/1    S+   19:11   0:00              |           \_ grep --color=auto etcd
etcd      4984  0.9  0.2 11548340 17532 ?      Ssl  19:10   0:00 /usr/bin/etcd
boogie@satellite:~/Documents/stuff/kube$ ss -atln |grep 2379
LISTEN   0         128               127.0.0.1:2379             0.0.0.0:*    

- ecrire des données en base : 

on peut utiliser le client etcdctl (fourni dans le paquet pour injecter /retrouver des data ) :
boogie@satellite:~/Documents/stuff/kube$ etcdctl set key1 value1
value1
boogie@satellite:~/Documents/stuff/kube$ etcdctl set key2  boogie
boogie
boogie@satellite:~/Documents/stuff/kube$ etcdctl set bingo  bongo
bongo

- lire les données en base : 
boogie@satellite:~/Documents/stuff/kube$ etcdctl get key1
value1
boogie@satellite:~/Documents/stuff/kube$ etcdctl get key2
boogie
boogie@satellite:~/Documents/stuff/kube$ etcdctl get bingo
bongo

le detail des options est valable avec etcdctl --help

- Etcd -  role dans kubernetes : 

le role d'etcd dans le cluster kube est de stocker les valeurs des nodes, pods, configs, secrets, accounts, roles, bindings ....
chaque modification dans notre cluster est enregistrée dans le cluster etcd 

Nous pouvons déployer notre cluster etcd de plusieurs méthodes : ex : from scratch , via kubeadm ...il est important de comprendre les différences.
 
- from scratch :
on va downlader le binaire ou installer via notre distro 
definir un fichier de service (si ce n'est fait automatiquement via le package)
boogie@satellite:~/Documents/stuff/kube$ cat /lib/systemd/system/etcd.service
[Unit]
Description=etcd - highly-available key value store
Documentation=https://github.com/coreos/etcd
Documentation=man:etcd
After=network.target
Wants=network-online.target

[Service]
Environment=DAEMON_ARGS=
Environment=ETCD_NAME=%H
Environment=ETCD_DATA_DIR=/var/lib/etcd/default
EnvironmentFile=-/etc/default/%p
Type=notify
User=etcd
PermissionsStartOnly=true
#ExecStart=/bin/sh -c "GOMAXPROCS=$(nproc) /usr/bin/etcd $DAEMON_ARGS"
ExecStart=/usr/bin/etcd $DAEMON_ARGS
Restart=on-abnormal
#RestartSec=10s
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
Alias=etcd2.service

toute une partie de configuration n'est pas présente de base et est fondamentale pour notre serveur kubernetes : la gestion de certif , les config cluster etc ...
notamment la ligne : --advertise-clients-urls https://${INTERNAL_IP}::2379 

- kubeadm :
en deployant le cluster kube via kubeadm celui ci cree un cluster etcd sous forme de container que l'on peut voir avec :
kubectl get pods -n kube-system 
..
kube-system etcd-master

- pour examiner les clés enregistrées dans le cluster etcd on peut faire simplement :

kubectl exec etcd-master -n kube-system etcdctl get / --prefix -keys-only 

kubernetes stocke ses datas dans une arbo précise de sa registry : "/" 
sous "/" on a les differentes constructions : repertoires : minions, pods,nodes, replicasets ...

dans un cluster kube haute dispo on a plusieurs master kube ayant chacun des server etcd master : il faut s'assurer dans ce cas que chaque serveur etcd puisse communiquer avec les autres en adaptant la conf dédiée ( examen de la conf en pratique plus tard. )
--initial-cluster controller-0=https://${CONTROLLER0_IP}:2380,controller-1=https://${CONTROLLER1_IP}:2380

--> Chap2-video4 done

= kube apiserver : 

c'est le composant principal de managment dans kubernetes.
quand on utilise kubectl pour consulter par exemple les données stockées dans etcd , kubectl  va contacter le kube apiserver qui va authentifier la requette ( examiner les droits du user qui se connecte ) , puis interroger etcd qui retourne les valeurs a kube apiserver qui les renvoi au client ayant initié la demande.
on peut aussi directement communiquer avec l'api sans utiliser kubectl .

ex : grande etapes pour créer un pod : 
curl -X POST /api/v1/namespaces/default/pods ...[other] 
kube apiserver va :
1- authentifier le user 
2- valider la requette
3- recupérer les data 
4- mettre a jour etcd
5 -informer le user que le pod est crée
ensuite 
6- le scheduler qui observe en permanence le kube api server va realiser qu'il y a un nouveau pod non assigné sur un node.
7- le scheduleur identifie grace aux specificités du pods et des ressources des nodes un node qui va heberger le nouveau pod
8 - il informe le kube apiserver qu'un node a un nouveau pod attribué
9 - kube apiserver met a jour la base etcd avec les infos du node ébergeant le pod
10 - kube apiserver va ensuite envoyer l'information à l'agent kubelet présent sur le node qui va héberger le nouveau pod. 
11 - le kubelet va creer le pod et donner l'instruction au runtime de container de deployer l'image 
12 - kubelet informe ensuite kube apiserver que les opérations sont en place
13 - kubeapiserver update la base etcd

Ce type d'opérations est appliqué a chaque fois que des modifications sont a appliquer sur le cluster. Le kube apiserver est au centre de toutes les communications.

L'installation de kube apiserver peut se faire aisement via kubeadm mais il est important de connaitre les différents éléments de configuration présents pour une installation manuelle.
Il est fondamental de prendre en compte que tous les composants du cluster doivent pouvoir communiquer et que des certificats ssl sont à mettre en place . Le kube apiserver est le seul élement a communiquer directement avec etcd server . Sa presence dans la confi de apiserver n'est donc pas surprenante. 
Nous pouvons examiner la conf de kubeapiserver en downloadant le binaire sur le site de kube.
En fonction du mode d'installation ( kubadm/ scratch )on peut voir la conf dans :

/etc/kubernetes/manifeest/kube-apiserver.yaml
ou en service 
/etc/systemd/system/kube-apiserver.service

ch2 v5 -> done

= kube controller manager : 

il manage différents controllers dans kube.
Le role essentiel est de controller en permanence l'etat des ressources et remédier à un pb quand il y en a. puisque son role est s'assurer que l'etat de fonctionnement est totalement equivalent a la configuration et l'etat désiré du system.

- node controller : va s'occuper de monitorer les nodes et s'assurer que les applications fonctionnent correctement sur les nodes .Toutes les actions transitent par le kube apiserver comme on l'a vu.
le node controller recupere l'etat des nodes toutes les 5 secondes.
Si le node controler recoit une alerte de heartbeat d'un node : il flaggue celui ci comme unreachable pendant 40 secondes.
Si le node est toujours ko au bout de 5 minutes , le node controller va ejecter les pods et les héberger sur d'autre nodes si ces pods font partis d'un replicaset.

- replication controller : il est responsable des replicasets et doit s'assurer que le nombre de pods on line définis dans les confs de replicasets est bien corrects
si un pod crash alors le replication controller en recrée un.

Il y a a beaucoup de controller au sein de kube et du controller manager :
deployement controller, namespace controller , job controller , service account controller , endpoints controller ......

Tous ces controllers sont embarqués dans l'installation du pacquet de kube controller manager
si on download le paquet depuis le site de kube et qu'on lance le service : on peut examiner la conf quand on regarde le kube-controller-manager.service 
On peut configurer les conf de heartbeat, check etc ..dans cette config , de meme on peut définir l'aaplication des services controller manager que l'on veut activer dans cette section.

pour examiner le kube-controller-manager : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
la conf est comme d'habitude dans ce cas dans :
/etc/kubernetes/manifests/kube-controller-manager.yaml

sinon on peut examiner la conf dans le service gérer par systemctl.

on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v6 -> done.

= kube-scheduler : 

il est responsable de la planification des pods sur les nodes.Il va gérer le dispatch des pods sur les nodes. Quel pod va sur quel nodes en fonction des besoins du pods et des ressources des nodes.
Le sheduler examine la conf des pods et va s'assurer que le node qui va acceuillir ce pod en fonction de cette conf est correctment sizer en terme de ressource et de config specifique par exemple.
exemple : si notre pod a besoin de 10 cpu et qu'on est fasse a 4 nodes : deux nodes ont 4 cpu dispos , 1 a 12 cpu , le dernier a 16 cpu.
Les deux premiers nodes sont filtrés par le sheduler : pas assez de ressources.
Pour les deux autres nodes : le sheduler va faire un classement en donnant des scores de 0 a 10 aux nodes .
le scheduller va calculer le nombre de ressource dispo apres le set up de notre pod
ici : 2 et 6 . Le dernier node a donc une meilleur note.

L'installation du process se fait comme d'habitude en downloadant le binaire et l'exxecutant .

pour examiner le kube-scheduler : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
la conf est comme d'habitude dans ce cas dans :
/etc/kubernetes/manifests/kube-controller-manager.yaml

sinon on peut examiner la conf dans le service gérer par systemctl.

on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v7 -> done

= kubelet : 

Ce process est hébergé sur les nodes .c'est le process qui va être responsable de toutes les interractions avec le master.
iL va s'occuper de charger / décharger les containers.
Le kubelet va enregister le nodes, declencher la creation du pod via le runtime container et il va monitorer les pods 

kubeadm ne deploit pas automatiquement le conf du kubelet.
On doit toujours faire les installations manuelles des kubelet sur nos nodes.
on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v8 -> done

= kube-proxy : 

dans un cluster kube chaque pod peut contacter tous les autres pods.Ceci est possible grace a un reseau dédié pour les pods. Il y a beaucoup de solution dispos pour gérer la conf reseau des pods.

On peut avoir un pod qui heberge un server web ( 10.1.1.1 ) et un pod qui heberge une db ( 10.1.1.2)
le web peut contacter la db via son adresse ip de pod ...mais on est jamais sur que le pod ne va pas changer d'ip ou que le pod ne va pas crasher.
Il est plus sage de créer un service qui va expposer notre db dans le cluster : service db : 10.96.0.12 
On va donc contacter ce service qui va forwarder le traffic vers le backend. ( notre db ) , le service doit etre accessible depuis tous les nodes.
Le service est virtuel ..qui n'existe que dans la memoire de notre cluster kube.

kube-proxy est un process qui tourne sur tous les nodes du cluster .Son role est de surveiller toutes les nouvelles creations de services et d'ecrire les regles qui vont forwarder le service vers le backend correspondant .
Ceci est fait par des regles iptables.
on va avoir une regle iptable sur tous les nodes qui va forwarder le traffic du service db ( 10.96.0.12 ) vers le pod de backend ( 10.1.1.2 )

L'installation se fait comme d'habitude.

pour examiner le kube-proxy : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
Il est present sur tous les nodes en tant que daemonset :
kubectl get daemonset -n kube-system 

v2 ch9 -> done

= pods : 

on s'assure que le pod existe : un cluster existe, une image docker existe et kube peut la puller depuis une registry

un pod est l'objet le plus petit qu'on peut créer dans kube.
le plus petit est donc un container , d'une application contenu dans un pod.
En cas de charge ..on ne va pas creer un nouveau container de notre appli dans le pod ..on va créer un nouveau pod 
Si on est en cas de grosse charge on va créer des nouveaux pods sur un nouveau node.

On peut biensur avoir plusieurs container dans notre pod mais ils doivent chacun héberger une appli différente.
On est obliger de creer des pods dans kube ...meme si on a une tres simple appli ..mais c'est tres bien car on peut modifier nos archi sans pb


lancer un pod :

kubectl run nginx --image nginx 














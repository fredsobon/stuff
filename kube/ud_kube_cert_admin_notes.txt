====     notes cours kube admin certif : ===


== Core concepts : ==

=  architecture générale :
kubernetes est un systeme qui va gérer le de containers qui vont embarquer des applications. Il va entre autre permettre le déploiment de containers.

- les workers nodes : sont les serveurs qui vont héberger les containers contenant les applications.
- le master  est le cerveau qui va gerer les containers, les identifier, sur les workers, les monitorer etc ...: plan; schedule, monitor les nodes 

- etcd : est le composant qui va enregister la configuration sous forme clé valeur de nos containers, config etc ...
- kube-scheduler : est le composant du master qui va permettre de gérer le bon dispatch des containers en fonctions des ressourcers nécéssaires et différentes config inhérantes au cluster.
- le controller manager du master est le composant qui s'assure que tous les evenements sont corrects et que la situation actuelle est bien celle qui est défini dans les confs : que la flotte de container est cohérente  : il existe plusieurs sous composants du controller manager ( chaqun dédié à un périmetre précis. ex :
 node controller : va s'assurer de la bonne santé des containers, s'assurer qu'ils répondent bien ...
 replication controller : va s'assurer que le nombre de container défini et nécéssaire est bien correctement lancé 
 .... il ya toute une liste de controller qui seront etudier précisement.
- kube-api : est le composant central de kube qui va gérer l'orchestration de tous les composants : les composants internes au master, les communications des nodes vers le master et les communication de l'exterieur du cluster vers les composants. C'est le point d'entrée global.
- container runtime : toutes les applications vont être hébergées sous forme de container sur nos nodes. Il est possible d'avoir les composants du master sous forme de container aussi , ex dns etc ..donc pour faire tourner des containers il va falloir avoir le runtime dédié. 
Dans le cas le plus courant à l'heure actuelle il s'agit de docker.
-kubelet : ce composant est un agent qui est hébergé sur les nodes et qui va communiquer en permanence avec le kube-api server afin de recevoir les confs et tous les ordres a appliquer sur les containers.
- kube-proxy est le composant qui va permettre aux differentes applications hébergées sur les nodes de communiquer entre elles ( ex un serveur web d'un container hébergé sur un node voulant communiquer avec une database hébergée sur un autre node... )

= etcd : 

- presentation : 
etcd def : "etcd is a distributed  reliable key value store that is simple, secure and fast"
un systeme clé valeur permet de stocker les infos sous forme de document (contrairement aux bdd transactionnelles qui stockent en tables)
chaque document (comportant donc des couples clés valeurs) peuventt être modifiés indépendamment (contrairement aux tables habituelles dans lesquelles un ajout de colonnes ou de valeurs affectent toutes les données des tables)
Nous n'avons pas a updater les autres documents quand nous ajoutons une info dans un document précis.
Le format est assez simple : yaml ou json 

- install etcd : 
sudo apt install etcd 
ou recupérer le binaire sur github

Le serveur etcd écoute sur le port 2379 

boogie@satellite:~/Documents/stuff/kube$ ps fauxw |grep etcd
boogie    5102  0.0  0.0  17480   892 pts/1    S+   19:11   0:00              |           \_ grep --color=auto etcd
etcd      4984  0.9  0.2 11548340 17532 ?      Ssl  19:10   0:00 /usr/bin/etcd
boogie@satellite:~/Documents/stuff/kube$ ss -atln |grep 2379
LISTEN   0         128               127.0.0.1:2379             0.0.0.0:*    

- ecrire des données en base : 

on peut utiliser le client etcdctl (fourni dans le paquet pour injecter /retrouver des data ) :
boogie@satellite:~/Documents/stuff/kube$ etcdctl set key1 value1
value1
boogie@satellite:~/Documents/stuff/kube$ etcdctl set key2  boogie
boogie
boogie@satellite:~/Documents/stuff/kube$ etcdctl set bingo  bongo
bongo

- lire les données en base : 
boogie@satellite:~/Documents/stuff/kube$ etcdctl get key1
value1
boogie@satellite:~/Documents/stuff/kube$ etcdctl get key2
boogie
boogie@satellite:~/Documents/stuff/kube$ etcdctl get bingo
bongo

le detail des options est valable avec etcdctl --help

- Etcd -  role dans kubernetes : 

le role d'etcd dans le cluster kube est de stocker les valeurs des nodes, pods, configs, secrets, accounts, roles, bindings ....
chaque modification dans notre cluster est enregistrée dans le cluster etcd 

Nous pouvons déployer notre cluster etcd de plusieurs méthodes : ex : from scratch , via kubeadm ...il est important de comprendre les différences.
 
- from scratch :
on va downlader le binaire ou installer via notre distro 
definir un fichier de service (si ce n'est fait automatiquement via le package)
boogie@satellite:~/Documents/stuff/kube$ cat /lib/systemd/system/etcd.service
[Unit]
Description=etcd - highly-available key value store
Documentation=https://github.com/coreos/etcd
Documentation=man:etcd
After=network.target
Wants=network-online.target

[Service]
Environment=DAEMON_ARGS=
Environment=ETCD_NAME=%H
Environment=ETCD_DATA_DIR=/var/lib/etcd/default
EnvironmentFile=-/etc/default/%p
Type=notify
User=etcd
PermissionsStartOnly=true
#ExecStart=/bin/sh -c "GOMAXPROCS=$(nproc) /usr/bin/etcd $DAEMON_ARGS"
ExecStart=/usr/bin/etcd $DAEMON_ARGS
Restart=on-abnormal
#RestartSec=10s
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
Alias=etcd2.service

toute une partie de configuration n'est pas présente de base et est fondamentale pour notre serveur kubernetes : la gestion de certif , les config cluster etc ...
notamment la ligne : --advertise-clients-urls https://${INTERNAL_IP}::2379 

- kubeadm :
en deployant le cluster kube via kubeadm celui ci cree un cluster etcd sous forme de container que l'on peut voir avec :
kubectl get pods -n kube-system 
..
kube-system etcd-master

- pour examiner les clés enregistrées dans le cluster etcd on peut faire simplement :

kubectl exec etcd-master -n kube-system etcdctl get / --prefix -keys-only 

kubernetes stocke ses datas dans une arbo précise de sa registry : "/" 
sous "/" on a les differentes constructions : repertoires : minions, pods,nodes, replicasets ...

dans un cluster kube haute dispo on a plusieurs master kube ayant chacun des server etcd master : il faut s'assurer dans ce cas que chaque serveur etcd puisse communiquer avec les autres en adaptant la conf dédiée ( examen de la conf en pratique plus tard. )
--initial-cluster controller-0=https://${CONTROLLER0_IP}:2380,controller-1=https://${CONTROLLER1_IP}:2380

--> Chap2-video4 done

= kube apiserver : 

c'est le composant principal de managment dans kubernetes.
quand on utilise kubectl pour consulter par exemple les données stockées dans etcd , kubectl  va contacter le kube apiserver qui va authentifier la requette ( examiner les droits du user qui se connecte ) , puis interroger etcd qui retourne les valeurs a kube apiserver qui les renvoi au client ayant initié la demande.
on peut aussi directement communiquer avec l'api sans utiliser kubectl .

ex : grande etapes pour créer un pod : 
curl -X POST /api/v1/namespaces/default/pods ...[other] 
kube apiserver va :
1- authentifier le user 
2- valider la requette
3- recupérer les data 
4- mettre a jour etcd
5 -informer le user que le pod est crée
ensuite 
6- le scheduler qui observe en permanence le kube api server va realiser qu'il y a un nouveau pod non assigné sur un node.
7- le scheduleur identifie grace aux specificités du pods et des ressources des nodes un node qui va heberger le nouveau pod
8 - il informe le kube apiserver qu'un node a un nouveau pod attribué
9 - kube apiserver met a jour la base etcd avec les infos du node ébergeant le pod
10 - kube apiserver va ensuite envoyer l'information à l'agent kubelet présent sur le node qui va héberger le nouveau pod. 
11 - le kubelet va creer le pod et donner l'instruction au runtime de container de deployer l'image 
12 - kubelet informe ensuite kube apiserver que les opérations sont en place
13 - kubeapiserver update la base etcd

Ce type d'opérations est appliqué a chaque fois que des modifications sont a appliquer sur le cluster. Le kube apiserver est au centre de toutes les communications.

L'installation de kube apiserver peut se faire aisement via kubeadm mais il est important de connaitre les différents éléments de configuration présents pour une installation manuelle.
Il est fondamental de prendre en compte que tous les composants du cluster doivent pouvoir communiquer et que des certificats ssl sont à mettre en place . Le kube apiserver est le seul élement a communiquer directement avec etcd server . Sa presence dans la confi de apiserver n'est donc pas surprenante. 
Nous pouvons examiner la conf de kubeapiserver en downloadant le binaire sur le site de kube.
En fonction du mode d'installation ( kubadm/ scratch )on peut voir la conf dans :

/etc/kubernetes/manifeest/kube-apiserver.yaml
ou en service 
/etc/systemd/system/kube-apiserver.service

ch2 v5 -> done

= kube controller manager : 

il manage différents controllers dans kube.
Le role essentiel est de controller en permanence l'etat des ressources et remédier à un pb quand il y en a. puisque son role est s'assurer que l'etat de fonctionnement est totalement equivalent a la configuration et l'etat désiré du system.

- node controller : va s'occuper de monitorer les nodes et s'assurer que les applications fonctionnent correctement sur les nodes .Toutes les actions transitent par le kube apiserver comme on l'a vu.
le node controller recupere l'etat des nodes toutes les 5 secondes.
Si le node controler recoit une alerte de heartbeat d'un node : il flaggue celui ci comme unreachable pendant 40 secondes.
Si le node est toujours ko au bout de 5 minutes , le node controller va ejecter les pods et les héberger sur d'autre nodes si ces pods font partis d'un replicaset.

- replication controller : il est responsable des replicasets et doit s'assurer que le nombre de pods on line définis dans les confs de replicasets est bien corrects
si un pod crash alors le replication controller en recrée un.

Il y a a beaucoup de controller au sein de kube et du controller manager :
deployement controller, namespace controller , job controller , service account controller , endpoints controller ......

Tous ces controllers sont embarqués dans l'installation du pacquet de kube controller manager
si on download le paquet depuis le site de kube et qu'on lance le service : on peut examiner la conf quand on regarde le kube-controller-manager.service 
On peut configurer les conf de heartbeat, check etc ..dans cette config , de meme on peut définir l'aaplication des services controller manager que l'on veut activer dans cette section.

pour examiner le kube-controller-manager : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
la conf est comme d'habitude dans ce cas dans :
/etc/kubernetes/manifests/kube-controller-manager.yaml

sinon on peut examiner la conf dans le service gérer par systemctl.

on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v6 -> done.

= kube-scheduler : 

il est responsable de la planification des pods sur les nodes.Il va gérer le dispatch des pods sur les nodes. Quel pod va sur quel nodes en fonction des besoins du pods et des ressources des nodes.
Le sheduler examine la conf des pods et va s'assurer que le node qui va acceuillir ce pod en fonction de cette conf est correctment sizer en terme de ressource et de config specifique par exemple.
exemple : si notre pod a besoin de 10 cpu et qu'on est fasse a 4 nodes : deux nodes ont 4 cpu dispos , 1 a 12 cpu , le dernier a 16 cpu.
Les deux premiers nodes sont filtrés par le sheduler : pas assez de ressources.
Pour les deux autres nodes : le sheduler va faire un classement en donnant des scores de 0 a 10 aux nodes .
le scheduller va calculer le nombre de ressource dispo apres le set up de notre pod
ici : 2 et 6 . Le dernier node a donc une meilleur note.

L'installation du process se fait comme d'habitude en downloadant le binaire et l'exxecutant .

pour examiner le kube-scheduler : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
la conf est comme d'habitude dans ce cas dans :
/etc/kubernetes/manifests/kube-controller-manager.yaml

sinon on peut examiner la conf dans le service gérer par systemctl.

on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v7 -> done

= kubelet : 

Ce process est hébergé sur les nodes .c'est le process qui va être responsable de toutes les interractions avec le master.
iL va s'occuper de charger / décharger les containers.
Le kubelet va enregister le nodes, declencher la creation du pod via le runtime container et il va monitorer les pods 

kubeadm ne deploit pas automatiquement le conf du kubelet.
On doit toujours faire les installations manuelles des kubelet sur nos nodes.
on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v8 -> done

= kube-proxy : 

dans un cluster kube chaque pod peut contacter tous les autres pods.Ceci est possible grace a un reseau dédié pour les pods. Il y a beaucoup de solution dispos pour gérer la conf reseau des pods.

On peut avoir un pod qui heberge un server web ( 10.1.1.1 ) et un pod qui heberge une db ( 10.1.1.2)
le web peut contacter la db via son adresse ip de pod ...mais on est jamais sur que le pod ne va pas changer d'ip ou que le pod ne va pas crasher.
Il est plus sage de créer un service qui va expposer notre db dans le cluster : service db : 10.96.0.12 
On va donc contacter ce service qui va forwarder le traffic vers le backend. ( notre db ) , le service doit etre accessible depuis tous les nodes.
Le service est virtuel ..qui n'existe que dans la memoire de notre cluster kube.

kube-proxy est un process qui tourne sur tous les nodes du cluster .Son role est de surveiller toutes les nouvelles creations de services et d'ecrire les regles qui vont forwarder le service vers le backend correspondant .
Ceci est fait par des regles iptables.
on va avoir une regle iptable sur tous les nodes qui va forwarder le traffic du service db ( 10.96.0.12 ) vers le pod de backend ( 10.1.1.2 )

L'installation se fait comme d'habitude.

pour examiner le kube-proxy : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
Il est present sur tous les nodes en tant que daemonset :
kubectl get daemonset -n kube-system 

v2 ch9 -> done

= pods : 

on s'assure que le pod existe : un cluster existe, une image docker existe et kube peut la puller depuis une registry

un pod est l'objet le plus petit qu'on peut créer dans kube.
le plus petit est donc un container , d'une application contenu dans un pod.
En cas de charge ..on ne va pas creer un nouveau container de notre appli dans le pod ..on va créer un nouveau pod 
Si on est en cas de grosse charge on va créer des nouveaux pods sur un nouveau node.

On peut biensur avoir plusieurs container dans notre pod mais ils doivent chacun héberger une appli différente.
On est obliger de creer des pods dans kube ...meme si on a une tres simple appli ..mais c'est tres bien car on peut modifier nos archi sans pb

Pour lancer un pod :
on  va invoquer kubectl qui va demarrer un container nginx en downloadant l'image depuis la registry configurée ( docker hub ou une registry privée) :
kubectl run nginx --image nginx 
on va pouvoir lister les pods présents avec :
kubectl get pods 

on va pouvoir definir un objet pod puis creer le pod qui sera ici de cette conf 

cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels: 
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container 
      image: nginx

on instancie la creation de l'objet avec : 
kubectl create -f myapp-pod.yaml

si on modifie un param dans notre objet on pourra reappliquer le changement avec :
kubectl apply -f myapp-pod.yaml

v2 ch10 -> done

= kubernetes controllers : Replication controller : 

les controllers sont les cerveaux. Ce sont les process qui monitorent les objets kube et s'assurent qu'ils répondent correctement.

- replicat : 
on va assurer le principe de HA high availibility en assurant que si un pod crash le service est rendu au user. 
Même si on a un seul pod , le replication controller va nous aider a assurer le fonctionnement de notre appli.
Il s'assure que le nombre de pod défini dans la conf est toujours équivalent a ce qu'il y a dans le cluster kube.
Le replication controller va naturellement nous aider à assurer les montée en charge en créant les pods nécéssaires pour assurer le bon maintien de l'appli.
et ce même sur différents nodes.

On a deux termes qui ont le même sens :
replication controller 
replica set 

mais attention ils sont diférents.
replication controller  est l'ancienne techno qui a été remplacée par le replicat set. 
! Le replica set est à privilégier !

v11 no lesson 

- replication controller :

on va utiliser une structure qui sera présente dans toutes les fichiers de définition  de nos objets kube :
la premiere partie de notre definitoin est assez similaire a celle de la creation d'un pod 
la seconde partie est cruciale et c'est elle qui va gérer les specificité de notre objet ( section spec: )
vi rc-definition.yaml 

apiVersion: v1 ( -> cela va dependre de l'objet que nous creons: pour un replication controller c'est l'api v1 qui supporte cet objet.)
kind: ReplicationController (->  ici on renseigne le type d'objet que l'on veut creer : ici ReplicationController)
metadata:  on va rajouter ici différentes information specifique 
  name: myapp-rc
  labels: 
    app: my-app
    type: front-end
spec: on sait ici que dans l'object replication controller la definition va gérer la creation de plusieurs instances de pods
  - template: ( -> on va creer un template de pod qui sera utiliser par le replication controller pour gérer nos réplicas pour cela on va juste inserer la definition d'un pod au préalable créer par exemple : on va imbriquer les définitions d'objects. On reprend l'integralite de notre defintion d'objet pod SAUF la section apiVersion et kind ))
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas : 3 ( -> c'est dans cette section (qui doit se situer au même niveau que le spec du replication-controller que l'on doit setté le nombre de pod que l'on veut afin d'assurer la HA de notre appli. )
    
nous avons donc deux objets imbriqués avec deux sections metadata et specs.
Attention à l'indentation comme dans tout fichier yaml on doit bien s'assurer de la structure de notre fichier 
vi rc-definition.yaml 

apiVersion: v1 
kind: ReplicationController 
metadata:  
  name: myapp-rc
  labels: 
    app: my-app
    type: front-end
spec: 
  - template: 
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas: 3 

on va ensuite creer notre replication-controller :

kubectl create -f rc-definition.yaml

on va pouvoir examiner les replication-controller :
kubectl get replicationcontroller

NAME   DESIRED  CURRENT READY AGE
my-app       3        3     3  5s
on voit donc les différents status : etat attendu, l'etat actuel et le status du rc

kubectl get pods va nous montrer les trois pods differents crées via notre rc.

- replica set :

vi replicaset-definition.yaml 

apiVersion: apps/v1   --> ici on est obligé ded saisir apps/v1 pour l'apiversion : sous peine d'avoir une erreur de kube
kind: ReplicaSet      --> on renseigne ici le replicatset comme type 
metadata:  
  name: myapp-replicaset  
  labels: 
    app: my-app
    type: front-end
spec: 
  - template: 
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas: 3 
selector:         --> on est obligé ici de définir le pod qui va être gérer par le replicaset. 
  matchLabels: 
    type: front-end  --> le matchlabels va reprendre le type defini dans notre définition de pod.
    
Ceci est nécéssaire car le replicaset peut gérer d'autre pods que ceux inclus dans la definition même du replicaset. C'est la difference majeure entre les replication controller et les replicaset.
on voit qu'on est obliger de setter une mention matchLabels qui va recupérer les  infos settées dans notre section labels précédente.

kubectl create -f replicatset-definition.yaml

on va pouvoir examiner les replication-controller :
kubectl get replicaset

NAME   DESIRED  CURRENT READY AGE
my-app       3        3     3  5s
on voit donc les différents status : etat attendu, l'etat actuel et le status du rc

kubectl get pods va nous montrer les trois pods differents crées via notre rc.


- Labels et selectors : 

on a vu que le replicaset /controller s'occupe de gérer les pods crées et s'assure qu'ils sont up and running.
ce sont donc des process qui monitorent les pods.
Comment le replicat set connait les pods qu'il doit surveiller ..il peut y avoir des centaines de pods faisant tourner plusieurs applications.
C'est la que donner un label à notre pod lors de sa creation peut être vraiment utile..
on peut donc maintenant donner ce label comme filtre pour notre réplicatset.
Le replicaset sait donc maintenant quel pods il doit monitorer.
Le replicaset monitore l'etat des pods crées et s'assure que le nombre en prod en cohérent avec la définition.


on retrouve les concepts de labels dans diffrents objets gérés dans kubernetes.


Attention si on doit gérer un replicatset de pods déja crées ..il faut s'assurer que le bon template est charger dans notre definition. Le rc ne créera pas de nouveau pod si ceux existant avant sa creation comporte les bonnes informations.

- scaling :

il peut nous arriver de definit un replicat a 3 puis avoir une montée en charge et definir a 6 notre réplicat .
On peut agir de plusieurs manieres :

- update le champ replicat dans notre définition : le passer de 3 à 6.
ensuite on met a jour notre rc :

kubectl replace -f replicaset-definition.yaml

- on peut utiliser la commande scale en donnant nos arguments :

kubectl scale --replicas=6 -f replicaset-definition.yaml

on peut utiliser cette commande en utilisant le type name format : 
kubectl scale --replicas=6  replicaset myapp-replicaset

Attention dans ces cas le fichier de definition ne sera pas updater de 3 à 6 réplicats : les pods seront créer mais la conf inchangée.

cmds quick recap : 

kubectl create -f def.yaml
kubectl get replicaset
kubectl delete replicaset myapp-replicaset
kubectl replace -f def.yaml
kubectl scale --replicas=6 -f def.yaml 
kubectl edit replicasets new-replica-set

v2 ch12 ->done


-> tp replicatset -> done

kubectl create -f /root/replicaset-definition-1.yaml
master $ kubectl delete replicasets replicaset-1 replicaset-2
kubectl edit replicasets new-replica-set
master $ kubectl describe replicasets new-replica-set
Name:         new-replica-set
Namespace:    default
Selector:     name=busybox-pod
Labels:       name=busybox-pod
Annotations:  <none>
Replicas:     4 current / 4 desired
Pods Status:  0 Running / 4 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  name=busybox-pod
  Containers:
   busybox-container:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      echo Hello Kubernetes! && sleep 3600
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-b4dzk
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-5hs7t
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-w6pcw
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-vtfv9
  Normal  SuccessfulCreate  16m   replicaset-controller  Created pod: new-replica-set-j2pkm
  Normal  SuccessfulCreate  15m   replicaset-controller  Created pod: new-replica-set-9sx4x

= deployment : 

Le deployment  est un objet kube qui va permettre le deploiement / mise à jour, rollback d'application sans interruption de service pour le client : seamless deployment / rolling update ( mise à jour séquentielle des pods ) sans interruption de service.
On va pouvoir mettre a jour une nouvelle version d'une appli dispo dans le docker hub pod par pod , sans devoir relancer tous les pods comme cela se passerai avec le replicaset.
on va pouvoir rolbacker sur la version précedente de notre appli, on va pouvoir tester le comportement de notre nouvelle appli sur un pod avant de déployer sur tout notre parc ..toutes ses possibilités sont offertes par l'objet deployment.

Tous les containers sont encapsulés dans des pods qui sont deployés par des replicaset ou replication controller : le deployment encapsule tout ces objets dans la hierarchie kube.

La définition d'un deployment se fait comme pour les replicasets :


vi deployment-definition.yaml

apiVersion: apps/v1   --> ici on est obligé ded saisir apps/v1 pour l'apiversion : comme pour le replicaset.
kind: Deployment      --> on renseigne ici le deployment  comme type
metadata:
  name: myapp-deployment
  labels:
    app: my-app
    type: front-end
spec:
  - template:
     metadata:
       name: myapp-pod
       labels:
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container
           image: nginx
replicas: 3
selector:        
  matchLabels:
    type: front-end 


On retrouve le template de pod et le nombre de replicas désiré.

kubectl create -f deployment-definition.yaml
on verifie la creation : 
kubectl get deployments
le deployment crée automatiquement le replicaset 
kubectl get replicaset
et les pods sont aussi crées automatiquement :
kubectl get pods

on peut voir la création globale de nos objets avec :

kubectl get all 

ch2 v13 -> done


- tp deployment -> done

ex deployment :

master $ kubectl describe deployments
Name:                   frontend-deployment
Namespace:              default
CreationTimestamp:      Wed, 01 May 2019 17:57:33 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision=1
Selector:               name=busybox-pod
Replicas:               4 desired | 4 updated | 4 total | 0 available | 4 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=busybox-pod
  Containers:
   busybox-container:
    Image:      busybox888
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      echo Hello Kubernetes! && sleep 3600
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   frontend-deployment-b9b7c6cb6 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set frontend-deployment-b9b7c6cb6 to 4



creation de deploiement :
avec comme nom httpd-frontend  / replicas 3 et image : httpd:2.4-alpine
master $ cat my-deploy.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      name: httpd-pod
  template:
    metadata:
      labels:
        name: httpd-pod
    spec:
      containers:
      - name: httpd-container
        image: httpd:2.4-alpine
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600



= Namespaces : 

Les namespaces vont permettre de cloisonner nos resssources dans des espaces complétements distincts et hermétique. on pourra interragir diectement dans un namespace si il est accessible depuis notre namespace  actuel . 
On peut interroger cependant des objets appartenant a différents namespaces.

De base tous les objets que l'on crée et avec lesquels on interréagis sont dans le "default" namespace de kube.
Kube pour ses besoins internes crées des objets, pods services , dns, network etc ... dans un namespace privé afin de le protéger des users : c'est le namespace appellé kube-system

Un troisieme namespace est crée automatiquement c'est le kube-public : dans lequel on devrait avoir tous les objets qu'on l'on veut rendre accessible, dispo pour les users.

Bien evidemment pour des tests on peut continuer a travailler dans le namespace par default mais on doit impérativement créer des namespace dans les environmments devant important et en production.

on va pouvoir utiliser le même cluster pour différents env ( dev, preprod )mais on va isoler leur ressources avec des namespaces .

On va pouvoir attribuer des quotats par namespaces ( nombre de pods, cpu , memoire ) : afin d'assurer la garantie de ne pas dépasser des ressources physiques .


On va pouvoir au sein d'un namespace appellé directement un service :
ex : dans le namespace default :

- un web-pod
- un web-deployment
- un service-db

pour se connecter a la db on pourra simplement faire :

ex : mysql_connect("db.service")

Si on doit atteindre un service dans un autre namespace ex dev on devra alors utiliser un nom complet créer et diffusé dans le service dns a la création du service par kub lui même :
ex : mysql_connect("db.service.dev.svc.cluster.local") 

on retrouve le nom du service , le namespace , le type d'objet : svc ici et le domain kube natif cluster.local

- lister les pods d'un autre namespace :

kubectl get pods --namespace="notre_namespace"

- creer un pod dans un namespace particulier : 
kubectl create -f pod-definition.yaml --namespace="my_namespace"

On peut rajouter directement le namespace concerné par notre pod a la creation de celui ci en rajoutant :


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod

metadata:
  name: dev
  namespace: my-namespace  <<<< on ajoute ici le namespace de notre object.
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

- creation de namespace :

on peut ecrire un objet namespace :

cat myapp-namespace.yaml
---
apiVersion: v1
kind: Namespace

metadata:
  name: dev

puis :
kubectl create namespace -f myapp-namespace.yaml

on peut creer a la volée le namespace aussi :

kubectl create namespace  dev 

- Switching de namespace : 

on va pouvoir pour plus de facilité quand on travaille longtemps sur un namespace le définir par défaut : de maniere a ne pas devoir systematiqueme ntutiliser le flag : --namespace

kubectl config set-context $(kubectl config current-context) --namespace=dev 

- listing de tous les pods de tous les namespaces : 

pour voir tous les pods de tous les namespaces :

kubectl get pods --all-namespaces


= Quota :
on va pouvoir definir des quota de ressources pour nos namespaces :


cat myapp-quota.yaml
---
apiVersion: v1
kind: ResourceQuota

metadata:
  name: compute-quota
  namespace: dev 
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limit.cpu: "10"
    limit.memory: 10Gi


kubectl create  -f myapp-quota.yaml

ch2 v14 -> done

tp namespace ->  done

- creation d'un pod pour le namespace finance : 
master $ cat fin-pod.yaml
apiVersion: v1
kind: Pod

metadata:
  name: redis

metadata:
  name: redis
  namespace: finance
  labels:
    app: redis
    type: cache
spec:
  containers:
    - name: redis-container
      image: redis
master $ kubectl create -f fin-pod.yaml --namespace=finance
pod/redis created


= services : 
Les services vont nous permettre la communication interne et externe à notre cluster avec notre application.
On va donc pouvoir connecter nos applications et nos users entre eux.
par exemple : les services vont permettre à nos users de communiquer avec nos front-end, nos frontend avec nos backends et nos backends avec les db.

ex : 

un user sur un laptop en 192.168.0.2 
un node sur le même reseau que le user : 192.168.0.5
un pod sur le node en 10.244.0.2 qui héberge un service web en 80
un reseau pour nos pods en 10.244.0.0 

1/ de base : 
le user ne ping pas le pod
2/ en se connectant en ssh sur le node le user peut communiquer avec le pod : curl http://10.244.0.2 > ok
biensur on est donc dans ce cas dans le cluster kube ....ce n'est pas vraiment ce que l'on veut.

on veut pouvoir atteindre l'appli de notre pod via notre laptop: c'est la qu'interviennent les services.
Le role du service est d'ecouter un port sur le node et de forwarder les connexions sur ce port au port de notre application sur le pod : c'es tce qu'on appelle un NodePort service.

Il y a plusieurs type de service :

NodePort c'est le cas le plus simple que nous venons de décrire brièvement.
ClusterIp : qui sera un service virtuel crée au sein du cluster qui permettra la commnunication entre différents services ( ex: frontend avec backend via ce service )
LoadBalancer : qui va servir à repartir la charge entre nos différents pods hébergeant une appli frontend par exemple.

- NodePort : 
dans notre cas le laptop accedera a l'appli en appellant l'ip pod suivi d'un port .Cette connexion sera forwardée par le service sur le pod qui hébergera l'application.

sur le pod
on a le "target port" du pod qui sert l'application -> 10.244.0.2:80
sur le service 
on a le port 80  qui sera le même que celui de notre appli , le service a une ClusterIp qui permettra de recevoir les flux du client passant par le node puis envoyé ce flux vers le pod et le service final : ex: 10.106.1.12:80 
sur le node :
on a un port qui permettra d'acceder a ce node et au service  : c'est le NodePort ex: 192.169.0.5:3008

/!\ les NodePorts sont assignés dans un range bien dédié et obligatoire compri entre 30000 et 32767

On va créer un objet service comme nos différents objects :


master $ cat service-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: myapp-service

spec:
  type: NodePort  <<< c'est le type qu'on vient de voir;
  ports: 
    - targetPort: 80 <<< c'est le port final qui sera sur notre pod
      port: 80 <<< ici il s'agit du port de notre service vers lequel on enverra les flux vers le pod ( c'est le même port )
      nodeport: 30008 <<<< c'est le port qui sera sur notre node et qui sera le point d'entrée par lequel les appels pourront joindre notre appli
master $ kubectl create -f service-definition.yaml 

Attention les ports sont sous forme de tableau 

On va maintenant devoir mapper vers quel node on doit envoyer nos requettes ..il peut y avoir enormement de nodes .

On va pour cela utiliser les selectors et labels qu'on a utiliser avec la creation de pod.

master $ cat service-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: myapp-service

spec:
  type: NodePort  
  ports: 
    - targetPort: 80 
      port: 80 
      nodeport: 30008 
  selector: 
      app: myapp     <<<< on va ici ajouter les deux params que l'on a créer dans nos pods : afin de bien matcher les bons pods 
      type: frontend 

-commandes : 

kubectl get services

On va donc pouvoir maintenant atteindre notre application depuis notre laptop en utilisant l'ip du node et le nodeport défini :

curl http://192.168.0.5:30008


Nous avons vu ici l'utilisation d'un service permettant l'acces a une app hébergée sur un pod.

Biensur il est possible sur un node 'avoir une multitude de pod hébergeant la même application pour assurer la high availibility et le load balancing .Comment faire ? 

Tous ces pods vont avoir le même label ex : myapp  à la creation du service on va utiliser en selector la même valeur que le label : on aura donc potentiellement un service qui redirigera vers une multitude de pods ayant tous une ip dédié.
Le service  selectionnera donc tous ces pods comme endpoint pour forward les connexions des users.
Le loadbalancing est fait automatiquement entre tous les pods via l'algorithme "random" 
Le service  héberge donc nativement un loadbalancer en build in pour distribuer la charge sur les pods.
Aucune actions de configuration supplémentaire à faire de notre coté.


Il est egalement possible d'avoir plusieurs nodes hébergeant les pods de notre appplication. Comment permettre l'acces via le service ? 
Kubernetes va automatiquement créer un service qui va distribuer le traffic en asssignant le traget port sur les pods de nos nodes.
ex: si on a 3 nodes :
192.168.0.1
192.168.0.2
192.168.0.3

on pourra acceder naturellement à l'appli ecoutant sur le port 80 de nos pods hébergés sur chaque nodes en interrogant n'importe qu'ell ip suivi du NodePort défini :

curl http://192.168.0.1:30008
curl http://192.168.0.2:30008
curl http://192.168.0.3:30008

Donc on voit l'extreme fluidité du service qui est automatiquement opérationnel :
pour une app hébergée sur un pod d'un node
pour une app hébergée sur plusieurs pods d'un node
pour une app hébergée sur plusieurs pods de plusieurs nodes

ch2 v15 -> done

- ClusterIP :

Dans une appli classique on peut avoir des app de frontend qui doivent communiquées avec des app de backend qui doivent communiquer avec des db 
Comme on le sait chaque pod a une ip mais celle ci n'est pas a prendre en compte puisque potentiellement volatile.

Un service kube va permettre de grouper différentes ressource de pods afin de les rendre accessibles aux autres composants du cluster.
Chaque service  une ip et un nom associé :on pourra donc scale notre cluster sans souci.
Ce nom sera utilisé par les autres pods pour attenidre notre service :

ex les pods frontend appelleront le service backend . les pods du service backend appelleront le service db : derriere chaque service on retrouve les pods qui recevront de manière random les requetes des clients.

master $ cat service-clusterip-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: backend

spec:
  type: ClusterIp  <<< c'est le type qu'on vient de voir;
  ports:
    - targetPort: 80 <<< c'est le port final qui sera sur notre pod
      port: 80 <<< ici il s'agit du port de notre service vers lequel on enverra les flux vers le pod ( c'est le même port )
      
      selector: 
        app: myapp   <<<< on va linker notre service aux label des pods hebergeant le service .
        type: backend         
master $ kubectl create -f service-definition.yaml

on peut controller la creation de notre service avec 
kubectl get service


chap2 v16 -> done

- 
tp services ->  done.

master $ kubectl get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   6m    <<<< ce service est crée par default au lancement e kube.

master $ kubectl describe service
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP:                10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.17.0.79:6443
Session Affinity:  None
Events:            <none>


- creation de service de type NodePort pour atteindre depuis le browser via le port 30080 l'appli simple-webapp qui ecoute sur le port 8080
master $ cat service-definition-1.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 8080
      port: 8080
      nodePort: 30080
  selector:
    name: simple-webapp


== chapitre 3 : Scheduling : 

ch3 v2

= Fonctionnement du scheduling :

sur chaque definition de pod un champ : nodeName n'est pas à remplir quand on crée le pod mais  Kube renseigne se champ automatiquement à la creation
Le scheduleur va examiner toutes les definitions de pods et quand il voit le champ nodeName vide : il va alors examiner la conf et déterminer sur quel node il va falloir créer ce pod . 
Le nom du node va donc etre associer au nodeName de la definition de notre pod 
ex : 
nodeName : node2
S'il n'y a pas de sheduleur pour gérer les nouveaux pods ceux restent toujours en etat de pending : dans l'attente de l'assignation a un node.

Dans ce cas nous avons plusieurs solutions : 

= Manual scheduling : 

on va pouvoir assigner manuellement un pod a un node.
dans ce cas nous avons juste à renseigner la section nodeName dans la création de notre pod

ex : 


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
    - Ports: 
        containerPort : 8080

nodeName : node2

Par contre Kube NE nous laissera pas modifier ce champ si l'objet existe déja.
Dans ce cas nous allons devoir créer un objet qui referencera notre pod et ira donc sur le node que nous désirons : ce nouvel object est de type Binding.

cat pod-bind-definition.yaml

apiVersion: v1
kind: Binding

metadata:
  name: nginx
target: 
  apiVersion: v1
  kind: Node
  name: node2

on va ensuite envoyer en requete POST a l'api de binding notre définition d'objet :
on va convertir notre yaml en json : 
curl -H "Content-type: application/json" -X POST -D '{"apiVersion: v1, kind: Binding ......}' https://server/api/v1/namespaces/default/pods/$podname/binding

tp manual shedule -> done 

master $ cat nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginxmaster
master $ kubectl create -f nginx.yaml
pod/nginx created
master $ kubectl get pods
NAME      READY     STATUS    RESTARTS   AGE
nginx     0/1       Pending   0          3s

master $ kubectl get pods --all-namespaces
NAMESPACE     NAME                             READY     STATUS    RESTARTS   AGE
default       nginx                            0/1       Pending   0          1m
kube-system   coredns-78fcdf6894-7zbqs         1/1       Running   0          6m
kube-system   coredns-78fcdf6894-c9zll         1/1       Running   0          6m
kube-system   etcd-master                      1/1       Running   0          5m
kube-system   kube-apiserver-master            1/1       Running   0          5m
kube-system   kube-controller-manager-master   1/1       Running   0          5m
kube-system   kube-proxy-fdlnz                 1/1       Running   0          6m
kube-system   kube-proxy-wgccj                 1/1       Running   0          6m
kube-system   weave-net-9mhgm                  2/2       Running   1          6m
kube-system   weave-net-wqfwn                  2/2       Running   1          6m

on voit qu'il n'y a pas de scheduler 
on va detruire le pod puis le recreer en inserant dans la defintion le nodeName que l'on veut : 

cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01



master $ vi nginx.yaml
master $ kubectl create -f nginx.yaml
pod/nginx created
master $ kubectl get pod
NAME      READY     STATUS    RESTARTS   AGE
nginx     1/1       Running   0          11s
master $ kubectl describe pod nginx
Name:               nginx
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               node01/172.17.0.73
Start Time:         Sun, 05 May 2019 17:57:28 +0000
Labels:             <none>
Annotations:        <none>
Status:             Running
IP:                 10.32.0.2
Containers:
  nginx:
    Container ID:   docker://e990f90c76b0712ddfcba7632c047b3158e24c2e2b23de406b593b8cfd925234
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:e71b1bf4281f25533cf15e6e5f9be4dac74d2328152edf7ecde23abc54e16c1c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Sun, 05 May 2019 17:57:37 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-89rxh (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-89rxh:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-89rxh
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason   Age   From             Message
  ----    ------   ----  ----             -------
  Normal  Pulling  22s   kubelet, node01  pulling image "nginx"
  Normal  Pulled   15s   kubelet, node01  Successfully pulled image "nginx"
  Normal  Created  14s   kubelet, node01  Created container
  Normal  Started  14s   kubelet, node01  Started container


= Labels et selectors : 

 Les labels et les selectors sont des methodes pour grouper les choses entre elles.
 On veut etre capable de filtrer en fonctions de group mais également en fonction de certains critères.
 ex : tous les animaux verts
 tous les animaux de type oiseaux verts
 Les labels sont alors ideaux . Les labels sont des propriétés attachees a des items 
ex : 
class : mammal
kind : domestic 
color : green

class : reptile
kind : wild 
color : purple

on trouve les labels dans plein de contexte web : sur les blogs, dans les boutiques on line ...
  
- utilisation des labels et selectors dans kube : 

on crée plein de differents objects : pods, deployment, replicaset, service ....
avec le temps on peu donc avoir une multitude d'objet dans notre cluster.
on doit donc etre capable de voir nos différents objects par catégories.
on peut grouper les objects par leur type ( pods ..) , leur application ( my-app1 ), leut fonctionnalité ( frontend )
chaque object attache les labels selon nos besoins.

On va donc créer dansla section metadata les labels que l'on veut sans limitation de nombre :

cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
    function: web 
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01


Le selector va utiliser des labels pour filtrer nos objects : 
selectors :
  app: app1
  function: front-end 

le selector sera ici app=app1 par exemple.

Un fois l'object créer on va pourvoir selectionner en fonction de critère passés en arguments au selector :


kubectl get pods --selector app=app1 

kube utilise les labels et selectors en interne pour grouper les ressources.

ex dans un replicaset on va definir les pods utilisés via leur labels 

Attention on a dans ce cas 2 labels :

le premier en debut de definition concerne le replicaset lui meme alors que le second defini dans la section du template concerne les pods.  --> Il faut bien faire attention c'est une erreur classique.
Dans notre cas actuellement les labels de replicaset  ne sont pas utilisés : nous verrons plus tard que ceci peut être utile pour la decouverte auto de replicaset.

Afin de connecter le replicaset au pod : on va definir dans la conf de notre replicaset un matchingLabels qui va comporter une entrée qui va correspondre a une definition de label dans la section de notre pod :


definition de replicaset a revoir pour l'exemple de correspondance matchlabels repliscaset et label pod sur entrée app: app1

kind: ReplicaSet      
metadata:
  name: simple-webapp
  labels:
    app: app1
    type: front-end
spec:
  replicas: 3
  selector:        
    matchLabels:
      type: app1
  - template:
     metadata:
       name: myapp-pod
       labels:
         app: app1
         type: front-end
     spec:
       containers:
         - name: simple-webapp
           image: simple-webapp


C'est le même principe pour les differents objects kube

ex : pour les services :
quand un service est créer il va utiliser  l'entrée defini dans le selector pour matcher avec l'entree du label du pod defini dans le replicaset.

On peut rajouter une autre caracteristique en plus des labels et selectors : les annotations .
Cela va anous permettre de donner des infos supplementaires dans la description de nos pods ( numero de version , adresse mail ...)


kind: ReplicaSet      
metadata:
  name: simple-webapp
  labels:
    app: app1
    type: front-end
  annotations:
    buildversion: 1.34

ch 3 v3 -> done


ch3 tp labels /selector -> done 

aster $ kubectl get pods --selector env=dev
NAME          READY     STATUS    RESTARTS   AGE
app-1-g9k8r   1/1       Running   0          1m
app-1-t7qcs   1/1       Running   0          1m
app-1-zhbdm   1/1       Running   0          1m
db-1-2vgkv    1/1       Running   0          1m
db-1-7678l    1/1       Running   0          1m
db-1-fj5rm    1/1       Running   0          1m
db-1-z6hbq    1/1       Running   0          1m
master $ kubectl get pods --selector bu=finance
NAME          READY     STATUS    RESTARTS   AGE
app-1-g9k8r   1/1       Running   0          1m
app-1-t7qcs   1/1       Running   0          1m
app-1-zhbdm   1/1       Running   0          1m
app-1-zzxdf   1/1       Running   0          1m
auth          1/1       Running   0          1m
db-2-sdnz9    1/1       Running   0          1m
master $ kubectl get all --selector env=prod
NAME              READY     STATUS    RESTARTS   AGE
pod/app-1-zzxdf   1/1       Running   0          2m
pod/app-2-cfnth   1/1       Running   0          2m
pod/auth          1/1       Running   0          2m
pod/db-2-sdnz9    1/1       Running   0          2m

kubectl get all --selector env=prod,bu=finance,tier=frontend
NAME              READY     STATUS    RESTARTS   AGE
pod/app-1-zzxdf   1/1       Running   0          3m

cat replicaset-definition-1.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend   <<<< il faut faire matcher le label de notre replicatset avec celui des pods
  template:
    metadata:
      labels:
        tier: frontend   <<<< label des pods
    spec:
      containers:
      - name: nginx
        image: nginx



= Ressources limits : 

comme on l'a vu le scheduleur va s'occuper de la creation et la repartition des pods sur les différents nodes, en fonction des ressources nécéssaires pour le pod et des ressources dispos sur les nodes.

Si un pod demande des ressources non dispos alors il reste en status pending et on a une info spécifique au pb : ..insufficiant cpu  ....etc .

kube de base considère qu'un container  de base a besoin de :

0.5 cpu 
256M ram 
Si de base on sait que notre appli va consommer plus , il suffit de déclarer dans la définition les valeurs nécéssaires.


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
    function: web 
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01

  ressources: 
    requests:
      memory: "1Gb"
      cpu: 1

- cpu :
L'unite 1 cpu correspond a :
1 vcpu
1 thread
on peut mettre 100m et decendre jusqu'a 0.1m qui veut dire milli.
on n'est pas obligé de mettre des multiples de 0.5 ou 100m 

- memory :
on peut mettre les valeurs que l'on veut avec le suffixes qu'on veut mais  il s'agit de Mi Gi ayant pour base 1k = 1024bytes 
contrairement a 1K = 1000bytes pour Mo et Go

Dans le monde de docker ..il n'y a pas de limite et le container peut consumer les ressources qu'il veut sur le node.
De base kube fixe des limites pour les containers afin de preserver les nodes :

1vcpu par container
512mi de ram 

On va pouvoir definir des limits à nos container dans la definition de notre pod 

le container ne pourra pas depasser le cpu defini mais il pourra depasser la memoire jusqu'a crasher ....


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
    function: web 
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01

  ressources:                    <<<<< definition d'une section de resources: on defini le besoin de notre pod 
    requests:
      memory: "1Gi"
      cpu: 1
    limits:                     <<<<<< definition des limites de notre container : limites fixées manuellement pour contenir les eventuels debordements du container
      memory: "2Gi"
      cpu: 2


ch 3 v4 -> done


tp ressources -> done 

creation d'un pod avec ressources cpu : 


Attention à l'indentation -> le yaml ici est correct.

master $ cat lion.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: lion
spec:
  containers:
  - name: lion
    image: vish/stress
    resources:
       requests:
        cpu: 2
      

cat elephant.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: elephant
spec:
  containers:
  - name: elephant
    image: polinux/stress
    resources:
       limits:
        memory: "20Mi"
       requests:
        memory: "15Mi"
        cpu: 2

= Daemon sets : 

Le daemonset va permettre comme le replicaset de s'assurer du deploiement correct des pods définis.
A la différence du replicaset , le daemonset va déployer un pod apres sa creation  sur CHAQUE node du cluster.
Il s'assure qu'une copie de chaque pods et toujours presentes sur tous les nodes du cluster.
Chaque nouveau node va donc recevoir les pods définis dans notre daemonset.

Un exemple parfait d'utilisation est l'ajout de monitoring et de logger pour le cluster.
On va deployer ses agents sous formes de pod et déployer sur chaque node du cluster via un daemonset pour s'assurer du bon fonctionnement de notre cluster.
Dans ce cas on n'a pas besoin de s'occuper de retirer / rajouter des agents de log ou monito sur un node qui sort du cluster ou un nouveau node qui rentre dans le cluster.
Un second exemple d'utilisation du daemonset est le kube-proxy qui va être deployer sous forme de pod sur chaque noeud de notre cluster en daemonset.
Une solution de networking comme weave nécéssite le deployment de conf sur chaque node : le daemonset est la encore conseillé.



La definition du daemonset est identique au replicaset :
on a les pods du daemonset definis et imbriquer dans la section template, on a un selector défini pour matcher ces pods.

cat daemonset-definition.yaml 
apiVersion: apps/v1  
kind: DeaemonSet      
metadata:
  name: monitoring-daemon
spec:
  selector:         
    matchLabels:
      app: monitoring-agent
  - template:
      metadata:
        labels:
          app: monitoring-agent
      spec:
        containers:
          - name: monitoring-agent
            image: monitoring-agent

on doit s'assurer que le matchLabels de notre selector correspond au label defini dans le template de nos pods.            


kubectl create -f daemonset-definition.yaml

kubectl get daemonsets

kubectl describe daemonsets our_daemonset_name

Fonctionnement :

on peut definir manuellement comme on l'a vu l'affectation de pod à un node  

-> c'etait comme cela avant kubernetes v1.12

Depuis le daemonsets utilise le scheduleur par defaut et les nodes affinity.


ch 3 v5 -> done


tp daemonsets -> done 

master $ kubectl get daemonsets --all-namespaces
NAMESPACE     NAME         DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                   AGE
kube-system   kube-proxy   2         2         2         2            2           beta.kubernetes.io/arch=amd64   3m
kube-system   weave-net    2         2         2         2            2           <none>

master $ kubectl describe daemonset kube-proxy --namespace=kube-system
Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  beta.kubernetes.io/arch=amd64
Labels:         k8s-app=kube-proxy
Annotations:    <none>
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy-amd64:v1.11.3
    Port:       <none>
    Host Port:  <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
    Environment:  <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  5m    daemonset-controller  Created pod: kube-proxy-q2csm
  Normal  SuccessfulCreate  5m    daemonset-controller  Created pod: kube-proxy-dssqz

creation d'un daemonset dont on défini le nom "elasticsearch" , le namespace dans le quel il evolue : kube-system et avec une image : k8s.gcr.io/elasticsearch:1.20

master $ cat fluend-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
     metadata:
       labels:
         app: elasticsearch
     spec:
       containers:
         - name: elasticsearch
           image: k8s.gcr.io/elasticsearch:1.20



= multiple schedulers : =

On a vu que le sheduling va dependre de plusieurs criteres que l'on peut avoir défini : ( node affinity , ressources ..).
Il est possible d'avoir des besoins complexes avec par exemple une appli qui devra avoir ses pods placés sur des nodes après des tests additionnels (??)
Que ce passe il quand aucune des specificité ne sont disponibles ?

On va pouvoir définir notre propre sheduleur.
Dans ce cas les applications normales utiliseront le scheduleur par defaut et notre application scpecifique elle sera gérée par notre scheduleur personel.: 
le cluster kube peut utiliser en même temps plusieurs scheduleurs.

Quand on crée un pod ou un deployement on va pouvoir definir le scheduleur qui va être utilisé.
On peut downloader le binaire du sheduleur sur le site de kube.
Si on ne precise rien : se sera un sheduleur par defaut qui se lancera  comme un service (linux)

  --sheduleur-name=default
on peut déployer un scheduleur perso en utilisant le même bianaire mais en specifiant un nom :

 ... 
  --sheduleur-name=my-own-scheduler

Il va être nécéssaire de bien identifié le nom du scheduleur

ex : kubeadm deploi le scheduleur sous forme de pod ( pas comme un service ) 
on peut s'inspirer de la conf de kubeadm pour créer la definition de notre scheduler 

il faudra biensur s'assurer de bien renseigner son nom et aussi de rajouter la section : --leader-elect: true
qui sert a s'assurer qu'un seul master est le sheduler dans le cas d'un cluster multi master ( HA )

Un seul scheduleur doit être actif sur un master : il faut faire une election de leader.
ex : 
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: custom-scheduler
    namespace: kube-system
spec:
  containers:
    - command:
    - /usr/local/bin/kube-scheduler
    - --address=127.0.0.1
    - --leader-elect=true
    - --scheduler-name=custom-scheduler
    image: g1g1/custom-kube-scheduler:1.0
    name: kube-second-scheduler

On va pouvoir examiner une fois que l'on lance notre pods qu'il tourne bien . Une fois que notre sheduleur est up , on va definir les pods qui vont l'utiliser.
dans la définition du pod on va definir le scheduleur crée au prealable: 
cat elephant.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: elephant
spec:
  containers:
  - name: elephant
    image: polinux/stress
    resources:
       limits:
        memory: "20Mi"
       requests:
        memory: "15Mi"
        cpu: 2
  schedulerName: custom-scheduler    <<< on defini explicitement le scheduleur qui va être utilisé pour notre pod.

  On crée notre popd et on le lance : il doit être en runnning 


- Comment identifier quel sheduler est  utilisé ?

on va utiliser la gestion des events : 

kubectl get events : va lister les evenement s dans le namespace courant.
on peut voir dans notre cas apres la creation du pods qu'il y a une section SOURCE qui indique les scheduleurs en charge de l'event : ici la creation du pod est bien dépendante de notre scheduleur personalisé.


on va egalement pouvoir examiner les logs :

kubectl logs custom-scheduler --namespace=kube-system

ch3 v6 ->  done 

tp scheduler -> done

creation d'un scheduler :

nom :my-sheduler 


master $ cat /var/answers/my-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  creationTimestamp: null
  labels:
    component: my-scheduler
    tier: control-plane
  name: my-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=false
    - --port=10282
    - --scheduler-name=my-scheduler
    image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10282
        scheme: HTTP
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
status: {}


creation d'un pod rattaché au sheduler crée :

master $ cat nginx-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginx
  schedulerName: my-scheduler


Pour résumer :
on peut installer / gérer un scheduleur :

-> en service linux
-> en pod comme kubeadm

references supplémentaires pour scheduling .

ch3 v7 -> en cours

== Logging /monitoring kube cluster : ==


= monitoring : 

-> nodes level
-> pods level 
etc ...

des solutions existent pour monitorer le cluster kube : prometheus, elasticstac , datadog ..
Une solution native existait mais est maintenant déprecated : heapster 

on va pouvoir cependant recupérer les métriques des nodes et des pods , les aggréger et les stocker en mémoire.
On devra cependant utiliser une solution externe pour stocker les data.

Les metrics sont récoltées par un sous composants de kubelet sur les nodes. cet agent se nomme CAdvisor ( container advisor ) : cet agent est responsable de la récupération des métriques et de leur envoi à kubelet qui va les transférer au server de metrique final.

on peut tester sur minikube :

minkube addons enable metrics-server
sinon on peut recupérer le code :

git clone https://github.com/kubernetes-incubator/metrics-server


https://github.com/kodekloudhub/kubernetes-metrics-server.git

kubectl create -f deploy/1.8+/

kubectl top node
kubectl top pod

tp monitoring -> done

= logging : 

comme pour docker on va pouvoir examiner les logs de nos pods :

kubectl logs my_pod
kubectl logs -f my_pod   <<<< -f pour voir les logs défiler en continu.

Si on a plusieurs container dans notre pod on va devoir nommer explicitement le container dont on veut voir les logs .
ex: 

kubectl logs -f my_pod my_container1_name 

tp logging -> done


== Application life cycle managment : ==





====     notes cours kube admin certif : ===


== Core concepts : ==

=  architecture générale :
kubernetes est un systeme qui va gérer le de containers qui vont embarquer des applications. Il va entre autre permettre le déploiment de containers.

- les workers nodes : sont les serveurs qui vont héberger les containers contenant les applications.
- le master  est le cerveau qui va gerer les containers, les identifier, sur les workers, les monitorer etc ...: plan; schedule, monitor les nodes 

- etcd : est le composant qui va enregister la configuration sous forme clé valeur de nos containers, config etc ...
- kube-scheduler : est le composant du master qui va permettre de gérer le bon dispatch des containers en fonctions des ressourcers nécéssaires et différentes config inhérantes au cluster.
- le controller manager du master est le composant qui s'assure que tous les evenements sont corrects et que la situation actuelle est bien celle qui est défini dans les confs : que la flotte de container est cohérente  : il existe plusieurs sous composants du controller manager ( chaqun dédié à un périmetre précis. ex :
 node controller : va s'assurer de la bonne santé des containers, s'assurer qu'ils répondent bien ...
 replication controller : va s'assurer que le nombre de container défini et nécéssaire est bien correctement lancé 
 .... il ya toute une liste de controller qui seront etudier précisement.
- kube-api : est le composant central de kube qui va gérer l'orchestration de tous les composants : les composants internes au master, les communications des nodes vers le master et les communication de l'exterieur du cluster vers les composants. C'est le point d'entrée global.
- container runtime : toutes les applications vont être hébergées sous forme de container sur nos nodes. Il est possible d'avoir les composants du master sous forme de container aussi , ex dns etc ..donc pour faire tourner des containers il va falloir avoir le runtime dédié. 
Dans le cas le plus courant à l'heure actuelle il s'agit de docker.
-kubelet : ce composant est un agent qui est hébergé sur les nodes et qui va communiquer en permanence avec le kube-api server afin de recevoir les confs et tous les ordres a appliquer sur les containers.
- kube-proxy est le composant qui va permettre aux differentes applications hébergées sur les nodes de communiquer entre elles ( ex un serveur web d'un container hébergé sur un node voulant communiquer avec une database hébergée sur un autre node... )

= etcd : 

- presentation : 
etcd def : "etcd is a distributed  reliable key value store that is simple, secure and fast"
un systeme clé valeur permet de stocker les infos sous forme de document (contrairement aux bdd transactionnelles qui stockent en tables)
chaque document (comportant donc des couples clés valeurs) peuventt être modifiés indépendamment (contrairement aux tables habituelles dans lesquelles un ajout de colonnes ou de valeurs affectent toutes les données des tables)
Nous n'avons pas a updater les autres documents quand nous ajoutons une info dans un document précis.
Le format est assez simple : yaml ou json 

- install etcd : 
sudo apt install etcd 
ou recupérer le binaire sur github

Le serveur etcd écoute sur le port 2379 

boogie@satellite:~/Documents/stuff/kube$ ps fauxw |grep etcd
boogie    5102  0.0  0.0  17480   892 pts/1    S+   19:11   0:00              |           \_ grep --color=auto etcd
etcd      4984  0.9  0.2 11548340 17532 ?      Ssl  19:10   0:00 /usr/bin/etcd
boogie@satellite:~/Documents/stuff/kube$ ss -atln |grep 2379
LISTEN   0         128               127.0.0.1:2379             0.0.0.0:*    

- ecrire des données en base : 

on peut utiliser le client etcdctl (fourni dans le paquet pour injecter /retrouver des data ) :


boogie@satellite:~/Documents/stuff/kube$ etcdctl set key1 value1
value1
boogie@satellite:~/Documents/stuff/kube$ etcdctl set key2  boogie
boogie
boogie@satellite:~/Documents/stuff/kube$ etcdctl set bingo  bongo
bongo

- lire les données en base : 
boogie@satellite:~/Documents/stuff/kube$ etcdctl get key1
value1
boogie@satellite:~/Documents/stuff/kube$ etcdctl get key2
boogie
boogie@satellite:~/Documents/stuff/kube$ etcdctl get bingo
bongo

le detail des options est valable avec etcdctl --help

- Etcd -  role dans kubernetes : 

le role d'etcd dans le cluster kube est de stocker les valeurs des nodes, pods, configs, secrets, accounts, roles, bindings ....
chaque modification dans notre cluster est enregistrée dans le cluster etcd 

Nous pouvons déployer notre cluster etcd de plusieurs méthodes : ex : from scratch , via kubeadm ...il est important de comprendre les différences.
 
- from scratch :
on va downlader le binaire ou installer via notre distro 
definir un fichier de service (si ce n'est fait automatiquement via le package)
boogie@satellite:~/Documents/stuff/kube$ cat /lib/systemd/system/etcd.service
[Unit]
Description=etcd - highly-available key value store
Documentation=https://github.com/coreos/etcd
Documentation=man:etcd
After=network.target
Wants=network-online.target

[Service]
Environment=DAEMON_ARGS=
Environment=ETCD_NAME=%H
Environment=ETCD_DATA_DIR=/var/lib/etcd/default
EnvironmentFile=-/etc/default/%p
Type=notify
User=etcd
PermissionsStartOnly=true
#ExecStart=/bin/sh -c "GOMAXPROCS=$(nproc) /usr/bin/etcd $DAEMON_ARGS"
ExecStart=/usr/bin/etcd $DAEMON_ARGS
Restart=on-abnormal
#RestartSec=10s
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
Alias=etcd2.service

toute une partie de configuration n'est pas présente de base et est fondamentale pour notre serveur kubernetes : la gestion de certif , les config cluster etc ...
notamment la ligne : --advertise-clients-urls https://${INTERNAL_IP}::2379 

- kubeadm :
en deployant le cluster kube via kubeadm celui ci cree un cluster etcd sous forme de container que l'on peut voir avec :
kubectl get pods -n kube-system 
..
kube-system etcd-master

- pour examiner les clés enregistrées dans le cluster etcd on peut faire simplement :

kubectl exec etcd-master -n kube-system etcdctl get / --prefix -keys-only 

kubernetes stocke ses datas dans une arbo précise de sa registry : "/" 
sous "/" on a les differentes constructions : repertoires : minions, pods,nodes, replicasets ...

dans un cluster kube haute dispo on a plusieurs master kube ayant chacun des server etcd master : il faut s'assurer dans ce cas que chaque serveur etcd puisse communiquer avec les autres en adaptant la conf dédiée ( examen de la conf en pratique plus tard. )
--initial-cluster controller-0=https://${CONTROLLER0_IP}:2380,controller-1=https://${CONTROLLER1_IP}:2380

--> Chap2-video4 done

= kube apiserver : 

c'est le composant principal de managment dans kubernetes.
quand on utilise kubectl pour consulter par exemple les données stockées dans etcd , kubectl  va contacter le kube apiserver qui va authentifier la requette ( examiner les droits du user qui se connecte ) , puis interroger etcd qui retourne les valeurs a kube apiserver qui les renvoi au client ayant initié la demande.
on peut aussi directement communiquer avec l'api sans utiliser kubectl .

ex : grande etapes pour créer un pod : 
curl -X POST /api/v1/namespaces/default/pods ...[other] 
kube apiserver va :
1- authentifier le user 
2- valider la requette
3- recupérer les data 
4- mettre a jour etcd
5 -informer le user que le pod est crée
ensuite 
6- le scheduler qui observe en permanence le kube api server va realiser qu'il y a un nouveau pod non assigné sur un node.
7- le scheduleur identifie grace aux specificités du pods et des ressources des nodes un node qui va heberger le nouveau pod
8 - il informe le kube apiserver qu'un node a un nouveau pod attribué
9 - kube apiserver met a jour la base etcd avec les infos du node ébergeant le pod
10 - kube apiserver va ensuite envoyer l'information à l'agent kubelet présent sur le node qui va héberger le nouveau pod. 
11 - le kubelet va creer le pod et donner l'instruction au runtime de container de deployer l'image 
12 - kubelet informe ensuite kube apiserver que les opérations sont en place
13 - kubeapiserver update la base etcd

Ce type d'opérations est appliqué a chaque fois que des modifications sont a appliquer sur le cluster. Le kube apiserver est au centre de toutes les communications.

L'installation de kube apiserver peut se faire aisement via kubeadm mais il est important de connaitre les différents éléments de configuration présents pour une installation manuelle.
Il est fondamental de prendre en compte que tous les composants du cluster doivent pouvoir communiquer et que des certificats ssl sont à mettre en place . Le kube apiserver est le seul élement a communiquer directement avec etcd server . Sa presence dans la confi de apiserver n'est donc pas surprenante. 
Nous pouvons examiner la conf de kubeapiserver en downloadant le binaire sur le site de kube.
En fonction du mode d'installation ( kubadm/ scratch )on peut voir la conf dans :

/etc/kubernetes/manifeest/kube-apiserver.yaml
ou en service 
/etc/systemd/system/kube-apiserver.service

ch2 v5 -> done

= kube controller manager : 

il manage différents controllers dans kube.
Le role essentiel est de controller en permanence l'etat des ressources et remédier à un pb quand il y en a. puisque son role est s'assurer que l'etat de fonctionnement est totalement equivalent a la configuration et l'etat désiré du system.

- node controller : va s'occuper de monitorer les nodes et s'assurer que les applications fonctionnent correctement sur les nodes .Toutes les actions transitent par le kube apiserver comme on l'a vu.
le node controller recupere l'etat des nodes toutes les 5 secondes.
Si le node controler recoit une alerte de heartbeat d'un node : il flaggue celui ci comme unreachable pendant 40 secondes.
Si le node est toujours ko au bout de 5 minutes , le node controller va ejecter les pods et les héberger sur d'autre nodes si ces pods font partis d'un replicaset.

- replication controller : il est responsable des replicasets et doit s'assurer que le nombre de pods on line définis dans les confs de replicasets est bien corrects
si un pod crash alors le replication controller en recrée un.

Il y a a beaucoup de controller au sein de kube et du controller manager :
deployement controller, namespace controller , job controller , service account controller , endpoints controller ......

Tous ces controllers sont embarqués dans l'installation du pacquet de kube controller manager
si on download le paquet depuis le site de kube et qu'on lance le service : on peut examiner la conf quand on regarde le kube-controller-manager.service 
On peut configurer les conf de heartbeat, check etc ..dans cette config , de meme on peut définir l'aaplication des services controller manager que l'on veut activer dans cette section.

pour examiner le kube-controller-manager : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
la conf est comme d'habitude dans ce cas dans :
/etc/kubernetes/manifests/kube-controller-manager.yaml

sinon on peut examiner la conf dans le service gérer par systemctl.

on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v6 -> done.

= kube-scheduler : 

il est responsable de la planification des pods sur les nodes.Il va gérer le dispatch des pods sur les nodes. Quel pod va sur quel nodes en fonction des besoins du pods et des ressources des nodes.
Le sheduler examine la conf des pods et va s'assurer que le node qui va acceuillir ce pod en fonction de cette conf est correctment sizer en terme de ressource et de config specifique par exemple.
exemple : si notre pod a besoin de 10 cpu et qu'on est fasse a 4 nodes : deux nodes ont 4 cpu dispos , 1 a 12 cpu , le dernier a 16 cpu.
Les deux premiers nodes sont filtrés par le sheduler : pas assez de ressources.
Pour les deux autres nodes : le sheduler va faire un classement en donnant des scores de 0 a 10 aux nodes .
le scheduller va calculer le nombre de ressource dispo apres le set up de notre pod
ici : 2 et 6 . Le dernier node a donc une meilleur note.

L'installation du process se fait comme d'habitude en downloadant le binaire et l'exxecutant .

pour examiner le kube-scheduler : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
la conf est comme d'habitude dans ce cas dans :
/etc/kubernetes/manifests/kube-controller-manager.yaml

sinon on peut examiner la conf dans le service gérer par systemctl.

on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v7 -> done

= kubelet : 

Ce process est hébergé sur les nodes .c'est le process qui va être responsable de toutes les interractions avec le master.
iL va s'occuper de charger / décharger les containers.
Le kubelet va enregister le nodes, declencher la creation du pod via le runtime container et il va monitorer les pods 

kubeadm ne deploit pas automatiquement le conf du kubelet.
On doit toujours faire les installations manuelles des kubelet sur nos nodes.
on peut biensur examiner en details le process lancé avec un ps fauxww 

ch2 v8 -> done

= kube-proxy : 

dans un cluster kube chaque pod peut contacter tous les autres pods.Ceci est possible grace a un reseau dédié pour les pods. Il y a beaucoup de solution dispos pour gérer la conf reseau des pods.

On peut avoir un pod qui heberge un server web ( 10.1.1.1 ) et un pod qui heberge une db ( 10.1.1.2)
le web peut contacter la db via son adresse ip de pod ...mais on est jamais sur que le pod ne va pas changer d'ip ou que le pod ne va pas crasher.
Il est plus sage de créer un service qui va expposer notre db dans le cluster : service db : 10.96.0.12 
On va donc contacter ce service qui va forwarder le traffic vers le backend. ( notre db ) , le service doit etre accessible depuis tous les nodes.
Le service est virtuel ..qui n'existe que dans la memoire de notre cluster kube.

kube-proxy est un process qui tourne sur tous les nodes du cluster .Son role est de surveiller toutes les nouvelles creations de services et d'ecrire les regles qui vont forwarder le service vers le backend correspondant .
Ceci est fait par des regles iptables.
on va avoir une regle iptable sur tous les nodes qui va forwarder le traffic du service db ( 10.96.0.12 ) vers le pod de backend ( 10.1.1.2 )

L'installation se fait comme d'habitude.

pour examiner le kube-proxy : si celui ci a été installer avec kubeadm , alors il est deployer en tant que pod au sein du namespace kube-system
Il est present sur tous les nodes en tant que daemonset :
kubectl get daemonset -n kube-system 

v2 ch9 -> done

= pods : 

on s'assure que le pod existe : un cluster existe, une image docker existe et kube peut la puller depuis une registry

un pod est l'objet le plus petit qu'on peut créer dans kube.
le plus petit est donc un container , d'une application contenu dans un pod.
En cas de charge ..on ne va pas creer un nouveau container de notre appli dans le pod ..on va créer un nouveau pod 
Si on est en cas de grosse charge on va créer des nouveaux pods sur un nouveau node.

On peut biensur avoir plusieurs container dans notre pod mais ils doivent chacun héberger une appli différente.
On est obliger de creer des pods dans kube ...meme si on a une tres simple appli ..mais c'est tres bien car on peut modifier nos archi sans pb

Pour lancer un pod :
on  va invoquer kubectl qui va demarrer un container nginx en downloadant l'image depuis la registry configurée ( docker hub ou une registry privée) :
kubectl run nginx --image nginx 
on va pouvoir lister les pods présents avec :
kubectl get pods 

on va pouvoir definir un objet pod puis creer le pod qui sera ici de cette conf 

cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels: 
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container 
      image: nginx

on instancie la creation de l'objet avec : 
kubectl create -f myapp-pod.yaml

si on modifie un param dans notre objet on pourra reappliquer le changement avec :
kubectl apply -f myapp-pod.yaml

v2 ch10 -> done

= kubernetes controllers : Replication controller : 

les controllers sont les cerveaux. Ce sont les process qui monitorent les objets kube et s'assurent qu'ils répondent correctement.

- replicat : 
on va assurer le principe de HA high availibility en assurant que si un pod crash le service est rendu au user. 
Même si on a un seul pod , le replication controller va nous aider a assurer le fonctionnement de notre appli.
Il s'assure que le nombre de pod défini dans la conf est toujours équivalent a ce qu'il y a dans le cluster kube.
Le replication controller va naturellement nous aider à assurer les montée en charge en créant les pods nécéssaires pour assurer le bon maintien de l'appli.
et ce même sur différents nodes.

On a deux termes qui ont le même sens :
replication controller 
replica set 

mais attention ils sont diférents.
replication controller  est l'ancienne techno qui a été remplacée par le replicat set. 
! Le replica set est à privilégier !

v11 no lesson 

- replication controller :

on va utiliser une structure qui sera présente dans toutes les fichiers de définition  de nos objets kube :
la premiere partie de notre definitoin est assez similaire a celle de la creation d'un pod 
la seconde partie est cruciale et c'est elle qui va gérer les specificité de notre objet ( section spec: )
vi rc-definition.yaml 

apiVersion: v1 ( -> cela va dependre de l'objet que nous creons: pour un replication controller c'est l'api v1 qui supporte cet objet.)
kind: ReplicationController (->  ici on renseigne le type d'objet que l'on veut creer : ici ReplicationController)
metadata:  on va rajouter ici différentes information specifique 
  name: myapp-rc
  labels: 
    app: my-app
    type: front-end
spec: on sait ici que dans l'object replication controller la definition va gérer la creation de plusieurs instances de pods
  - template: ( -> on va creer un template de pod qui sera utiliser par le replication controller pour gérer nos réplicas pour cela on va juste inserer la definition d'un pod au préalable créer par exemple : on va imbriquer les définitions d'objects. On reprend l'integralite de notre defintion d'objet pod SAUF la section apiVersion et kind ))
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas : 3 ( -> c'est dans cette section (qui doit se situer au même niveau que le spec du replication-controller que l'on doit setté le nombre de pod que l'on veut afin d'assurer la HA de notre appli. )
    
nous avons donc deux objets imbriqués avec deux sections metadata et specs.
Attention à l'indentation comme dans tout fichier yaml on doit bien s'assurer de la structure de notre fichier 
vi rc-definition.yaml 

apiVersion: v1 
kind: ReplicationController 
metadata:  
  name: myapp-rc
  labels: 
    app: my-app
    type: front-end
spec: 
  - template: 
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas: 3 

on va ensuite creer notre replication-controller :

kubectl create -f rc-definition.yaml

on va pouvoir examiner les replication-controller :
kubectl get replicationcontroller

NAME   DESIRED  CURRENT READY AGE
my-app       3        3     3  5s
on voit donc les différents status : etat attendu, l'etat actuel et le status du rc

kubectl get pods va nous montrer les trois pods differents crées via notre rc.

- replica set :

vi replicaset-definition.yaml 

apiVersion: apps/v1   --> ici on est obligé ded saisir apps/v1 pour l'apiversion : sous peine d'avoir une erreur de kube
kind: ReplicaSet      --> on renseigne ici le replicatset comme type 
metadata:  
  name: myapp-replicaset  
  labels: 
    app: my-app
    type: front-end
spec: 
  - template: 
     metadata:
       name: myapp-pod
       labels: 
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container 
           image: nginx
replicas: 3 
selector:         --> on est obligé ici de définir le pod qui va être gérer par le replicaset. 
  matchLabels: 
    type: front-end  --> le matchlabels va reprendre le type defini dans notre définition de pod.
    
Ceci est nécéssaire car le replicaset peut gérer d'autre pods que ceux inclus dans la definition même du replicaset. C'est la difference majeure entre les replication controller et les replicaset.
on voit qu'on est obliger de setter une mention matchLabels qui va recupérer les  infos settées dans notre section labels précédente.

kubectl create -f replicatset-definition.yaml

on va pouvoir examiner les replication-controller :
kubectl get replicaset

NAME   DESIRED  CURRENT READY AGE
my-app       3        3     3  5s
on voit donc les différents status : etat attendu, l'etat actuel et le status du rc

kubectl get pods va nous montrer les trois pods differents crées via notre rc.


- Labels et selectors : 

on a vu que le replicaset /controller s'occupe de gérer les pods crées et s'assure qu'ils sont up and running.
ce sont donc des process qui monitorent les pods.
Comment le replicat set connait les pods qu'il doit surveiller ..il peut y avoir des centaines de pods faisant tourner plusieurs applications.
C'est la que donner un label à notre pod lors de sa creation peut être vraiment utile..
on peut donc maintenant donner ce label comme filtre pour notre réplicatset.
Le replicaset sait donc maintenant quel pods il doit monitorer.
Le replicaset monitore l'etat des pods crées et s'assure que le nombre en prod en cohérent avec la définition.


on retrouve les concepts de labels dans diffrents objets gérés dans kubernetes.


Attention si on doit gérer un replicatset de pods déja crées ..il faut s'assurer que le bon template est charger dans notre definition. Le rc ne créera pas de nouveau pod si ceux existant avant sa creation comporte les bonnes informations.

- scaling :

il peut nous arriver de definit un replicat a 3 puis avoir une montée en charge et definir a 6 notre réplicat .
On peut agir de plusieurs manieres :

- update le champ replicat dans notre définition : le passer de 3 à 6.
ensuite on met a jour notre rc :

kubectl replace -f replicaset-definition.yaml

- on peut utiliser la commande scale en donnant nos arguments :

kubectl scale --replicas=6 -f replicaset-definition.yaml

on peut utiliser cette commande en utilisant le type name format : 
kubectl scale --replicas=6  replicaset myapp-replicaset

Attention dans ces cas le fichier de definition ne sera pas updater de 3 à 6 réplicats : les pods seront créer mais la conf inchangée.

cmds quick recap : 

kubectl create -f def.yaml
kubectl get replicaset
kubectl delete replicaset myapp-replicaset
kubectl replace -f def.yaml
kubectl scale --replicas=6 -f def.yaml 
kubectl edit replicasets new-replica-set

v2 ch12 ->done


-> tp replicatset -> done

kubectl create -f /root/replicaset-definition-1.yaml
master $ kubectl delete replicasets replicaset-1 replicaset-2
kubectl edit replicasets new-replica-set
master $ kubectl describe replicasets new-replica-set
Name:         new-replica-set
Namespace:    default
Selector:     name=busybox-pod
Labels:       name=busybox-pod
Annotations:  <none>
Replicas:     4 current / 4 desired
Pods Status:  0 Running / 4 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  name=busybox-pod
  Containers:
   busybox-container:
    Image:      busybox
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      echo Hello Kubernetes! && sleep 3600
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-b4dzk
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-5hs7t
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-w6pcw
  Normal  SuccessfulCreate  23m   replicaset-controller  Created pod: new-replica-set-vtfv9
  Normal  SuccessfulCreate  16m   replicaset-controller  Created pod: new-replica-set-j2pkm
  Normal  SuccessfulCreate  15m   replicaset-controller  Created pod: new-replica-set-9sx4x

= deployment : 

Le deployment  est un objet kube qui va permettre le deploiement / mise à jour, rollback d'application sans interruption de service pour le client : seamless deployment / rolling update ( mise à jour séquentielle des pods ) sans interruption de service.
On va pouvoir mettre a jour une nouvelle version d'une appli dispo dans le docker hub pod par pod , sans devoir relancer tous les pods comme cela se passerai avec le replicaset.
on va pouvoir rolbacker sur la version précedente de notre appli, on va pouvoir tester le comportement de notre nouvelle appli sur un pod avant de déployer sur tout notre parc ..toutes ses possibilités sont offertes par l'objet deployment.

Tous les containers sont encapsulés dans des pods qui sont deployés par des replicaset ou replication controller : le deployment encapsule tout ces objets dans la hierarchie kube.

La définition d'un deployment se fait comme pour les replicasets :


vi deployment-definition.yaml

apiVersion: apps/v1   --> ici on est obligé ded saisir apps/v1 pour l'apiversion : comme pour le replicaset.
kind: Deployment      --> on renseigne ici le deployment  comme type
metadata:
  name: myapp-deployment
  labels:
    app: my-app
    type: front-end
spec:
  - template:
     metadata:
       name: myapp-pod
       labels:
         app: myapp
         type: front-end
     spec:
       containers:
         - name: nginx-container
           image: nginx
replicas: 3
selector:        
  matchLabels:
    type: front-end 


On retrouve le template de pod et le nombre de replicas désiré.

kubectl create -f deployment-definition.yaml
on verifie la creation : 
kubectl get deployments
le deployment crée automatiquement le replicaset 
kubectl get replicaset
et les pods sont aussi crées automatiquement :
kubectl get pods

on peut voir la création globale de nos objets avec :

kubectl get all 

ch2 v13 -> done


- tp deployment -> done

ex deployment :

master $ kubectl describe deployments
Name:                   frontend-deployment
Namespace:              default
CreationTimestamp:      Wed, 01 May 2019 17:57:33 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision=1
Selector:               name=busybox-pod
Replicas:               4 desired | 4 updated | 4 total | 0 available | 4 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=busybox-pod
  Containers:
   busybox-container:
    Image:      busybox888
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -c
      echo Hello Kubernetes! && sleep 3600
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   frontend-deployment-b9b7c6cb6 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set frontend-deployment-b9b7c6cb6 to 4



creation de deploiement :
avec comme nom httpd-frontend  / replicas 3 et image : httpd:2.4-alpine
master $ cat my-deploy.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      name: httpd-pod
  template:
    metadata:
      labels:
        name: httpd-pod
    spec:
      containers:
      - name: httpd-container
        image: httpd:2.4-alpine
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600



= Namespaces : 

Les namespaces vont permettre de cloisonner nos resssources dans des espaces complétements distincts et hermétique. on pourra interragir diectement dans un namespace si il est accessible depuis notre namespace  actuel . 
On peut interroger cependant des objets appartenant a différents namespaces.

De base tous les objets que l'on crée et avec lesquels on interréagis sont dans le "default" namespace de kube.
Kube pour ses besoins internes crées des objets, pods services , dns, network etc ... dans un namespace privé afin de le protéger des users : c'est le namespace appellé kube-system

Un troisieme namespace est crée automatiquement c'est le kube-public : dans lequel on devrait avoir tous les objets qu'on l'on veut rendre accessible, dispo pour les users.

Bien evidemment pour des tests on peut continuer a travailler dans le namespace par default mais on doit impérativement créer des namespace dans les environmments devant important et en production.

on va pouvoir utiliser le même cluster pour différents env ( dev, preprod )mais on va isoler leur ressources avec des namespaces .

On va pouvoir attribuer des quotats par namespaces ( nombre de pods, cpu , memoire ) : afin d'assurer la garantie de ne pas dépasser des ressources physiques .


On va pouvoir au sein d'un namespace appellé directement un service :
ex : dans le namespace default :

- un web-pod
- un web-deployment
- un service-db

pour se connecter a la db on pourra simplement faire :

ex : mysql_connect("db.service")

Si on doit atteindre un service dans un autre namespace ex dev on devra alors utiliser un nom complet créer et diffusé dans le service dns a la création du service par kub lui même :
ex : mysql_connect("db.service.dev.svc.cluster.local") 

on retrouve le nom du service , le namespace , le type d'objet : svc ici et le domain kube natif cluster.local

- lister les pods d'un autre namespace :

kubectl get pods --namespace="notre_namespace"

- creer un pod dans un namespace particulier : 
kubectl create -f pod-definition.yaml --namespace="my_namespace"

On peut rajouter directement le namespace concerné par notre pod a la creation de celui ci en rajoutant :


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod

metadata:
  name: dev
  namespace: my-namespace  <<<< on ajoute ici le namespace de notre object.
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

- creation de namespace :

on peut ecrire un objet namespace :

cat myapp-namespace.yaml
---
apiVersion: v1
kind: Namespace

metadata:
  name: dev

puis :
kubectl create namespace -f myapp-namespace.yaml

on peut creer a la volée le namespace aussi :

kubectl create namespace  dev 

- Switching de namespace : 

on va pouvoir pour plus de facilité quand on travaille longtemps sur un namespace le définir par défaut : de maniere a ne pas devoir systematiqueme ntutiliser le flag : --namespace

kubectl config set-context $(kubectl config current-context) --namespace=dev 

- listing de tous les pods de tous les namespaces : 

pour voir tous les pods de tous les namespaces :

kubectl get pods --all-namespaces


= Quota :
on va pouvoir definir des quota de ressources pour nos namespaces :


cat myapp-quota.yaml
---
apiVersion: v1
kind: ResourceQuota

metadata:
  name: compute-quota
  namespace: dev 
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limit.cpu: "10"
    limit.memory: 10Gi


kubectl create  -f myapp-quota.yaml

ch2 v14 -> done

tp namespace ->  done

- creation d'un pod pour le namespace finance : 
master $ cat fin-pod.yaml
apiVersion: v1
kind: Pod

metadata:
  name: redis

metadata:
  name: redis
  namespace: finance
  labels:
    app: redis
    type: cache
spec:
  containers:
    - name: redis-container
      image: redis
master $ kubectl create -f fin-pod.yaml --namespace=finance
pod/redis created


= services : 
Les services vont nous permettre la communication interne et externe à notre cluster avec notre application.
On va donc pouvoir connecter nos applications et nos users entre eux.
par exemple : les services vont permettre à nos users de communiquer avec nos front-end, nos frontend avec nos backends et nos backends avec les db.

ex : 

un user sur un laptop en 192.168.0.2 
un node sur le même reseau que le user : 192.168.0.5
un pod sur le node en 10.244.0.2 qui héberge un service web en 80
un reseau pour nos pods en 10.244.0.0 

1/ de base : 
le user ne ping pas le pod
2/ en se connectant en ssh sur le node le user peut communiquer avec le pod : curl http://10.244.0.2 > ok
biensur on est donc dans ce cas dans le cluster kube ....ce n'est pas vraiment ce que l'on veut.

on veut pouvoir atteindre l'appli de notre pod via notre laptop: c'est la qu'interviennent les services.
Le role du service est d'ecouter un port sur le node et de forwarder les connexions sur ce port au port de notre application sur le pod : c'es tce qu'on appelle un NodePort service.

Il y a plusieurs type de service :

NodePort c'est le cas le plus simple que nous venons de décrire brièvement.
ClusterIp : qui sera un service virtuel crée au sein du cluster qui permettra la commnunication entre différents services ( ex: frontend avec backend via ce service )
LoadBalancer : qui va servir à repartir la charge entre nos différents pods hébergeant une appli frontend par exemple.

- NodePort : 
dans notre cas le laptop accedera a l'appli en appellant l'ip pod suivi d'un port .Cette connexion sera forwardée par le service sur le pod qui hébergera l'application.

sur le pod
on a le "target port" du pod qui sert l'application -> 10.244.0.2:80
sur le service 
on a le port 80  qui sera le même que celui de notre appli , le service a une ClusterIp qui permettra de recevoir les flux du client passant par le node puis envoyé ce flux vers le pod et le service final : ex: 10.106.1.12:80 
sur le node :
on a un port qui permettra d'acceder a ce node et au service  : c'est le NodePort ex: 192.169.0.5:3008

/!\ les NodePorts sont assignés dans un range bien dédié et obligatoire compri entre 30000 et 32767

On va créer un objet service comme nos différents objects :


master $ cat service-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: myapp-service

spec:
  type: NodePort  <<< c'est le type qu'on vient de voir;
  ports: 
    - targetPort: 80 <<< c'est le port final qui sera sur notre pod
      port: 80 <<< ici il s'agit du port de notre service vers lequel on enverra les flux vers le pod ( c'est le même port )
      nodeport: 30008 <<<< c'est le port qui sera sur notre node et qui sera le point d'entrée par lequel les appels pourront joindre notre appli
master $ kubectl create -f service-definition.yaml 

Attention les ports sont sous forme de tableau 

On va maintenant devoir mapper vers quel node on doit envoyer nos requettes ..il peut y avoir enormement de nodes .

On va pour cela utiliser les selectors et labels qu'on a utiliser avec la creation de pod.

master $ cat service-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: myapp-service

spec:
  type: NodePort  
  ports: 
    - targetPort: 80 
      port: 80 
      nodeport: 30008 
  selector: 
      app: myapp     <<<< on va ici ajouter les deux params que l'on a créer dans nos pods : afin de bien matcher les bons pods 
      type: frontend 

-commandes : 

kubectl get services

On va donc pouvoir maintenant atteindre notre application depuis notre laptop en utilisant l'ip du node et le nodeport défini :

curl http://192.168.0.5:30008


Nous avons vu ici l'utilisation d'un service permettant l'acces a une app hébergée sur un pod.

Biensur il est possible sur un node 'avoir une multitude de pod hébergeant la même application pour assurer la high availibility et le load balancing .Comment faire ? 

Tous ces pods vont avoir le même label ex : myapp  à la creation du service on va utiliser en selector la même valeur que le label : on aura donc potentiellement un service qui redirigera vers une multitude de pods ayant tous une ip dédié.
Le service  selectionnera donc tous ces pods comme endpoint pour forward les connexions des users.
Le loadbalancing est fait automatiquement entre tous les pods via l'algorithme "random" 
Le service  héberge donc nativement un loadbalancer en build in pour distribuer la charge sur les pods.
Aucune actions de configuration supplémentaire à faire de notre coté.


Il est egalement possible d'avoir plusieurs nodes hébergeant les pods de notre appplication. Comment permettre l'acces via le service ? 
Kubernetes va automatiquement créer un service qui va distribuer le traffic en asssignant le traget port sur les pods de nos nodes.
ex: si on a 3 nodes :
192.168.0.1
192.168.0.2
192.168.0.3

on pourra acceder naturellement à l'appli ecoutant sur le port 80 de nos pods hébergés sur chaque nodes en interrogant n'importe qu'ell ip suivi du NodePort défini :

curl http://192.168.0.1:30008
curl http://192.168.0.2:30008
curl http://192.168.0.3:30008

Donc on voit l'extreme fluidité du service qui est automatiquement opérationnel :
pour une app hébergée sur un pod d'un node
pour une app hébergée sur plusieurs pods d'un node
pour une app hébergée sur plusieurs pods de plusieurs nodes

ch2 v15 -> done

- ClusterIP :

Dans une appli classique on peut avoir des app de frontend qui doivent communiquées avec des app de backend qui doivent communiquer avec des db 
Comme on le sait chaque pod a une ip mais celle ci n'est pas a prendre en compte puisque potentiellement volatile.

Un service kube va permettre de grouper différentes ressource de pods afin de les rendre accessibles aux autres composants du cluster.
Chaque service  une ip et un nom associé :on pourra donc scale notre cluster sans souci.
Ce nom sera utilisé par les autres pods pour attenidre notre service :

ex les pods frontend appelleront le service backend . les pods du service backend appelleront le service db : derriere chaque service on retrouve les pods qui recevront de manière random les requetes des clients.

master $ cat service-clusterip-definition.yaml
apiVersion: v1
kind: Service

metadata:
  name: backend

spec:
  type: ClusterIp  <<< c'est le type qu'on vient de voir;
  ports:
    - targetPort: 80 <<< c'est le port final qui sera sur notre pod
      port: 80 <<< ici il s'agit du port de notre service vers lequel on enverra les flux vers le pod ( c'est le même port )
      
      selector: 
        app: myapp   <<<< on va linker notre service aux label des pods hebergeant le service .
        type: backend         
master $ kubectl create -f service-definition.yaml

on peut controller la creation de notre service avec 
kubectl get service


chap2 v16 -> done

- 
tp services ->  done.

master $ kubectl get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   6m    <<<< ce service est crée par default au lancement e kube.

master $ kubectl describe service
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP:                10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.17.0.79:6443
Session Affinity:  None
Events:            <none>


- creation de service de type NodePort pour atteindre depuis le browser via le port 30080 l'appli simple-webapp qui ecoute sur le port 8080
master $ cat service-definition-1.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 8080
      port: 8080
      nodePort: 30080
  selector:
    name: simple-webapp


== chapitre 3 : Scheduling : 

ch3 v2

= Fonctionnement du scheduling :

sur chaque definition de pod un champ : nodeName n'est pas à remplir quand on crée le pod mais  Kube renseigne se champ automatiquement à la creation
Le scheduleur va examiner toutes les definitions de pods et quand il voit le champ nodeName vide : il va alors examiner la conf et déterminer sur quel node il va falloir créer ce pod . 
Le nom du node va donc etre associer au nodeName de la definition de notre pod 
ex : 
nodeName : node2
S'il n'y a pas de sheduleur pour gérer les nouveaux pods ceux restent toujours en etat de pending : dans l'attente de l'assignation a un node.

Dans ce cas nous avons plusieurs solutions : 

= Manual scheduling : 

on va pouvoir assigner manuellement un pod a un node.
dans ce cas nous avons juste à renseigner la section nodeName dans la création de notre pod

ex : 


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx
    - Ports: 
        containerPort : 8080

nodeName : node2

Par contre Kube NE nous laissera pas modifier ce champ si l'objet existe déja.
Dans ce cas nous allons devoir créer un objet qui referencera notre pod et ira donc sur le node que nous désirons : ce nouvel object est de type Binding.

cat pod-bind-definition.yaml

apiVersion: v1
kind: Binding

metadata:
  name: nginx
target: 
  apiVersion: v1
  kind: Node
  name: node2

on va ensuite envoyer en requete POST a l'api de binding notre définition d'objet :
on va convertir notre yaml en json : 
curl -H "Content-type: application/json" -X POST -D '{"apiVersion: v1, kind: Binding ......}' https://server/api/v1/namespaces/default/pods/$podname/binding

tp manual shedule -> done 

master $ cat nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginxmaster
master $ kubectl create -f nginx.yaml
pod/nginx created
master $ kubectl get pods
NAME      READY     STATUS    RESTARTS   AGE
nginx     0/1       Pending   0          3s

master $ kubectl get pods --all-namespaces
NAMESPACE     NAME                             READY     STATUS    RESTARTS   AGE
default       nginx                            0/1       Pending   0          1m
kube-system   coredns-78fcdf6894-7zbqs         1/1       Running   0          6m
kube-system   coredns-78fcdf6894-c9zll         1/1       Running   0          6m
kube-system   etcd-master                      1/1       Running   0          5m
kube-system   kube-apiserver-master            1/1       Running   0          5m
kube-system   kube-controller-manager-master   1/1       Running   0          5m
kube-system   kube-proxy-fdlnz                 1/1       Running   0          6m
kube-system   kube-proxy-wgccj                 1/1       Running   0          6m
kube-system   weave-net-9mhgm                  2/2       Running   1          6m
kube-system   weave-net-wqfwn                  2/2       Running   1          6m

on voit qu'il n'y a pas de scheduler 
on va detruire le pod puis le recreer en inserant dans la defintion le nodeName que l'on veut : 

cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01



master $ vi nginx.yaml
master $ kubectl create -f nginx.yaml
pod/nginx created
master $ kubectl get pod
NAME      READY     STATUS    RESTARTS   AGE
nginx     1/1       Running   0          11s
master $ kubectl describe pod nginx
Name:               nginx
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               node01/172.17.0.73
Start Time:         Sun, 05 May 2019 17:57:28 +0000
Labels:             <none>
Annotations:        <none>
Status:             Running
IP:                 10.32.0.2
Containers:
  nginx:
    Container ID:   docker://e990f90c76b0712ddfcba7632c047b3158e24c2e2b23de406b593b8cfd925234
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:e71b1bf4281f25533cf15e6e5f9be4dac74d2328152edf7ecde23abc54e16c1c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Sun, 05 May 2019 17:57:37 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-89rxh (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-89rxh:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-89rxh
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason   Age   From             Message
  ----    ------   ----  ----             -------
  Normal  Pulling  22s   kubelet, node01  pulling image "nginx"
  Normal  Pulled   15s   kubelet, node01  Successfully pulled image "nginx"
  Normal  Created  14s   kubelet, node01  Created container
  Normal  Started  14s   kubelet, node01  Started container


= Labels et selectors : 

 Les labels et les selectors sont des methodes pour grouper les choses entre elles.
 On veut etre capable de filtrer en fonctions de group mais également en fonction de certains critères.
 ex : tous les animaux verts
 tous les animaux de type oiseaux verts
 Les labels sont alors ideaux . Les labels sont des propriétés attachees a des items 
ex : 
class : mammal
kind : domestic 
color : green

class : reptile
kind : wild 
color : purple

on trouve les labels dans plein de contexte web : sur les blogs, dans les boutiques on line ...
  
- utilisation des labels et selectors dans kube : 

on crée plein de differents objects : pods, deployment, replicaset, service ....
avec le temps on peu donc avoir une multitude d'objet dans notre cluster.
on doit donc etre capable de voir nos différents objects par catégories.
on peut grouper les objects par leur type ( pods ..) , leur application ( my-app1 ), leut fonctionnalité ( frontend )
chaque object attache les labels selon nos besoins.

On va donc créer dansla section metadata les labels que l'on veut sans limitation de nombre :

cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
    function: web 
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01


Le selector va utiliser des labels pour filtrer nos objects : 
selectors :
  app: app1
  function: front-end 

le selector sera ici app=app1 par exemple.

Un fois l'object créer on va pourvoir selectionner en fonction de critère passés en arguments au selector :


kubectl get pods --selector app=app1 

kube utilise les labels et selectors en interne pour grouper les ressources.

ex dans un replicaset on va definir les pods utilisés via leur labels 

Attention on a dans ce cas 2 labels :

le premier en debut de definition concerne le replicaset lui meme alors que le second defini dans la section du template concerne les pods.  --> Il faut bien faire attention c'est une erreur classique.
Dans notre cas actuellement les labels de replicaset  ne sont pas utilisés : nous verrons plus tard que ceci peut être utile pour la decouverte auto de replicaset.

Afin de connecter le replicaset au pod : on va definir dans la conf de notre replicaset un matchingLabels qui va comporter une entrée qui va correspondre a une definition de label dans la section de notre pod :


definition de replicaset a revoir pour l'exemple de correspondance matchlabels repliscaset et label pod sur entrée app: app1

kind: ReplicaSet      
metadata:
  name: simple-webapp
  labels:
    app: app1
    type: front-end
spec:
  replicas: 3
  selector:        
    matchLabels:
      type: app1
  - template:
     metadata:
       name: myapp-pod
       labels:
         app: app1
         type: front-end
     spec:
       containers:
         - name: simple-webapp
           image: simple-webapp


C'est le même principe pour les differents objects kube

ex : pour les services :
quand un service est créer il va utiliser  l'entrée defini dans le selector pour matcher avec l'entree du label du pod defini dans le replicaset.

On peut rajouter une autre caracteristique en plus des labels et selectors : les annotations .
Cela va anous permettre de donner des infos supplementaires dans la description de nos pods ( numero de version , adresse mail ...)


kind: ReplicaSet      
metadata:
  name: simple-webapp
  labels:
    app: app1
    type: front-end
  annotations:
    buildversion: 1.34

ch 3 v3 -> done


ch3 tp labels /selector -> done 

aster $ kubectl get pods --selector env=dev
NAME          READY     STATUS    RESTARTS   AGE
app-1-g9k8r   1/1       Running   0          1m
app-1-t7qcs   1/1       Running   0          1m
app-1-zhbdm   1/1       Running   0          1m
db-1-2vgkv    1/1       Running   0          1m
db-1-7678l    1/1       Running   0          1m
db-1-fj5rm    1/1       Running   0          1m
db-1-z6hbq    1/1       Running   0          1m
master $ kubectl get pods --selector bu=finance
NAME          READY     STATUS    RESTARTS   AGE
app-1-g9k8r   1/1       Running   0          1m
app-1-t7qcs   1/1       Running   0          1m
app-1-zhbdm   1/1       Running   0          1m
app-1-zzxdf   1/1       Running   0          1m
auth          1/1       Running   0          1m
db-2-sdnz9    1/1       Running   0          1m
master $ kubectl get all --selector env=prod
NAME              READY     STATUS    RESTARTS   AGE
pod/app-1-zzxdf   1/1       Running   0          2m
pod/app-2-cfnth   1/1       Running   0          2m
pod/auth          1/1       Running   0          2m
pod/db-2-sdnz9    1/1       Running   0          2m

kubectl get all --selector env=prod,bu=finance,tier=frontend
NAME              READY     STATUS    RESTARTS   AGE
pod/app-1-zzxdf   1/1       Running   0          3m

cat replicaset-definition-1.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend   <<<< il faut faire matcher le label de notre replicatset avec celui des pods
  template:
    metadata:
      labels:
        tier: frontend   <<<< label des pods
    spec:
      containers:
      - name: nginx
        image: nginx



= Ressources limits : 

comme on l'a vu le scheduleur va s'occuper de la creation et la repartition des pods sur les différents nodes, en fonction des ressources nécéssaires pour le pod et des ressources dispos sur les nodes.

Si un pod demande des ressources non dispos alors il reste en status pending et on a une info spécifique au pb : ..insufficiant cpu  ....etc .

kube de base considère qu'un container  de base a besoin de :

0.5 cpu 
256M ram 
Si de base on sait que notre appli va consommer plus , il suffit de déclarer dans la définition les valeurs nécéssaires.


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
    function: web 
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01

  ressources: 
    requests:
      memory: "1Gb"
      cpu: 1

- cpu :
L'unite 1 cpu correspond a :
1 vcpu
1 thread
on peut mettre 100m et decendre jusqu'a 0.1m qui veut dire milli.
on n'est pas obligé de mettre des multiples de 0.5 ou 100m 

- memory :
on peut mettre les valeurs que l'on veut avec le suffixes qu'on veut mais  il s'agit de Mi Gi ayant pour base 1k = 1024bytes 
contrairement a 1K = 1000bytes pour Mo et Go

Dans le monde de docker ..il n'y a pas de limite et le container peut consumer les ressources qu'il veut sur le node.
De base kube fixe des limites pour les containers afin de preserver les nodes :

1vcpu par container
512mi de ram 

On va pouvoir definir des limits à nos container dans la definition de notre pod 

le container ne pourra pas depasser le cpu defini mais il pourra depasser la memoire jusqu'a crasher ....


cat myapp-pod.yaml
---
apiVersion: v1
kind: Pod

metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
    function: web 
spec:
  containers:
    - name: nginx-container
      image: nginx
  nodeName: node01

  ressources:                    <<<<< definition d'une section de resources: on defini le besoin de notre pod 
    requests:
      memory: "1Gi"
      cpu: 1
    limits:                     <<<<<< definition des limites de notre container : limites fixées manuellement pour contenir les eventuels debordements du container
      memory: "2Gi"
      cpu: 2


ch 3 v4 -> done


tp ressources -> done 

creation d'un pod avec ressources cpu : 


Attention à l'indentation -> le yaml ici est correct.

master $ cat lion.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: lion
spec:
  containers:
  - name: lion
    image: vish/stress
    resources:
       requests:
        cpu: 2
      

cat elephant.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: elephant
spec:
  containers:
  - name: elephant
    image: polinux/stress
    resources:
       limits:
        memory: "20Mi"
       requests:
        memory: "15Mi"
        cpu: 2

= Daemon sets : 

Le daemonset va permettre comme le replicaset de s'assurer du deploiement correct des pods définis.
A la différence du replicaset , le daemonset va déployer un pod apres sa creation  sur CHAQUE node du cluster.
Il s'assure qu'une copie de chaque pods et toujours presentes sur tous les nodes du cluster.
Chaque nouveau node va donc recevoir les pods définis dans notre daemonset.

Un exemple parfait d'utilisation est l'ajout de monitoring et de logger pour le cluster.
On va deployer ses agents sous formes de pod et déployer sur chaque node du cluster via un daemonset pour s'assurer du bon fonctionnement de notre cluster.
Dans ce cas on n'a pas besoin de s'occuper de retirer / rajouter des agents de log ou monito sur un node qui sort du cluster ou un nouveau node qui rentre dans le cluster.
Un second exemple d'utilisation du daemonset est le kube-proxy qui va être deployer sous forme de pod sur chaque noeud de notre cluster en daemonset.
Une solution de networking comme weave nécéssite le deployment de conf sur chaque node : le daemonset est la encore conseillé.



La definition du daemonset est identique au replicaset :
on a les pods du daemonset definis et imbriquer dans la section template, on a un selector défini pour matcher ces pods.

cat daemonset-definition.yaml 
apiVersion: apps/v1  
kind: DeaemonSet      
metadata:
  name: monitoring-daemon
spec:
  selector:         
    matchLabels:
      app: monitoring-agent
  - template:
      metadata:
        labels:
          app: monitoring-agent
      spec:
        containers:
          - name: monitoring-agent
            image: monitoring-agent

on doit s'assurer que le matchLabels de notre selector correspond au label defini dans le template de nos pods.            


kubectl create -f daemonset-definition.yaml

kubectl get daemonsets

kubectl describe daemonsets our_daemonset_name

Fonctionnement :

on peut definir manuellement comme on l'a vu l'affectation de pod à un node  

-> c'etait comme cela avant kubernetes v1.12

Depuis le daemonsets utilise le scheduleur par defaut et les nodes affinity.


ch 3 v5 -> done


tp daemonsets -> done 

master $ kubectl get daemonsets --all-namespaces
NAMESPACE     NAME         DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR                   AGE
kube-system   kube-proxy   2         2         2         2            2           beta.kubernetes.io/arch=amd64   3m
kube-system   weave-net    2         2         2         2            2           <none>

master $ kubectl describe daemonset kube-proxy --namespace=kube-system
Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  beta.kubernetes.io/arch=amd64
Labels:         k8s-app=kube-proxy
Annotations:    <none>
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy-amd64:v1.11.3
    Port:       <none>
    Host Port:  <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
    Environment:  <none>
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  5m    daemonset-controller  Created pod: kube-proxy-q2csm
  Normal  SuccessfulCreate  5m    daemonset-controller  Created pod: kube-proxy-dssqz

creation d'un daemonset dont on défini le nom "elasticsearch" , le namespace dans le quel il evolue : kube-system et avec une image : k8s.gcr.io/elasticsearch:1.20

master $ cat fluend-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
     metadata:
       labels:
         app: elasticsearch
     spec:
       containers:
         - name: elasticsearch
           image: k8s.gcr.io/elasticsearch:1.20



= multiple schedulers : =

On a vu que le sheduling va dependre de plusieurs criteres que l'on peut avoir défini : ( node affinity , ressources ..).
Il est possible d'avoir des besoins complexes avec par exemple une appli qui devra avoir ses pods placés sur des nodes après des tests additionnels (??)
Que ce passe il quand aucune des specificité ne sont disponibles ?

On va pouvoir définir notre propre sheduleur.
Dans ce cas les applications normales utiliseront le scheduleur par defaut et notre application scpecifique elle sera gérée par notre scheduleur personel.: 
le cluster kube peut utiliser en même temps plusieurs scheduleurs.

Quand on crée un pod ou un deployement on va pouvoir definir le scheduleur qui va être utilisé.
On peut downloader le binaire du sheduleur sur le site de kube.
Si on ne precise rien : se sera un sheduleur par defaut qui se lancera  comme un service (linux)

  --sheduleur-name=default
on peut déployer un scheduleur perso en utilisant le même bianaire mais en specifiant un nom :

 ... 
  --sheduleur-name=my-own-scheduler

Il va être nécéssaire de bien identifié le nom du scheduleur

ex : kubeadm deploi le scheduleur sous forme de pod ( pas comme un service ) 
on peut s'inspirer de la conf de kubeadm pour créer la definition de notre scheduler 

il faudra biensur s'assurer de bien renseigner son nom et aussi de rajouter la section : --leader-elect: true
qui sert a s'assurer qu'un seul master est le sheduler dans le cas d'un cluster multi master ( HA )

Un seul scheduleur doit être actif sur un master : il faut faire une election de leader.
ex : 
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: custom-scheduler
    namespace: kube-system
spec:
  containers:
    - command:
    - /usr/local/bin/kube-scheduler
    - --address=127.0.0.1
    - --leader-elect=true
    - --scheduler-name=custom-scheduler
    image: g1g1/custom-kube-scheduler:1.0
    name: kube-second-scheduler

On va pouvoir examiner une fois que l'on lance notre pods qu'il tourne bien . Une fois que notre sheduleur est up , on va definir les pods qui vont l'utiliser.
dans la définition du pod on va definir le scheduleur crée au prealable: 
cat elephant.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: elephant
spec:
  containers:
  - name: elephant
    image: polinux/stress
    resources:
       limits:
        memory: "20Mi"
       requests:
        memory: "15Mi"
        cpu: 2
  schedulerName: custom-scheduler    <<< on defini explicitement le scheduleur qui va être utilisé pour notre pod.

  On crée notre popd et on le lance : il doit être en runnning 


- Comment identifier quel sheduler est  utilisé ?

on va utiliser la gestion des events : 

kubectl get events : va lister les evenement s dans le namespace courant.
on peut voir dans notre cas apres la creation du pods qu'il y a une section SOURCE qui indique les scheduleurs en charge de l'event : ici la creation du pod est bien dépendante de notre scheduleur personalisé.


on va egalement pouvoir examiner les logs :

kubectl logs custom-scheduler --namespace=kube-system

ch3 v6 ->  done 

tp scheduler -> done

creation d'un scheduler :

nom :my-sheduler 


master $ cat /var/answers/my-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  creationTimestamp: null
  labels:
    component: my-scheduler
    tier: control-plane
  name: my-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=false
    - --port=10282
    - --scheduler-name=my-scheduler
    image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10282
        scheme: HTTP
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
status: {}


creation d'un pod rattaché au sheduler crée :

master $ cat nginx-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginx
  schedulerName: my-scheduler


Pour résumer :
on peut installer / gérer un scheduleur :

-> en service linux
-> en pod comme kubeadm

references supplémentaires pour scheduling .

ch3 v7 -> en cours

== Logging /monitoring kube cluster : ==


= monitoring : 

-> nodes level
-> pods level 
etc ...

des solutions existent pour monitorer le cluster kube : prometheus, elasticstac , datadog ..
Une solution native existait mais est maintenant déprecated : heapster 

on va pouvoir cependant recupérer les métriques des nodes et des pods , les aggréger et les stocker en mémoire.
On devra cependant utiliser une solution externe pour stocker les data.

Les metrics sont récoltées par un sous composants de kubelet sur les nodes. cet agent se nomme CAdvisor ( container advisor ) : cet agent est responsable de la récupération des métriques et de leur envoi à kubelet qui va les transférer au server de metrique final.

on peut tester sur minikube :

minkube addons enable metrics-server
sinon on peut recupérer le code :

git clone https://github.com/kubernetes-incubator/metrics-server


https://github.com/kodekloudhub/kubernetes-metrics-server.git

kubectl create -f deploy/1.8+/

kubectl top node
kubectl top pod

tp monitoring -> done

= logging : 

comme pour docker on va pouvoir examiner les logs de nos pods :

kubectl logs my_pod
kubectl logs -f my_pod   <<<< -f pour voir les logs défiler en continu.

Si on a plusieurs container dans notre pod on va devoir nommer explicitement le container dont on veut voir les logs .
ex: 

kubectl logs -f my_pod my_container1_name 

tp logging -> done


== Application life cycle managment : ==


= rolling update and rollback : =

- rollout :

quand on crée un deployment , cela trigger un nouveau rollout.
Ce rollout crée un nouveau num de revision pour le deploiement.
ex : 
on cree un deploiement pour des images nginx:1.7.0 , cela trigger un rollout qui va générer un num de revision pour le deploiement: ex :revision 1
Dans le futur si on upgrade une nouvelle version de container avec des images updatées nginx:1.7.1 : on aura apres le deploiement un rollout qui créera un num de recision : ex : revision 2
Cela permet de garder un historique des déploiements et de faciliter le rollback si besoin.

On peut voir le status de notre rollout avec :
kubectl rollout status deployment/my-app-deploy

on peut voir l'historique de nos déployments avec :
kubectl rollout history deployment/my-app-deploy

- strategy de deployement :

il y a deux types de stratégie de déploiement :

- recreate :
dans cette strategie : on va detruire tous les pods de notre deploiement puis les recreer avec la nouvelle version.
Biensur : il y a une interruption de service pendant ce temps.

- rolling-update :
dans cette deuxieme strategie , on va detruire et recréer tous les pods un par un :
si on a un deployement de 4 pods nginx :
on detruit le premier pod puis on créer le premier pod nouvelle version, on fait pareil avec tous les autres pods.
Dans ce cas il n'y a aucune coupure de service et le deploiement est seamless.
Cette stratégie est celle utilisée par default dans kubernetes.
Si on ne precise rien dans notre deployement : cette stratégie sera utilisée par defaut.


update : on peut avoir beaucoup de parametres qu'on peut updaté : num d'application, numbre de replicat, images docker etc ...

on peut renseigner les changements dans notre fichier de déploiement et mettre a jour avec :
kubectl apply -f definition-deployment.yaml

on peut editer notre deployment : il sera pris à chaud 

on peut faire un update a chaud :
ex: 
kubectl set image deployment/my-app-deploy nginx=nginx:1.9.1 
Attention on aura donc une différence avec le fichier de déploiment qui aura la version de l'image inchangée.

On peut voir le detail du deploiment avec :

kubectl describe deployment my-app-deploy 
on peut bien voir le champ :
StrategyType: Recreate ou RollingUpdate 

on peut voir les différences de deploiement :
dans un cas tous est detruit puis recréer , dans le second les operations se font une a une pour chaque pod.

upgrade : 
fonctionnement interne :
dans le cas d'un nouveau deployment :
le deployment va crée un replicat set avec le nombre de pod necessaire
dans le cas d'une modif de deployment :
un nouveau replicatset est créer et le premier pod du premier replicaset est detruit, le premier pod du nouveau replicat set est crée etc ..

on peut voir le  resultat en consultant :
kubectl get replicasets
on voit le premier replicatset avec 0 pod et le second avec le nombre de pod definit dans notre deployement.

- Rollback :
on va pouvoir revenir a une version anterieure :

kubectl rollout undo deployment/my-app-deploy




tp ch5 rolling-update rollback -> done


on examine un deployment , on modifie l'image utilisée ( kubectl edit deployment frontend)

on voit ensuite la modification : on observe le shutdown des anciens pods et l'allumage des nouveaux pods avec nouvelle image qui se fait de maniere incrémentale 


master $ kubectl describe deployments.
Name:                   frontend
Namespace:              default
CreationTimestamp:      Sun, 19 May 2019 18:00:33 +0000
Labels:                 name=webapp
Annotations:            deployment.kubernetes.io/revision=2
Selector:               name=webapp
Replicas:               4 desired | 4 updated | 5 total | 3 available | 2 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        20
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp
  Containers:
   simple-webapp:
    Image:        kodekloud/webapp-color:v2
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  frontend-7965b86db7 (1/1 replicas created)
NewReplicaSet:   frontend-65998dcfd8 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  10m   deployment-controller  Scaled up replica set frontend-7965b86db7 to 4
  Normal  ScalingReplicaSet  25s   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 1
  Normal  ScalingReplicaSet  25s   deployment-controller  Scaled down replica set frontend-7965b86db7 to 3
  Normal  ScalingReplicaSet  25s   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 2
  Normal  ScalingReplicaSet  0s    deployment-controller  Scaled down replica set frontend-7965b86db7 to 1
  Normal  ScalingReplicaSet  0s    deployment-controller  Scaled up replica set frontend-65998dcfd8 to 4
master $ kubectl describe deployments.
Name:                   frontend
Namespace:              default
CreationTimestamp:      Sun, 19 May 2019 18:00:33 +0000
Labels:                 name=webapp
Annotations:            deployment.kubernetes.io/revision=2
Selector:               name=webapp
Replicas:               4 desired | 4 updated | 5 total | 3 available | 2 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        20
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp
  Containers:
   simple-webapp:
    Image:        kodekloud/webapp-color:v2
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  frontend-7965b86db7 (1/1 replicas created)
NewReplicaSet:   frontend-65998dcfd8 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  10m   deployment-controller  Scaled up replica set frontend-7965b86db7 to 4
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 1
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled down replica set frontend-7965b86db7 to 3
  Normal  ScalingReplicaSet  28s   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 2
  Normal  ScalingReplicaSet  3s    deployment-controller  Scaled down replica set frontend-7965b86db7 to 1
  Normal  ScalingReplicaSet  3s    deployment-controller  Scaled up replica set frontend-65998dcfd8 to 4

 
-script pour vérifier la version des pods déployés apres un update de deployment : 

master $ cat curl-test.sh
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""

Hello, Application Version: v1 ; Color: blue OK

master $ cat curl-test.sh
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done



en changeant la methode de déployement a recreate on a : 

master $ kubectl describe  deployments.
Name:               frontend
Namespace:          default
CreationTimestamp:  Sun, 19 May 2019 18:00:33 +0000
Labels:             name=webapp
Annotations:        deployment.kubernetes.io/revision=3
Selector:           name=webapp
Replicas:           4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:       Recreate
MinReadySeconds:    20
Pod Template:
  Labels:  name=webapp
  Containers:
   simple-webapp:
    Image:        kodekloud/webapp-color:v3
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   frontend-5c858cd557 (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  34m   deployment-controller  Scaled up replica set frontend-7965b86db7 to 4
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 1
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled down replica set frontend-7965b86db7 to 3
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 2
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled down replica set frontend-7965b86db7 to 1
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled up replica set frontend-65998dcfd8 to 4
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set frontend-7965b86db7 to 0
  Normal  ScalingReplicaSet  7m    deployment-controller  Scaled down replica set frontend-65998dcfd8 to 0
  Normal  ScalingReplicaSet  6m    deployment-controller  Scaled up replica set frontend-5c858cd557 to 4


on va pouvoir voir les différents déployements : 

master $ kubectl rollout history deployment/frontend
deployments "frontend"
REVISION  CHANGE-CAUSE
1         <none>
2         <none>
3         <none>

on peut executer un rollback : 
master $ kubectl rollout undo deployment/frontend
deployment.extensions/frontend


= Commands and arguments dans une définition de pod :


- memo sur les commandes et arguments dans docker :

quand on lance un container simplement avec docker :
docker run ubuntu
et qu'on liste les container up 
docker container ls 

il n'y a pas de process actif : le container s'est lancé puis s'est arrêté.
avec un docker ps -a : on peut voir que le container est en status exited.
les containers sont fait pour faire tourner une tache ou un process . contrairement a une vm qui fait tourner un os.
un container est up uniquement tant que le process a l'interrieur de celui ci est up.
ex : si un server web crash alors le container qui l'héberge se ferme.
On peut facilement voir la commande qui est executée dans notre container : en editant le Dockerfile de celui-ci on voit le param :
ex : pour un container nginx :
# default command :
CMD ["nginx"]

ex: pour un container hébergeant mysql :
#default command :
CMD ["mysqld"]

Quand on lance un container faisant tourner ubuntu on peut voir que la commande est :

CMD ["bash"] 
la commande bash n'est pas comme un serveur web.
Bash attend une commande sur un terminal ..s'il ne trouve pas de terminal alors il se ferme.
par defaut docker n'attache pas de terminal a un container quand il est lancé.
donc bash ne trouve pas de terminal ..il se ferme donc 

On peut ajouter une commande qui va overrider  la commande spécifiée dans le dockerfile.

ex :
docker run ubuntu sleep 5 <<<< dans ce cas  la commande sleep sera prise en compte la commande bash sera remplacée et le container sera up pendant 5 secondes.

On peut overrider en ligne de commande mais biensur on peut rendre se changement permanent :

on peut modifier la commande bash et la remplacer par sleep dans le dockerfile.

CMD sleep 5
2 formes sont possibles :
- forme simple :CMD command, param1 -> CMD sleep 5
- format json CMD ["command", "param1"]  -> CMD ["sleep","5"]

attention dans le format json le premier element doit etre un executable et tous les élements doivent etre séparés dans la liste :
CMD ["sleep 5"] --<< ne fonctionne PAS

on peut maintenant builder notre nouvelle image :

docker build -t ubuntu-sleeper .
docker run ubuntu-sleeper

pour augmenter le delai passer de 5 à 10 secondes on peut modifier en cli : docker run ubuntu-sleeper sleep 10 mais comme son nom l'indique ubuntu-sleeper doit sleep ..;donc rajouter la commande en argumant n'est pas génial .

on voudrait juste passer en argument le nbr de secondes pendant lequel le container sleep :
docker run ubuntu-sleeper 10 

pour cela on va utiliser la directive "entrypoint" 
cette instruction ENTRYPOINT est commande CMD on va donc pouvoir juste saisir notre nombre de secondes.
CMD ENTRYPOINT ["sleep"]
docker run ubuntu-sleeper 10 

CMD peut être completement overrider par les parametres passés en cli 
ENTRYPOINT les arguments de la ligne de commande sont ajoutés à l'entrypoint 

si on lance le container avec l'entrypoint sleep sans argumant on a une erreur :
c'est normal car sleep a besoin d'argument.

On va donc pouvoir donner une valeur par default pour se faire on va cumuler les deux directives :


FROM ubuntu
...
...
ENTRYPOINT ["sleep"]
CMD ["5"]
dans se cas si on lance la commande docker run ubuntu-sleeper sans argumant par defaut le container prendra 5 secondes comme valeur
si on passe un argument alors celui-ci ovverridera la valeur contenu dans la directive CMD 

ex : docker run ubuntu-sleeper 10

Il est cependant si besoin possible d'overrider l'entrypoint en cli en utilisant l'argumant --entrypoint 

docker run ubuntu-sleeper --entrypoint sleep.2 5 

ch5 v3 -> done 

Todo tp ch5 -> commandes args

- commands et arguments dans kube :

on a vu qu'on pouvait créer avec docker un container qui se lancerait et pendant 5 secondes ne ferait rien
docker run --name ubuntu-sleeper ubuntu-sleeper
on a vu qu'on pouvait overrider les commandes et arguments pour le lancer 10 secondes : 
docker run --name ubuntu-sleeper ubuntu-sleeper 10

on va pouvoir créer un pod ayant le même comportement : 
et on va pouvoir passer les argumants overridant le comportement par defaut du pod dans la section args de notre container : 
pod-def.yaml

apiVersion: v1 
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
specs:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      args: ["10"]

kubectl create -f pod-def.yaml

 c'est comme si on overridait la partie CMD dans le dockerfile :

 From ubuntu
 ..
 ENTRYPOINT ["sleep"]
 CMD ["5"]

 Comment faire si on veut overrider l'entrypoint ? 
 dans ce cas on va renseigner le champ command dans la def de notre pod: 



pod-def.yaml

apiVersion: v1 
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
specs:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ["sleepv2"]
      args: ["10"]
Attention donc on a 

ENTRYPOINT : docker -> command: kubernetes
CMD: docker         -> args: kubernetes


ch5 v4 -> done

tp ch5 command args

master $ cat ubuntu-sleeper-2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-2
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command: ["sleep"]
    args: ["5000"]
master $


Il est possible de passer plusieurs commandes à la suite sous forme de tableau dans la directive commandes :

master $ cat ubuntu-sleeper-3.yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-3
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "1200"

      
ici on voit que la commande qui sera executé au demarrage du container sera:  "--color","green" : car la valeur déclarée dans le pod va ovverider ce qui est defini dans le dockerfile.

- dockerfile : 
master $ cat /root/webapp-color-2/Dockerfile2
FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]

CMD ["--color", "red"]

-pod : 
master $ cat /root/webapp-color-2/webapp-color-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["--color","green"]


= Variable d'environment dans kube : =

on va pouvoir specifier dans la definition de notre pod des variables.
La section commencera par env qui est lui même suivi de tableau :chaque élément est compris dans le tableau :
chaque item du tableau a un nom et une valeur

apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    ports: 
      - containerport: 8080
    command: ["watch -n2 "]
    env: 
      - name: APP_COLOR
        value: green

On va pouvoir configurer nos variables autrement qu'en associant des clés valeur dans notre pod.
On peut utiliser des configMap et des secrets 

- configMap :

    env: 
      - name: APP_COLOR
        valueFrom:
          configMapKeyRef:


- secret :           

    env: 
      - name: APP_COLOR
        valueFrom:
          secretKeyRef: 

-> ch5 v5 -> done

=  configMap :

on a vu qu'on pouvait setter des variables d'environment dans la defintion de notre pod. 
Quand on a beaucoup de definition de pod cela devient compliquer de gérer les variables au sein de multiples fichiers.

on va donc externaliser les differentes variables au sein d'un fichier pour centraliser nos données.
Ce fichier peut être un configMap 

configMap est un fichier comportant des clés /valeurs.
on va donc dans la definition de notre pod faire matcher les entrées de nos configmap dans les caracteriqtiques de notre pod .

On va donc créer notre configmap puis l'injecter dans notre pod.

ex: 
ConfigMap 

APP_COLOR: blue
APP_MODE: prod

- creation de configmap : 

on va avoir deux methodes pour créere notre configmap :
-> impératif : sans fichier de config :
kubectl create configmap  

-> declaratif : avec un fichier de definition.
kubectl create -f 


- imperatif :

kubectl create configmap  
  <config-name> --from-litteral=<key>=<value>
  app-config --from-litteral=APP_COLOR=blue

l'utilisation de --from-litteral permet de passer directement en cli les clés /valeurs
on peut ajouter autant de clés /valeurs que l'on veut en rajoutant une ligne -from-litteral suivie des clés/ valeurs: 



kubectl create configmap  
  app-config --from-litteral=APP_COLOR=blue
             --from-litteral=APP_MODE=prod

on peut également charger nos données depuis un fichier :

kubectl create configmap  
  app-config --from-file=app_config.properties

- declaratif :

kubectl create -f configmap-def.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:  <<<< a la place de spec que l'on trouve dans les objects kube habituels  ici on a la definition data
  APP_COLOR: blue
  APP_MODE: prod


kubectl create -f configmap-def.yaml

On peut biensur avoir plusieurs configmap :
mysql-config:

port: 3306
max_allowed-packets: 32M

etc ...

pour examiner les configmap :

kubectl get configmaps

kubectl describe  configmaps

- Declaration dans le pod :

on va maintenant definir notre configmap dans le pod :

on va utiliser des declaration specifique:
envFrom:
  configMapRef:
    name: notre_config_map 

pod-def.yaml

apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    ports: 
      - containerport: 8080
    command: ["watch -n2 "]
    envFrom:            <<<<<<<<  on va donc maintenant rattacher les valeurs definis dans notre configmap au pod 
      - configMapRef:
          name: app-config


kubectl create -f pod-def.yaml

ch5 v6 -> done


tp configmap : 

master $ kubectl get configmaps
NAME        DATA      AGE
db-config   3         14s
master $ kubectl describe configmaps db-config
Name:         db-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
DB_PORT:
----
3306
DB_HOST:
----
SQL01.example.com
DB_NAME:
----
SQL01
Events:  <none>
master $


master $ cat webapp-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config-map
data:
  APP_COLOR: darkblue

master $ kubectl create -f webapp-configmap.yaml
configmap/webapp-config-map created
master $ kubectl get configmaps
NAME                DATA      AGE
db-config           3         5m
webapp-config-map   1         5s


on va maintenant accrocher notre configmap a notre pod :

master $ cat webapp-color.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp-color
  labels:
      name: webapp-color
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    envFrom:
      - configMapRef:
          name: webapp-config-map


tp var / configmap -> done

= secrets : =

comme on l'a vu on peut charger des variables depuis un fichier de type configMap 
mais pour les données sensibles (mdp ..) il faut preserver une securité ..

https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

On va donc dans ce cas utiliser un secret qui est un object qui sert a stocker les données sensibles.

Les data seront hachées et invisibles pour les users.
comme pour les configmap on va créer dans un premier temps les secrets puis injecter la conf dans le pod.

ex: 

secret 

DB_HOST: mysql
DB_USER: root
DB_PASSWORD: psswd

- imperatif :

kubectl create secret generic
  <secret_name> --from-litteral=key=value
  app-secret --from-litteral=DB_HOST=mysql
             --from-litteral=DB_USER=root
             --from-litteral=DB_PASSWORD=psswd

biensur cela commence a etre compliqué quand on a beaucoup de secret a passer 
on va pouvoir passer l'option -from-file qui permetrra de gérer un fichier contenant toutes nos valeurs

kubectl create secret generic
  <secret-name> --from-file=<path-to-file>
  app-secret --from-file=fichier_contenant_nos_secrets


- declaratif :

on va biensur pouvoir utiliser un fichier pour créer notre object en mode déclaratif :

secret-data.yaml

apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data: 
  DB_HOST: mysql
  DB_USER: root
  DB_PASSWORD: psswd

kubectl create -f  secret-data.yaml
  
Biensur ici on voit que les données sont en clair se qui n'est biensur pas tolérable.
On va donc devoir présenter les valeurs dans un format hashé :

on pourra par exemple utiliser la base64 :
echo "mysql" |base64
bXlzcWwK
echo "root" |base64
cm9vdAo=
echo "psswd" |base64
cHNzd2QK

On pourra donc facilement injecter ces datas hashé dans la conf de notre object :


apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_HOST: bXlzcWwK
  DB_USER: cm9vdAo=
  DB_PASSWORD: cHNzd2QK

pour lister les secrets :

kubectl get secrets

Kube en crée naturellement pour le fonctionnement interne du cluster.

kubectl describe secrets permet la description mais les data sont cachées 

on peut voir les valeurs des secrets avec :

kubectl get secrets app-secret -o yaml

pour decoder les valeurs hachées on va utiliser la méthode inverse :

echo bXlzcWwK |base64 --decode
mysql
echo cm9vdAo= |base64 --decode
root
echo cHNzd2QK |base64 --decode
psswd

on va maintenant configurer notre secret dans la definition du pod 


pod-def.yaml

apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    ports: 
      - containerport: 8080
    command: ["watch -n2 "]
    envFrom:            <<<<<<<<  on va donc maintenant rattacher les valeurs definis dans notre configmap au pod 
      - secretRef:
        name: app-secret

on va pouvoir créer notre pod et les data seront donc présentes         
kubectl create -f pod-def.yaml


On peut créer des secrets :

-> en env

envFrom:
  - secretRef:
    name: app-secret

-> en single env

env:
  - name: DB_PASSWORD
    valueFrom:
      secretkeyRef:
        - name: app-secret
        key: DB_PASSWORD

-> dans un fichier qui sera monté dans un volume :

volumes:
  - name: volume-app-secret
    secret:
      secretName: app-secret 
      
ch 5 v7 -> done 


"notes : 
Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.
The concept of safety of the Secrets is a bit confusing in Kubernetes. The kubernetes documentation page and a lot of blogs out there refer to secrets as a "safer option" to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. In my opinion it's not the secret itself that is safe, it is the practices around it.
Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:
Not checking-in secret object definition files to source code repositories.
Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD.
Also the way kubernetes handles secrets. Such as:
A secret is only sent to a node if a pod on that node requires it.
Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.
Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.
Read about the protections and risks of using secrets here
Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault."


tp secrets :


master $ kubectl get secrets
NAME                  TYPE                                  DATA      AGE
default-token-mn8z6   kubernetes.io/service-account-token   3         7m

on a 3 secrets dans l'objet secret default-token-mn8z6

master $ kubectl describe secrets default-token-mn8z6
Name:         default-token-mn8z6
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=default
              kubernetes.io/service-account.uid=4d9de3b5-7ece-11e9-99f1-0242ac11004b

Type:  kubernetes.io/service-account-token      <<<<<<<< type de secret utilisé 

Data
====
ca.crt:     1025 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tbW44ejYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjRkOWRlM2I1LTdlY2UtMTFlOS05OWYxLTAyNDJhYzExMDA0YiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.FPP7-jwZsweSyn8tEps3PTJBgIGND4MliO4AwiP_Sj8yl2Xcb6mjO1eCpkD5hz-_taJNIVLvIrSm5W9oyNAzp1DL2_UChcFDb82Y8y-u0NZEFbCOvJTt1hcdOA_Idj4f8iELaCxxJwuuHTGWcIzjv952g7hB_09otZiRUxVcXKWvVQ0KObOvziOvelWRI14tOzosijebtywrUnt_6A41A5-xa1ULXaXfapOKrBzOPe5ntj1hfMzOQkq31WHkaLML5i5gee952klRelCAj69E9N16RMnO3qudC5qVGwpXKQKwUsXMPg4KEguqOPcaL9d3x8L6sye6OVLRK77n04Q4Ew

une appli est déployée : 

master $ kubectl get pods
NAME         READY     STATUS    RESTARTS   AGE
mysql        1/1       Running   0          32s
webapp-pod   1/1       Running   0          33s
master $ kubectl get service
NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes       ClusterIP   10.96.0.1       <none>        443/TCP          12m
sql01            ClusterIP   10.100.100.76   <none>        3306/TCP         36s
webapp-service   NodePort    10.96.171.216   <none>        8080:30080/TCP   36s
master $ kubectl get secrets
NAME                  TYPE                                  DATA      AGE
default-token-mn8z6   kubernetes.io/service-account-token   3         12m


creation d'un secret en litteral : 

master $ kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123
secret/db-secret created
master $ kubectl get secrets db-secret -o yaml
apiVersion: v1
data:
  DB_Host: c3FsMDE=
  DB_Password: cGFzc3dvcmQxMjM=
  DB_User: cm9vdA==
kind: Secret
metadata:
  creationTimestamp: 2019-05-25T09:37:22Z
  name: db-secret
  namespace: default
  resourceVersion: "1814"
  selfLink: /api/v1/namespaces/default/secrets/db-secret
  uid: b24325ca-7ed0-11e9-99f1-0242ac11004b
type: Opaque



master $ cat webapp-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp-pod
spec:
  containers:
  - name: webapp-pod
    image: kodekloud/simple-webapp-mysql
    envFrom:
      - secretRef:
          name: db-secret


tp secret -> done          


== Readiness  probe / monitoring concepts : =

on a vu qu'un pod a un status et des conditions.

ex : 
-pending (quand le sheduler va s'occupé du dispatch ..)
- quand le pod est dispatché sur un node on est dans l'etat containercreating 
- quand le container est up le pod passe en running


les conditions competent le pod status 

ce sont des tableaux true /false sur le status des pods :

PodScheduled  -> true / false
Initialized   -> true /false 
ContainerReady -> true /false
Ready          -> true /false

on peut voir les conditions d'un pod avec un :

kubectl describe pod 


Le status ready indique que l'application dans le pod est running 
et prete a accepter du traffic user
l'appli peut etre un simple script, une db, un server web ...

Certaines applications peuvent mettre plusieurs minutes à démarrer ...

ex : quand jenkins démarre c'est tres long 
pourtant même si l'application n'est pas completement démarrée le status du pod peut etre en ready ce qui n'est pas complétement vrai.

Comment kube peut savoir si les applications d'un pod sont bien démarrées ou non ?

des que le container est créer kube pense que l'appli est up.

un pod s'il expose via un service un acces aux users sera donc vu up même si l'appli n'est pas démarrée completement.

On doit donc trouver un moyen de vraiment s'assurer que l'appli est dispo 

on va donc builder des tests :

readiness probes : ex http /api test ; tcp test port 3306 ...

On va pouvoir le faire au sein de notre pod :
on fait un set up de sonde qui ne permettra a kube de déclarer le pod en ready que quand la sonde sera ok : 

pod-def.yaml

apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
      name: webapp-green
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    ports: 
      - containerport: 8080
    command: ["watch -n2 "]
    envFrom:            
      - secretRef:
        name: app-secret
    readinessProbe:   <<<<<< definition de notre sonde  
      httpGet:
        path: /api/ready
        port: 8080
        

on a plusieurs type de sondes :


    readinessProbe:   <<<<<< check http  
      httpGet: 
        path: /api/ready
        port: 8080
        

    readinessProbe:   <<<< check tcp 
      tcpSocket:
        port: 3306

    readinessProbe:   <<< execution de script 
      exec:
        command: 
          - cat 
          - /app/is_ready

on peut définir d'autres options :

initialDelaySeconds -> nombre de secondes nécéssaire à l'appli pour être up
periodSeconds -> nombre d'attente entre chaque sonde

readinessProbe:   <<<<<< check http  
  httpGet: 
    path: /api/ready
    port: 8080
  initialDelaySeconds: 10 
  periodSeconds: 5

par défault trois tests seront fait pour assurer que l'appli est up et donc la passer en running mais on peut modifier ce nombre :
avec le param failureThreshold qui va permettre d'indique le nombre de test qui seront fait avant que le status ne passe soit en running soit en failure.
  failureThreshold: 8 

Il existe beaucoup de params : consulter la doc.

Les  readiness probes vont être très utiles dans un set up de multipods :

ex : une appli a deux pods exposant un service par lesquels se connectent les users : si on met un troisieme pod et qu'on l'injecte sans test de vie alors le traffic sera envoyer sur tous les pods même celui qui n'est pas pret ..on aura donc des failures coté users ...si on met une sonde readiness alors on pourra correctement mettre en prod le pod quand il sera vraiment pret.


ch5 v8 -> done 

pas de tp sur les sondes : pas requis sur certif kube admin mais requis sur kube dev application certif

== cluster maintenance ==


= operations d'upgrade sur notre cluster ( os, patch secu ...) : =

Si on perd un node ou qu'on fait une maintenance sur un node ...le service peut etre transparent pour les users sir le deploiement implique des multipod ..si on a qu'un seul pod déployé et qu'il  se trouve sur le noeud en erreur / maintenance le service ne sera pas rendu .

si on a juste une tres petite interruption : kubelet va remettre online les pods rapidement.
si la panne dure plus de 5 miniutes alors les pods de ce nodes passsent en status  terminated.
si les pods font partis d'un replicat set alors ils sont recrées sur un nouveau node.
Le delai pour lequel le pod est considéré ko est le timeeviction défini dans le controller-manager :
ex :
kube-controller-manager --pod-eviction-timeout=5m0s ...

pour assurer des maintenances propre on va drain le node :
les pods vont etre détruit du node sur lequel l'intervention va avoir lieu puis ils vont etre recréés sur les autres nodes dispos.

kubectl drain node1

le node va être marqué cordon egalement -> ce qui va empécher d'avoir des pods schedule dessus .
Quand le node revient en prod on va devoir permettre la creation de pods dessus : on va donc le "uncordon" :

kubectl uncordon node1

il est possible de simplement empecher le schedule de nouveaux pods sur le node en inter avec :

kubectl cordon node1 

dans ce cas les pods presents sur ce node ne sont pas éjectés ....

ch6 v2 -> done 

tp maintenance cluster 

on va mettre en maintenance le node01 : 

master $ kubectl get pods -o wide
NAME                   READY     STATUS    RESTARTS   AGE       IP          NODE      NOMINATED NODE
blue-684d969bd-dlr7q   1/1       Running   0          2m        10.38.0.2   node03    <none>
blue-684d969bd-h8qcl   1/1       Running   0          2m        10.44.0.2   node01    <none>
blue-684d969bd-mcfsn   1/1       Running   0          2m        10.32.0.2   node02    <none>
red-67465685c-27ncp    1/1       Running   0          2m        10.44.0.1   node01    <none>
red-67465685c-fjgj4    1/1       Running   0          2m        10.38.0.1   node03    <none>

Il est possible de drain un node tout en ignorant les daemonsets :

master $ kubectl drain node01 --ignore-daemonsets


WARNING: Ignoring DaemonSet-managed pods: kube-proxy-wf675, weave-net-89bh8
pod/blue-684d969bd-h8qcl evicted
pod/red-67465685c-27ncp evicted
master $ kubectl get pods -o wide
NAME                   READY     STATUS    RESTARTS   AGE       IP          NODE      NOMINATED NODE
blue-684d969bd-dlr7q   1/1       Running   0          3m        10.38.0.2   node03    <none>
blue-684d969bd-mcfsn   1/1       Running   0          3m        10.32.0.2   node02    <none>
blue-684d969bd-rc94x   1/1       Running   0          6s        10.32.0.3   node02    <none>
red-67465685c-fjgj4    1/1       Running   0          3m        10.38.0.1   node03    <none>
red-67465685c-w2wrk    1/1       Running   0          6s        10.32.0.4   node02    <none>

une fois que l'operation de maintenance sur le node01 est fait on permet de reschedule des pods dessus : 

master $ kubectl uncordon node01
node/node01 uncordoned


le master ne prend pas de pod car il est marqué :

Taints:             node-role.kubernetes.io/master:NoSchedule  quand on fait un kubectl describe nodes master.

on peut forcer un drain même si des pods ne font pas partis de replicatset ..dans ce cas ces pods sont detruits !

kubectl drain node02 --ignore-daemosents --force


pour forcer un node a ne plus prendre de nouveaux pods : 
master $ kubectl cordon node03
node/node03 cordoned



= kubernetes releases : =

on voit que la version de kube est dispo avec un kubectl get nodes 

ex : v1.11.3

1  -> version majeur
11 -> version mineure  : sortent regulierement avec des améliorations 
3  -> numero de patch  : sorent plus regulierement avec des bugfix ,secu ...

on a aussi comme d'habitude des alphas, betas 

sur le site de kube on peut downlader le package relatif à notre version : tous les composants du backplane sont present et a la meme version 
seuls etcd et coredns ne suivent pas la version car ils sont des projets à part.


exam de liens de version a faire cf slides

ch6 v3 -> done

= kubernetes upgrade version =

cluster upgrade process : on ne parle pas de coredns ni d'etcd 
tous les composants doivent etre à la même version 
aucun composant ne doit avoir de version supérieure à apiserver 

on peut avoir une version en moins pour controller-manager et kube-scheduleur
on peut avoir deux version en moins pour kubelet et kube-proxy 

le kubectl peut avoir une version sup a l'apiserver , la meme version ou une version en moins que l'apiserver

on peut  upgrader composants par composants.

- Quand upgrader ? 

!! Attention kubernetes ne peux pas supporter d'upgrade supérieure à 3 versions !! 

1.12 -- 1.11 -- 1.10 
l'upgrade de 1.9 a 1.12 ne passe pas.

Il est recommandé d'upgrader une version mineure à la fois.

Dans l'examen actuel nous allons étudier l'upgrade avec kubeadm :

1/ on upgrade d'abord le master
2/ les nodes apres.


1/ upgrade de master :

on met a jour le master : pendant ce temps les workers travaillent toujours
les users ne sont pas impactés.

pour les nodes ont a plusieurs stratégies :

- upgrade de tous les nodes :
dans ce cas plus aucuns service n'est dispo pour les users.

- upgrade node par node :

on update un node ..les pods partent sur les nodes dispos du cluster et on fait cela au fur et mesure


- injection d'un nouveau node avec la nouvelle version :
on injecte les pods dessus et on eteind le vieux node ..on poursuit pour tous les noeuds du cluster.


- kubeadm upgrade :

on va utiliser cette methode

- kubeadm upgrade plan 

-> va nous montrer les info de versions presentes et disponibles pour l'upgrade 

on peut voir egalement que l'upgrade de certains composants (kubelet) doit se faire manuellement sur chaque node.
puis on peut faire un kubadm apply num_version quand on est pret 

1- on va upgrade kubeadm :

apt upgrade -y kubeadm=1.14.0-00

2- upgrade 

kubeadm upgrade apply v1.14.0 : l'upgrade se fait naturellement.

quand on fait un kubectl get nodes on voit les num de versions inchangés -> c'est uniquement car c'est la version de kubelet qui est remontées sur les nodes.

en fonction de la maniere dont on a déployer notre cluster on peut avoir kubelet present sur notre master node. : si on a installer avec kubeadm.

on upgrade kubelet :
- sur le master en premier ( si présent ) 
apt upgrade -y kubelet=1.14.0-00

- sur les nodes :
on drain les nodes d'un node puis on upgrade ..on le fait partout
puis on uncordon 
kubeadm upgrade node config  --kubelet-version v1.14.0
systemctl restart kubelet


ch6 v4 -> done 


tp upgrade :

on est en version 1.11.3 on va donc pouvoir upgrader max jusqu'en 1.14 
on ne pourra le faire que de version mineure en mineure 
on va upgrader le master qui héberge aussi des pods :
kubectl drain master --ignore-daemonsets

on update kubeadm avec la version mineure du premier step : 
apt-get update && apt-get upgrade -y kubeadm=1.12.0-00

on lance l'update avec kubeadm : on verifie d'abord le status d'upgrade possible : 

master $ kubeadm upgrade plan
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.11.3
[upgrade/versions] kubeadm version: v1.12.0
[upgrade/versions] Latest stable version: v1.14.2
[upgrade/versions] Latest version in the v1.11 series: v1.11.10
[upgrade/versions] WARNING: No recommended etcd for requested kubernetes version (v1.14.2)

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     1 x v1.11.3   v1.11.10
            1 x v1.11.3   v1.11.10

Upgrade to the latest version in the v1.11 series:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.11.3   v1.11.10
Controller Manager   v1.11.3   v1.11.10
Scheduler            v1.11.3   v1.11.10
Kube Proxy           v1.11.3   v1.11.10
CoreDNS              1.1.3     1.2.2
Etcd                 3.2.18    3.2.18

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.11.10

_____________________________________________________________________

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     1 x v1.11.3   v1.14.2
            1 x v1.12.0   v1.14.2

Upgrade to the latest stable version:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.11.3   v1.14.2
Controller Manager   v1.11.3   v1.14.2
Scheduler            v1.11.3   v1.14.2
Kube Proxy           v1.11.3   v1.14.2
CoreDNS              1.1.3     1.2.2
Etcd                 3.2.18    N/A

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.14.2

Note: Before you can perform this upgrade, you have to update kubeadm to v1.14.2.

_____________________________________________________________________


master $ kubeadm upgrade apply v1.12.0
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[upgrade/apply] Respecting the --cri-socket flag that is set with higher priority than the config file.
[upgrade/version] You have chosen to change the cluster version to "v1.12.0"
[upgrade/versions] Cluster version: v1.11.3
[upgrade/versions] kubeadm version: v1.12.0
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y

....

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.12.0". Enjoy!

on remet le master en prod on permet d'acceuillir des pods :

kubectl uncordon master

on passe au node :

master $ kubectl drain node01 --ignore-daemonsets
node/node01 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-proxy-wcn2g, kube-system/weave-net-58ssj
evicting pod "coredns-576cbf47c7-z48rv"
evicting pod "red-67465685c-j5vcg"
evicting pod "coredns-576cbf47c7-2lzjk"
evicting pod "red-67465685c-8sxwc"
pod/red-67465685c-j5vcg evicted
pod/coredns-576cbf47c7-z48rv evicted
pod/red-67465685c-8sxwc evicted
pod/coredns-576cbf47c7-2lzjk evicted
node/node01 evicted

ssh node01
node01 $ apt-get update && apt-get upgrade -u kubeadm=1.12.0-00

node01 $ kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2)
[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.12" ConfigMap in the kube-system namespace
[kubelet] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[upgrade] The configuration for this node was successfully updated!
[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.


on update kubelet : 
node $ apt install kubelet=1.12.0-00

tp upgrade done 


= Backup and restore = 

que doit on backuper ?

les configurations de nos ressources 

on peut stocker tout nos fichiers de définition d'objects dans un repertoire dédié.
il nous faut donc une copie de sauvegarde de ces fichiers. --> on va habituellement les stocker dans un cvs : git / gitlab /github.

on peut pour s'assurer que toutes les ressources sont bien backupées intérroger l'apiserver et stocker l'intégralité de nos ressources dans un fichier de backup.

kubectl get all --all-namespaces -o yaml > all-deployed-services.yaml

certains outils dédiés existent.


Etcd stocke les états du cluster : les infos du cluster y sont stockées.

On va pouvoir backuper les data etcd 

>> on défini le datadir de etcd : l'endroit ou sont stockées les data.

Un outil de snap est fourni avec etcd :

Attention il faut préfixer avec l'api V3 de etcd

ETCDCTL_API=3 etcdctl snapshot save snapshot.db 
>>> ici on backup dans le repertoire courant nos data dans un fichier appellé snapshot.db
on peut biensur backupé ou on veut en précisant le path :

ETCDCTL_API=3 etcdctl snapshot save /home/boogie/backup/etcd_bck.db


On peut voir l'état de notre snap :

ETCDCTL_API=3 etcdctl snapshot status snapshot.db

Pour restaurer un backup :

1/ stopper le kubeapiserver : car on va devroir arreter le service etcd

service kube-apiserver stop

2/ restauration du snap sur l'intégralité des nodes du cluster etcd :

ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --data-dir /var/lib/etcd-for_backup \
  --initial-cluster etcd01=https://etcd01.boogie.net:2380,etcd02=https://etcd02.boogie.net:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls https://${internal_ip}:2380

!! /!\ Etcd restore les data depuis le backup et instancie une NOUVELLE configuration de cluster 
et configure les membres comme des nouveaux membres d'un nouveau cluster !!

C'est fait par prévention pour empecher un nouveau membre de s'integrer dans un cluster déja existant.
a ce moment la un nouveau datadir est créer dans notre exemple : /var/lib/etcd-from-backup

On va maintenant configurer notre service etcd pour qu'il prenne en compte notre nouveau data dir domain et notre nouveau token 

une fois notre service modifier on reload le service daemon et etcd :

systemctl daemon-reload
service etcd restart 

3/ on démarre le kubeapserver service : 

service kube-apiserver start

Attention pour le tls on doit indiquer a etcd les endpoint; ca; crt et key 


ch6 v5 -> done 

todo tp backup


== security ==


ch7 v6

rappel des fondamentaux sur le chiffrement tls /ssl 


-> CA (certification authorithy) pour signer les certificats
-> certificats servers : ( private key / cert (pub key )
-> clients certificats ( pour authentifier les users se connectant au server )

nomenclature :

clé pub -> *cert / .pem  : quand on a des fichier de type .cert ou pem se sont les clés pub ( ex : server.crt , client.crt ..)
clé prive -> .key / *-key.pem  : quand le nom du fichier contient key ..c'est la clé privée. ( ex: server.key , client.key ..)


Dans kube on a des master et worker dont les communications sont chiffrées.
un admin se connectant au cluster aura une connexion chiffrée et c'est la même chose pour les composants du cluster kube.
on a donc des certificats server et des certificats client 

- server : 
-> kube api : server ( on génere : apiserver.key / apiserver.cert ) : composant principal : les users et les composants s'y connectent
on essaye de nommer les fichiers clairement 

-> etcd cluster : server ( on genere : etcdserver.key /etcdserver.cert ) : stocke les infos du cluster.

-> kubelet : server ( on genere kubelet.key / kubelet.cert) : ce server est présent sur tous les nodes 

- clients:

-> admin : client ( on genere admin.key /admin.crt ) : on s'authentifie pour se connecter a l'apiserver  avec kubectl 
-> scheduler : client ( on genere scheduler.key /scheduler.cert) : le scheduler s'autentifie a l'apiserver 
-> controller-manager : client ( on genere controllermanager.key / controllermanager.cert) : le controller manager s'authentifie apures de l'apiserver
-> kube-proxy: client  ( on genere kube-proxy.key /kube-proxy.cert) le kube-proxy s'authentifie aupres de l'api server


Les servers communiquent aussi entre eux :

-> apiserver se connecte a etcd 
l'apiserver peut utiliser la meme paire de clé qu'il utilise pour servir ses clients. On peut en générer une paire de clé dédiée si on veut.
-> apiserver se connecte a kubelet : on peut utiliser la meme paire de clé ou en générer une nouvelle pour que l'apiserver s'authentifie a kubelet.

on va devoir avoir 2 CA :
-> une pour l'api  : ca.key / ca.cert
-> une pour etcd  etcd-ca.key / etcd-ca.cert

ch7 v6 -> done

- generation des certificats ssl :

plusieurs outils existent : openssl ,cfssl, easyrsa .
on va dans notre cas utiliser openssl 

1/ CA : 
on va créer notre ca pour signer et valider nos futurs certificats :

- on génère la clé :

openssl genrsa -out ca.key 2048
ca.key
- on génere le csr ( certificat signing request ) : on renseigne des infos nécéssaires minimales :

openssl req -new -key ca.key -subj "/CN=Kubernetes-CA" -out ca.csr 
ca.csr

- on signe notre certificat à l'aide de notre clé et du csr généré :

openssl x509 -req in ca.csr -signkey ca.key -out ca.crt 
ca.crt

-> on vient donc de faire notre ca avec un certif autosigné


2/ Admin user :

on va maintenant nous occuper de la conf pour notre admin user : on génere la clé, on renseigne un nom explicite dans le csr et on ajoute le groupe dédié interne à kube pour l'administration de notre cluster (O=system:masters) signe avec le cert et la clé de notre ca.

openssl genrsa -out admin.key 2048 
admin.key 

openssl req -new -key admin.key -subj "/CN=kube-admin/O=system:masters" -out admin.csr
admin.csr

openssl x509 -req in admin.csr -CA ca.crt -CAkey ca.key -signkey admin.key -out admin.crt 
admin.crt

Comment fait on pour différencier notre admin kube d'un autre admin ?
on doit spécifier un groupe particulier dans la description  de notre csr :

3/ clients : les autres composants de notre cluster vont devoir parler a l'apiserver 

on va faire la même chose pour (scheduler, controller, kubeproxy )et préfixer leur nom de system car se sont des composants internes.

pour les cmposants server ont va voir plus loin.


On va pouvoir nous connecter en https ou mettre notre conf  dans un fichier de conf dédié : "kubeconfig"

Tous les composants devront en plus avoir à dispo le certificat "root" de notre ca : ca.crt 


4/ tls pour les servers :
pour rester simple et uniquement pour l'exemple on ne génere pas pour nos tests de ca pour etcd. Nous restons sur la ca kube déja gérée.
on va avoir trois groupes  : etcd / apiserver /kubelet 

- etcd :

on génere un cert pour etcdserver :

openssl genrsa -out etcd_server.key 2048 
etd_server.key 

openssl req -new -key etcd_server.key -subj "/CN=kube-admin/O=system:masters" -out etcd_server.csr
etcd_server.csr

openssl x509 -req in etcd_server.csr -CA ca.crt -CAkey ca.key -signkey etcd_server.key -out etcd_server.crt 
etcd_server.crt

-> on va deployer ces fichiers sur tous les serveurs etcd qui seront en Ha.
on va egalement générer un etcd peer certificate pour assurer la comm entre les noeuds du cluster etcd 


openssl genrsa -out etcd_peer.key 2048 
etd_peer.key 

openssl req -new -key etcd_peer.key -subj "/CN=kube-admin/O=system:masters" -out etcd_peer.csr
etcd_peer.csr

openssl x509 -req in etcd_peer.csr -CA ca.crt -CAkey ca.key -signkey etcd_peer.key -out etcd_peer.crt 
etcd_peer.crt

on va ensuite devoir renseigner les paths de nos certif et da la ca dans la conf etcd ( etcd.yaml ) 

- apiserver :

même principe que le server etcd pour apiserver

Attention tout passe par l'apiserver dans le cluster kube

Il va y avoir plusieurs noms qu'on va definir dans la conf de notre kube :
les  noms minimums à définir sont : 
kubernetes
kubernetes.default
kubernetes.default.svc
kubernetes.default.svc.cluster.local   >>>> nom global 

on doit ausssi avoir l'ip de l'api 
ex: 10.96.0.1
on peut voir l'ip du pod qui héberge l'apiserver :
ex : 172.17.0.87

Pour définir plusieurs noms on va créer un fichier de config dans lequel on va saisir nos données pour générer le csr :
on va remplir les champs dans la section alt_names :
ex : openssl.cnf

[req ]
....
[v3_req]
...
[alt_names]
DNS.1=kubernetes
DNS.2=kubernetes.default
DNS.3=kubernetes.default.svc
DNS.4=kubernetes.default.svc.cluster.local   >>>> nom global 
IP.1=10.96.0.1
IP.2=172.17.0.87



openssl genrsa -out apiserver.key 2048 
apiserver.key 


openssl req -new -key apiserver.key -subj "/CN=kube-apiserver" -out apiserver.csr -config openssl.conf
aapiserver.csr

openssl x509 -req in etcd_peer.csr -CA ca.crt -CAkey ca.key -signkey etcd_peer.key -out etcd_peer.crt 
apiserver.crt

on va definir les path des fichiers server (et clients) pour l'api directement dans l'executable ou alors en service.


- kubelet :

cet api est sur chaque node : on doit avoir une clé /cert par nodes 

on doit donc le faire pour chque node du cluster
node01, node02 ....

on a aussi des kubelet client qui vont s'autehntifier pour communiquer avec kube-apiserver 

on doit aussi donc avoir le nom :

le nom du groupe a définir dans notre cert est : system:node:node01 etc ....

ch7 v7 -> done

= examen des certificats dans un cluster existant :

on va devoir connaitre la maniere dont les certif ont été générés : hard way / kubeadm

-> hard way : tout  ce fait de manière manuelle et  la conf des certif se fait dans les services des composants de kube
-> kubeadm : va dans son cas gérer en auto les  certif et appliquer leur conf au sein de pod

Dans nos exemples on va utiliser kubeadm.
On peut se faire une feuille détaillant tous les composants, le type de cert associés (server / client ), le path des cert, la ca ,le cn ,les alt names , l'organisation, l'issuer ,la date d'expiration.

avec kubeadm utilisé pour le setup on va pouvoir identifier les certif en examinant les différents fichier de conf :

ex: fichier de l'apiserver d'un minikube :
# cat /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --admission-control=Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota
    - --service-account-key-file=/var/lib/localkube/certs/sa.pub
    - --client-ca-file=/var/lib/localkube/certs/ca.crt
    - --tls-private-key-file=/var/lib/localkube/certs/apiserver.key
    - --secure-port=8443
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --service-cluster-ip-range=10.96.0.0/12
    - --kubelet-client-certificate=/var/lib/localkube/certs/apiserver-kubelet-client.crt
    - --requestheader-client-ca-file=/var/lib/localkube/certs/front-proxy-ca.crt
    - --proxy-client-cert-file=/var/lib/localkube/certs/front-proxy-client.crt
    - --proxy-client-key-file=/var/lib/localkube/certs/front-proxy-client.key
    - --insecure-port=0
    - --allow-privileged=true
    - --requestheader-username-headers=X-Remote-User
    - --tls-cert-file=/var/lib/localkube/certs/apiserver.crt
    - --enable-bootstrap-token-auth=true
    - --requestheader-allowed-names=front-proxy-client
    - --advertise-address=192.168.99.100
    - --kubelet-client-key=/var/lib/localkube/certs/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --authorization-mode=Node,RBAC
    - --etcd-servers=https://127.0.0.1:2379
    - --etcd-cafile=/var/lib/localkube/certs/etcd/ca.crt
    - --etcd-certfile=/var/lib/localkube/certs/apiserver-etcd-client.crt
    - --etcd-keyfile=/var/lib/localkube/certs/apiserver-etcd-client.key
    image: k8s.gcr.io/kube-apiserver-amd64:v1.10.0
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 192.168.99.100
        path: /healthz
        port: 8443
        scheme: HTTPS
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-apiserver
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /var/lib/localkube/certs/
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
  hostNetwork: true
  volumes:
  - hostPath:
      path: /var/lib/localkube/certs/
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
status: {}



on voit que tous les certificats sont gérés pour le démarrage du service 

on va pouvoir examiner les infos de nos cert avec des commandes openssl classiques :

openssl --noout -text -in /etc/kubernetes/manifests/kube-apiserver.yaml

on peut biensur faire la même chose pour tout nos composants kube.

- troubleshooting :

En cas de pb : si on setté nos conf en hard way : on va examiner les fichiers de logs des services :
ex: journalctl -u etcd.service -l
si c'est via kubedam :
kubectl logs etcd-master 

si kubect lne repond pas alors on doit decendre un niveau plus bas : ex du container 

ex :
docker ps  -a

on trouve notre container et on exam ses logs :

docker logs id_container

ch7 v8 -> done

= Certificats api dans kube : 

quand un nouveau user arrive ..il va falloir faire toute la chaine pour qu'elle ait un cert valide ....biensur cela devient tres fastidieux avec un nombre conséquent de user ...
kube a une api build in pour gérer les certs 
-> le user crée sa clé  :
openssl genrsa -out user.key 2048

-> on genere un object de type csr
openssl req -new-key user.key -subj "CN=user" -out user.csr
 
un type kube dédié existe :

usr-csr.yaml

apiVersion ..
kind: CertificatSigningRequest


on va par contre encoder en base 64 le csr avant de l'injecter dans notre objet kube :

cat user.csr |base64  
on ajoute ensuite la sortie dans le champ request de l'objet yaml kube


-> on exam les requests ..
kubectl get csr 

-> on approuve le cert
kubectl certificate approve  user 
user approved!

-> on deploy le cert
on peur exam le cert :

kubectl get csr user -o yaml

on va devoir pour exm le cert le decoder :

cette fois :

echo "le texte de notre cert" |base64 --decode


dans kube c'est le controller manager qui est en charge des operations sur les certifs :
il a des composants internes :
csr-approving 
csr-signing

ch7 v9 -> done 


= kubeconfig : =

on a  vu la génération des conf ( key, cert ) pour un admin 
on sait qu'on peut interroger notre api en http via curl 

c'est forcement laborieux. 
On va donc mettre les infos dans un fichier -> kiubeconfig. 
Ce fichier a plusieurs sections 

-> cluster : on peut avoir plusieurs cluster a administrer :
production, dev, google 

-> user 
admin , dev , prod_ user 

-> context : associe quel user va acceder quel cluster :

admin@production
dev@google 


on ne crée rien en terme de droit on ne fait qu'associer les infos 


on va donc  renseigner les différentes infos dans les différentes section :

ex :

 cat .kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0dfdfsdfsfscscciBDRVJUSUZJQ0FURS0tLS0tCk1JSUR3VENDQXFtZ0F3SUJBZ0lKQUlzK3RGQUdDVkFiTUEwR0NTcUdTSWIzRFFFQkN3VUFNSGN4Q3pBSkJnTlYKQkFZVEFrWlNNUll3RkFZRFZRUUlEQTFKYkdVdFpHVXRSbkpoYm1ObE1RNHdEQVlEVlFRSERBVlFZWEpwY3pFUApNQTBHQTFVRUNnd0dUV1ZsZEdsak1Rd3dDZ1lEVlFRTERBTkVVMGt4SVRBZkJnTlZCQU1NR0V0MVltVkdiM0pFClpYWWdTM1ZpWlhKdVpYUmxjeUJEUVRBZUZ3MHhPVEF6TURRd09UVXlNamhhRncweU9UQXpNREV3T1RVeU1qaGEKTUhjeEN6QUpCZ05WQkFZVEFrWlNNUll3RkFZRFZRUUlEQTFKYkdVdFpHVXRSbkpoYm1ObE1RNHdEQVlEVlFRSApEQVZRWVhKcGN6RVBNQTBHQTFVRUNnd0dUV1ZsZEdsak1Rd3dDZ1lEVlFRTERBTkVVMGt4SVRBZkJnTlZCQU1NCkdFdDFZbVZHYjNKRVpYWWdTM1ZpWlhKdVpYUmxjeUJEUVRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVAKQURDQ0FRb0NnZ0VCQUtYMWxoOTZOZFRqY1NTK0I5Y2ZJSURPSHNqc0xEK2ZzajVrMHhFU2JWRy84Z25oaEo5cwprcEQ0NXdvWU9YOXZGTlFTM1VCZWNoZG91QUpVNWJnWkx4a0dVS0FMUlcrek1ZMGlqU0t6dGVKcDl2T3dQV1NGClcwTUQyaVpLQStaRS9ZdEZmTUFlbUo4Nm5DUjAzUGdvYmdyK1JrOTBHaU13dXo1cnpsdnBZbUkwZldLb3piRWgKenNZa05haStwV2NnMlRZSlNlb3NHdGdTWGJpb2VSWXMvOWw0NmdsTzg4V1pubEdOOHZHYk45MWxuV3NscHUxcwpUamUzWmQwRXVIMUtQd0w1UXp2K01zQTlZSU4zWXFwcEZxNmRqcUxKNkFzem9WQTBHRkRkbjcvVnNRaXp4TndmCmtyVmJpRldHZmVmblh5WmxMdjRlT0lJN1htc055cm1TWE84Q0F3RUFBYU5RTUU0d0hRWURWUjBPQkJZRUZHTTgKSVdsZW5mbSt2TVBqK2JDM2V4Vi8xMlhTTUI4R0ExVWRJd1FZTUJhQUZHTThJV2xlbmZtK3ZNUGorYkMzZXhWLwoxMlhTTUF3R0ExVWRFd1FGTUFNQkFmOHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRjE1UXplVlU2dDMrbUhqCmt5R3pWUHE3YXI2NEVhR05tMGZ2dHI1WG1lT2s1VFl2ck5ZVStKUHM0elY2UTFYMjlrWE01Wk0rMVc3aGxWRjIKNDZ5V1JjbmhodkhxajlHT2VvV2pkUldJdnBFb0o2ZFpXandYcVdVbnNKNEZzTGNIRmVUc2VZSlRWamFTSENmNgpFUWR4cWFVcWQra2N3c2MyeHFhSWJnZTg0Z0dTVGFUL0VsVjUycndmQnJsRGk1WG1seVBDcVdoakdEUTdndkx0CmphVU5naFQ4T1JGbTgwMWlCVS9PWFFuNGVYdEtza3gxaERWcVdWdG82ZWRjMy8vMC8ycXh0Tm9CMHU2ZnI4RVAKTlZZZmlpM0lxVWk5dWNlUnRyK1Rkdm1kbnAxbC80b25OZnEzaTNOTDVLNUhHMW1uOERKRnpRMVNYc2pIREpLdApJSDY5MFg0PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.8.10:6443
  name: kubefordev
- cluster:
    certificate-authority: /home/boogie/.minikube/ca.crt
    server: https://192.168.99.100:8443
  name: minikube
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR1ekNDQXFPZ0F3SUJBZ0lKQVBTMkNwSkRWeUIxTUEwR0NTcUdTSWIzRFFFQkN3VUFNSFF4Q3pBSkJnTlYKQkFZVEFrWlNNUll3RkFZRFZRUUlEQTFKYkdVdFpHVXRSbkpoYm1ObE1RNHdEQVlEVlFRSERBVlFZWEpwY3pFUApNQTBHQTFVRUNnd0dUV1ZsZEdsak1Rd3dDZ1lEVlFRTERBTkVVMGt4SGpBY0JnTlZCQU1NRlV0MVltVnlibVYwClpYTWdVbVZqWlhSMFpTQkRRVEFlRncweE9EQTJNRFV3T1RJek5EUmFGdzB5T0RBMk1ESXdPVEl6TkRSYU1IUXgKQ3pBSkJnTlZCQVlUQWtaU01SWXdGQVlEVlFRSURBMUpiR1V0WkdVdFJuSmhibU5sTVE0d0RBWURWUVFIREFWUQpZWEpwY3pFUE1BMEdBMVVFQ2d3R1RXVmxkR2xqTVF3d0NnWURWUVFMREFORVUwa3hIakFjQmdOVkJBTU1GVXQxClltVnlibVYwWlhNZ1VtVmpaWFIwWlNCRFFUQ0NBU0l3RFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0MKZ2dFQkFPN08yOTdOTlk4QTh3MHQ4K3BrMmd3R1dGdnBYeFZHaHRwNnFRejJmWGhsWUJ0VC9PWDgwL0oxSE80QQpoZC9hanNIdXdxem16L25Dd2xTcXlEcERYRVptSG1Tc0g0ZFNlOVpWSUVWL2llZ0ZPMzNCL1NCWDZzSTd2a0RqCi9zeVEyOEVqVExkNjZXbmtDY3JRWHZMZkR5dUpHdnFnS1RleU85QWx0SExjSVVrVm51U2doalFEenNUdGE4aEwKenFtRDVYZU1NTW16U3ArUGpFZFd1SW5TbXR5N0hrbUV0ZklzaTZ4Z1hHaUdlclduSU1CQXZoR0RpNWpsUDZVcwplZ0dYTGZnU01IRms0ZFBMbllyWkt3bi9CU0tFWlNHMU45Y2VvQmJwTUtQY2U1NnBqek5lU3RqVEZCT2JLZVBVCk0wd0paVzM3ZHVzbVk3cVg2dFBZaVVVd1NyOENBd0VBQWFOUU1FNHdIUVlEVlIwT0JCWUVGTUtkTVB1QXBrSnAKcW1Uc0FGOG14d3JRVnkrSU1COEdBMVVkSXdRWU1CYUFGTUtkTVB1QXBrSnBxbVRzQUY4bXh3clFWeStJTUF3RwpBMVVkRXdRRk1BTUJBZjh3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQUl0RHFrSGhIUUkxcm9LM2VRWHlVN05HClN2c2tNaFlSVklueUtZTitnNGZzUitCeDU3TVowZ0Y5bXdwdW5nOTV2VFgyS3AyT2FzT3E4TVE3bVpVOXFMOEUKMTNpTjJISWVvM0diTDVKek9hN0ZrV1lDWWNPQnV4M1Zlejl5Rld3YzNQYjBjVzJPcHhtdmwwT0RnTDhDcW85cQp3K3BvdE5STmF0bGcyWnVLdUY4dDBZS1ZJTFJ1MXN6cUtUZDM4empEczg3TksxSTArLzR2elNrdFdvL3BtL05zCndzS0FaL3Mrd1FqcGdhalh0ME5lVGRsZysva0VwQ1kxSFdZaTdwM1BoV2g2VTFhNExPSDJoSGtPV2pWWkp4OS8KclFpeDFDRHpPVGl0a0FHY3Z2cktPZXM5QkpKRWVReVNQUWgxaGFJeDZSazljS242QktqdEo2WFY0eFJOSC93PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.10.51:6443
  name: kubeprod
contexts:
- context:
    cluster: kubefordev
    user: boogie-dev-cert
  name: kubefordev-cert
- context:
    cluster: minikube
    user: minikube
  name: minikube
- context:
    cluster: kubeprod
    user: boogie-prod.cert
  name: kubeprod-cert
current-context: minikube
kind: Config
preferences: {}
users:
- name: boogie-dev-cert
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURMekNDQWhjQ0NRRHdSN3A3N29UL016QU5CZ2txaGtpRzl3MEJBUXNGQURCM01Rc3dDUVlEVlFRR0V3SkcKVWpFV01CUUdBMVVFQ0F3TlNXeGxMV1JsTFVaeVlXNWpaVEVPTUF3R0ExVUVCd3dGVUdGeWFYTXhEekFOQmdOVgpCQW9NQmsxbFpYUnBZekVNTUFvR0ExVUVDd3dEUkZOSk1TRXdId1lEVlFRRERCaExkV0psUm05eVJHVjJJRXQxClltVnlibVYwWlhNZ1EwRXdIaGNOTVRrd05EQTVNVEkxTnpJd1doY05NVGt3TnpBNE1USTFOekl3V2pBOE1SQXcKRGdZRFZRUUREQWRtTG5OdlltOXVNUll3RkFZRFZRUUtEQTFOWldWMGFXTXRRV1J0YVc1ek1SQXdEZ1lEVlFRSwpEQWRYWldKalpYSjBNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXBDYTY5b2NYCjliWmVzcVFQbVVTTGVldEZ4cG5obVZMbGNPd1RiWk5iN2x0MnVtQndVL3ova3Q0T0ZXZXkvMEdPenFuMjBLR1MKYlc1Z2Z3dW85a3FFNlQzYTEzVnAxZEI5RFNTajJCWjVncmNIalM0cGxjc3FCelB4TFpScHRDSGllZTBvVHJrRgorbGxuS1ptOTBhWEhSaExrVVdYZEZJWllUcXFHckVIZGZ2eGRUMHk5Z1d5RnByU04wdzdwTjd0V2JoT1IxRkRLCitzRDZpbFo5QkNxMWs2VGsvb3hibk0zM052ejhCczhnRDBCcWhzTG5oaUswaHk0djVESlh4Z0lOVkJJc3hpd24KTjd3eUZGQ21KMWNJMUhvYzlwb2ZUTFFCU0M1TnhlQWs1bDNGd25OTUtoc09pcERlUzE4R0RaRWtBSU12NlhVcAp4ektMb0hzblFOVHBtUUlEQVFBQk1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQVFJWlJ1bzE1cjVYRXJNNWh4CkE5Vmg3d0gralRaczcyWFZOWlQ4Q0pnUGRlMXZIaFZZLzd4UVJKaFRZcFZ5cnhUWlBUWlR0dnFhVzZpKy9PWjQKZEFTaUZWc1ovaU1vTHY0NUxWTmViWnFRUHdRMmRidnAzL2NIQTNQRFp6dldnNXhIa2ZySFBJeDhyOTRhbUJFYgpTMm1YUnErbFBEVzlqV2s2U0RNTTZYd3pJWE9XMzlKVGlvSzdOcloxWDg3YlhjWG1FZDIwV0l3Q01maG42eVFBCmhHaXcvOW1nV1ViVHVZczVKNnMzdGJvN1QwRW9HTmMweHk1aXBWRkY4dTd2cnVwOXlUWlk3RWh4T3RZMlcvK3QKT1gxVmhhaUZ5eWVRNUdDcCtDLzhSZlBKMTNzcEYzSERobnF0Y1d2Snp4UVFDTHQrWWJmNG5sMVNLVktpbEc0UgpjcW1ECi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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
- name: boogie-prod-cert
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURMRENDQWhRQ0NRQ1BZYm1zQzBkUDl6QU5CZ2txaGtpRzl3MEJBUXNGQURCME1Rc3dDUVlEVlFRR0V3SkcKVWpFV01CUUdBMVVFQ0F3TlNXeGxMV1JsTFVaeVlXNWpaVEVPTUF3R0ExVUVCd3dGVUdGeWFYTXhEekFOQmdOVgpCQW9NQmsxbFpYUnBZekVNTUFvR0ExVUVDd3dEUkZOSk1SNHdIQVlEVlFRRERCVkxkV0psY201bGRHVnpJRkpsClkyVjBkR1VnUTBFd0hoY05NVGt3TXpFeE1EZzFNekF5V2hjTk1Ua3dOakE1TURnMU16QXlXakE4TVJBd0RnWUQKVlFRRERBZG1Mbk52WW05dU1SWXdGQVlEVlFRS0RBMU5aV1YwYVdNdFFXUnRhVzV6TVJBd0RnWURWUVFLREFkWApaV0pqWlhKME1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBclhmZTNQUzlRa1pLCkgzODNzN1FxZFNScGp2a1Y3SUw5VWFGVGxub21CcEtPZ1BWQzNCbEFJR2pReE9aK1BlbmF2RUUxR2RSSDBEV0gKbVVnQlo2NWZJUGJUTmRvVFNMaThJTXZzMHh2aXl0ZkdBNjBYeW16QVBVcUlCUGZMZ2J5ZTQ0ZEM5VGlIS1B3ZAo2ejRXT0w5OE90Yi9PYWRTWlQxYUxUYmhyYTRmeHJ6aWxwTU96QlJtcWlJQk5ldGplTlRxWGNHemRMTXVGYnkwCnZtVnFyMHBsUXJ5Slg3Sjh5VWt2cDJYSjlMVmxyZ05NOTVRUStob0l0Y2k0SVlJOFR6azNOUGQvcTNwS0l5NlYKQlhlb1FwTExzY1hOSVBqbEtsaWNUUkkxcFErcm5Rb3lqQVZVNmVYV1FOd2htUUZoaWU0MzRDRkFMdkQ5RjRGdgptREFwV3Z6NHVRSURBUUFCTUEwR0NTcUdTSWIzRFFFQkN3VUFBNElCQVFDeDVYM3h1T2NoWG9CVkpkd2MyMGJzCjVOMjB1azZqdURmNXBzaU9PenM4Tkg2Sm9lTzFQMi9pWDdTZGN0cVVHMVMwbnlNc3dmZWk4bGxqQmhsWFRsUVUKNUxZbjFmbExpZmJCUlJ3eEVVa1p4RVhnbGFnL0NucWM2QzZ2RlRuTmR3NEhnWHBXVUtOT0tNUWNZZFRzMVFEZQpjekErYytuK3BLcS9id3o0TGVVRE14aDAxSnh4REtWcUluVVZYRTF3UGQvOTA3bmVkR3ZoWGJwTEQycnVPejBFCi9ZeXJ4K3dqRGgyaXZNWFp2QWxoUTl6YkF0K1B3cVNBL1R2SmRGM1FYL1dHUUxGb3h5Yjh0OFZFODBtV3JKbDAKaTJ0dHh5TElTazNwWUxBSkVmelNjaGtDSDNZZ01rUGY2dkFRMlBwK0pVR3IrN1FhRFF2YmNDcVN3dTBZMVFsQgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: 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
- name: minikube
  user:
    client-certificate: /home/boogie/.minikube/client.crt
    client-key: /home/boogie/.minikube/client.key

on voit ici  qu'on associe les infos de user et cluster dans les contextes pour setter nos infos.

On a un contexte par défault de setté : quand on lance kubectl on est connecté sur ce cluster et on peut modifier biensur cette valeur.

- kubectl commandes :


kubectl config view :


kubectl  config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://192.168.8.10:6443
  name: kubefordev
- cluster:
    certificate-authority: /home/boogie/.minikube/ca.crt
    server: https://192.168.99.100:8443
  name: minikube
- cluster:
    certificate-authority-data: REDACTED
    server: https://192.168.1.51:6443
  name: kubeprod
contexts:
- context:
    cluster: kubefordev
    user: boogie-dev-cert
  name: kubefordev-cert
- context:
    cluster: minikube
    user: minikube
  name: minikube
- context:
    cluster: kubeprod
    user: boogie-prod-cert
  name: kubeprod-cert
current-context: minikube
kind: Config
preferences: {}
users:
- name: boogie-dev-cert
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: boogie-prod-cert
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: minikube
  user:
    client-certificate: /home/boogie/.minikube/client.crt
    client-key: /home/boogie/.minikube/client.key

- modif de context :

kubeclt config use-context boogie-dev@kubedev 


>>> on a donc setté notre context par défaut sur un autre cluster maintenant.

- kubectl -h : liste des options possibles.

- Namespaces :

on va pouvoir setté un namespace particulier dans un contexte :

ex: on set le namespace kube-system par defaut dans le contexte kubedev 
...
contexts:
- context:
    cluster: kubefordev
    user: boogie-dev-cert
    namespace: kube-system 
  name: kubefordev-cert
....

- on peut définir les eléments tls dans notre kubeconfig :

on peut mettre le path de nos cert ou alors ( comme dans les exemples précédents ) mettre les data que l'on aura chiffrer au préalable 

echo "LsCCZD.." |base64 et que l'on renseigne dans  la section 

- cluster:
    certificate-authority-data:

ch7 v10 -> done 

= api kube = 

toutes les opérations sont effectuées via l'api (en rest ou via kubectl )

ex: 
curl https://kube-api:6443/version
curl https://kube-api:6443/api/v1/pods


on va donc se focus sur les parties qui nous interressent ici : /version /api  /logs /healthz /metrics /apis 

-> les points d'entrés sont regroupés en fonction des types.


- on va se focus sur /api et /apis


- /api : core group 
ici on a toutes les fonctionalitées qui existent.

/api/v1 :
/pods
/rc
/namespace
/nodes
/events
..
.....



- /apis : named group 
cette section est plus organisée et toutes les nouvelles features seront accessible via cette entrée.
/apps/ :
  /v1 :
    /deployments > create, delete, update.
    /replicasets
    /statefulsets
    ...
/extensions
/networking.k8s.io:
  /networkpolicies
  ..
/storage.k8s.io
/authentication.k8s.io
/certificates.k8s.io:
  /certificatesigningrequests


on a donc au premier niveau : les api groups (ex : apps, /certificates.k8s.io ..)
a un niveau plus fin les resources ( /deployments, /certificatesigningrequests ..)
on peut agit de differentes maniere sur les resource a l'aide de verbs : create , delete , update ...

on va pouvoir a l'aide de la doc kubernetes trouver a quel group d'api appartient quelle ressource.

on peut interroger via curl les differents points d'entrées 

[root@kmaster01 ~]# curl -k http://172.28.128.3:6443/version
[root@kmaster01 ~]# curl -k http://172.28.128.3:6443/api |grep name

on doit s'authentifier en utilisant notre cert, clé et ca 
curl http://localhost:6443 -k -cert admin.cert -key admin.key -cacert ca.crt


on peut sinon utiliser kubectl proxy qui va permettre d'acceder a notre cluster via un proxy 

[root@kmaster01 ~]# kubectl proxy
Starting to serve on 127.0.0.1:8001

en lancant une autre console on peut récupérer les infos de notre cluster 

Attention : kubeproxy != kubectl proxy 

ch7 v11 -> done 

= rbac : role base access control =

on va pouvoir créer un role de la même maniere que les autres objets kube :
on va donner un nom au group que l'on crée et trois sections seront mandatory ( apiGroups, ressources, verbs : comme on l'a vu dans la section api de kube) 
pour les ressources de type core on peut laisser la section apiGroups vide  

cat developer-role.yaml
apiversion: rbac.authorization.k8s.io/v1
kind: role 
metadata: 
  name: developer
rules: 
- apiGroups: [""]   
  ressources: ["pods"]
  verbs: ["list", "get", "create", "update", "delete"]
on peut biensur créer plusieurs rules dans notre role : ex on va rajouter la possibilité au group dev de créer des configmap :
- apiGroups: [""]   
  ressources: ["ConfigMap"]
  verbs: ["create"]
  
kubectl create -f developer-role.yaml


on va maintenant devoir linker les users à notre role : pour cela on va créer un nouvel object RoleBinding :
on relie un object user à un object role 
Il y a deux sections : 
Subjects : on y renseiggne les infos du user
RoleRef : va servir a definir les infos du role que l'on a crée


cat developper-rolebinding.yaml
apiversion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata: 
  name: devuser-developer-binding
Subjects:
- kind:
  name: devuser
  apiGroup: rbac.authorization.k8s.io
RoleRef:
  kind: Role
  name: devloper
  apiGroup: rbac.authorization.k8s.io
  
  

kubectl create -f developper-rolebinding.yaml

Attention : les roles et rolebindings sonr associés au namespace par default : si on veut définir un scope particulier il faudra donc d&éfinir explicitement le namesapce concerné par nos  configurations.

on peut examiner les roles/rolebindings  crées avec les commandes :

kubectl get roles 
kubectl get rolebindings

kubectl describe role developer
kubectl describe rolebinding  devuser-developer-binding


Pour examiner les droits qu'un user dispose :

kubectl auth can-i create deployements
yes


kubectl auth can-i delete nodes
no


on peut en tant qu'admin examiner les droits d'un user :

kubectl auth can-i create deployements --as dev-user
no

kubectl auth can-i create pods --as dev-user
yes

on peut definir les perms sur un namespace aussi :

kubectl auth can-i create deployements --as dev-user --namespace test
no

Il va être possible de définir des droits specifiques a certaines ressources : ex acces uniquement a certains pods :


cat developer-role.yaml
apiversion: rbac.authorization.k8s.io/v1
kind: role 
metadata: 
  name: developer
rules: 
- apiGroups: [""]   
  ressources: ["pods"]
  verbs: ["list", "get", "create", "update", "delete"]
  ressourcesNames: ["app-web1", "app-vcs2"]

ch7 v12 -> done


=  cluster roles  / cluster roles binding = 

on a vu que les roles et rolesbinding sont associés à un namespace donné.

Comment peut on faire pour les nodes ? Ce type de ressource n'est pas associée à un namespace : on est dans ce cas dans le domaine du cluster.

Les ressources sont donc catégorisées en 2 :

-> namespaced : pods, replicatset, jobs, services ....
-> cluster scoped : nodes, cluster roles, clusters roles binding , namespaced, certificate signing requests 


on peut examiner les différents type de ressources avec :

kubectl api-resources --namespaced=true --> on va voir ici toutes les ressources du scope 'namespace'
kubectl api-resources --namespaced=false --> on va voir ici toutes les ressources du scope 'cluster'

Comment donner le droits a des users de gérer certaines parties du cluster ???
avec les  cluster roles et les clusterroles binding 

ex: 
cluster-admin-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-admin
rules:
  apigroups: [""]
  resources: ["nodes"]
  verbs: ["list", "create", "delete", "get"]

kubectl create -f cluster-admin-role.yaml


on va maintenance linker le user à notre cluster role :

ex: clusteradmin-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
subjects: 
- kind:
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef: 
  kind: 
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io

kubectl create -f  clusteradmin-rolebinding.yaml

ch7 v13 -> done

= image security =

- basics memo :

on deploy plein d'appli  avec des images

quand on deploit un pod de maniere basique on peut avoir par exemple une def très simple :

apiVersion: v1
kind: Pod 
metadata:
  name : nginx-pod
spec:
  - containers:
    - name: nginx
      image: nginx 

ici on deploit une image nginx pour un container nginx qui formera un pod nginx-pod       
  
ici on voit que l'image est tres simple : nginx : cela suit la convention d'image docker 

image: nginx => image repository
en fait si on on prefixe rien c'est qu'on a alors le même nom de repo pour le user account 
on sous entend : nginx(user account) /nginx (repo name) 

mais d'ou viennent les images ?
si on ne precise rien alors les images seront issues de la registry docker : on sous entend donc :

image : docker.io/nginx/nginx 

docker.io est la registry par defaut.

quand  on update notre image on va la pousser dans notre régistry ...

on a biensur plusieurs registry possible  :
ex :
gcr.io/kubernetes-test/dnsutils

en interne il est important d'avoir une registry private.

on doit pouvoir y avoir access avec des credentials 

ex : docker login private-registry.io
username: registry-user
password: 


une fois authentifié on pourra utiliser les images de la registry :

docker run private-registry.io/apps/internal-app


pour indiquer le chemin complet de notre image dans notre pod on doit donc renseigner le path de la registry.
ex: 
      
mais comment allons nous passer les crédentials a notre runtime docker sur les workers pour recupérer l'image ? 

on va créer un object de type secret en y renseignant les crédentials :

kubectl create secret docker-registry regcred \
  --docker-server=private-registry.io
  --docker-username=registry-user
  --docker-password=registry-password
  --docker-email=registry-user@org.com

et on va ajouter ce secret dans la definition de notre pod : 
apiVersion: v1
kind: Pod 
metadata:
  name : nginx-pod
spec:
  - containers:
    - name: nginx
      image: private-registry.io/apps/nginx 
    imagePullSecrets:                <<<<<<< section dans laquelle on rajoute le contenu du secret a injecter pour recupérer l'image de notre registry private 
    - name: regcred  

ch7 v14 -> done


= security contexts : =

on peut en lancant un container passer des arguments pour la secu ( definir le user, les capabilities )
docker run --user=1001 ubuntu sleep 100
docker run --cap-add MAC_ADMIN ubuntu

on peut biensur le faire dans kuberentes :
on peut setter notre secu au niveau :
-> container 
ou 
-> pod 

si on configure au niveau pod tous les containers au sein du pod seront affectés par les regles de secu.
Les settings fait au niveau du container overrident ceux au niveau du pod.

ex: 
on peut rajouter la conf secu au niveau du pod en rajoutant la section securityContext dans le pod 

apiVersion: v1
kind: Pod 
metadata:
  name : nginx-pod
spec:
  securityContext:
    runAsUser: 1001

  - containers:
    - name: nginx
      image: private-registry.io/apps/nginx 
      command: ["sleep", "3600"]

on peut rajouter la conf secu au niveau du container en rajoutant la section securityContext dans le pod 

apiVersion: v1
kind: Pod 
metadata:
  name : nginx-pod
spec:

  - containers:
    - name: nginx
      image: private-registry.io/apps/nginx 
      command: ["sleep", "3600"]
      securityContext:
        runAsUser: 1001
        capabilities:
          add: ["MAC_ADMIN"]
          
ch7 v15 done

= network policies intro : =

ex: on a un traffic web classic 


client  -> web    ->  api -> db 


on a deux type de traffic :

quand le server web recoit du traffic c'est du ingress 
quand le server web envoi du traffic a l'api c'est de l'egress

bien sur le traffic que recoit l'api du server web est de l'ingress 

on va avoir schematiquement plusieurs regles :

1/ ingress  port 80 (web <- client ) 
2/ egress   port 5000 ( web -> api) 
3/ ingress port 5000  (api <- web )
4/ egress port 3306 ( api -> db) 
5/ ingress port 3306 (db <- api)


dans kubernetes :
chaque node, service et pod a une adresse ip

Chaque pod doit communiquer avec tous les autres pods : c'est la regle : ils doivent communiquer  par l'adresse des pods ou service : toutes les regles sont autorisées.

il va donc falloir securiser nos applis :

nous ne voulons pas que notre pod web communique directement avec le pod db mais on veut que cela passe systematiquement par l'api

on va donc créer une network policie qui va permttre cela :

niveau db on crée une regle qui 
autorise l'ingress sur le port 3306 depuis le pod  api : tous les autres flux seront bloqués.

on va pour cela créer un  object en utilisant des labels :
ex: 

network-policie-db.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector: 
    matchLabel: 
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    -podSelector:
      matchLabels:
        name: api-pod
    ports:
    -protocol: TCP
     port: 3306


on cree l'object :

kubectl create -f network-policie-db.yaml

Attention les network policies ne sont appliquée que par des cni qui les gère ( calico, kube-router, weave ...) .
Flannel ne les gere pas : mais ne crée pas d'erreur .

Il faut donc bien choisir sa solution réseau pour les pods.

ch7 v16 -> done 

== Volumes : ==


Beaucoup de solutions de stockage sont disponibles pour kubernetes.
Une connaissance de la gestion interne doit nous permettre d'adapter nos besoins facilement avec des solutions externes (third parties).

= volumes : =


- concepts : 
on va d'abord se focus sur la notion de volumes dans docker : de base les données ne sont pas persistantes dans docker : une suppression de container implique naturellement une suppression de data : qui sont donc de base volatiles.
pour assurer de la persistance à nos data on va attacher un volume que l'on aura créer au prealable à notre container : même si notre container est détruit pas de souci les data sont présentes et un nouveau container peut les exploiter.

Le  principe est le même dans kubernetes : les pods par nature sont éphémère et perdent donc les data quand ils sont détruits.
On va donc attacher un volume dans  un pod : si celui ci est détruit pas de souci un nouveau pod pourrra accéder au data du volume en le montant.

- Implémentation :

ex: on va créer un pod qui va écrire dans le fichier d'un repertoire un nombre aléatoire : 

on va définir un volume qui va utiliser une méthode de stockage ( drivers nécéssaire à l'utilisation du stockage) 
ici on va monter un volume local à notre host et specifier que le stockage sera dans un repertoire local à notre systeme.
On va monter ce volume dans notre container en définissant un point de montage et le volume dédié crée sur notre host.

apiVersion: v1
kind: Pod 
metadata:
  name : random-number
spec:

  containers:
  - name: alpine
    image: alpine
    command: ["/bin/sh", "-c"]
    args: ["shuff -i 0-100 n -1 >> /opt/numb.out";]
    volumeMounts:                   <<<<<   definition du montage au sein de notre container
      mountPath: /opt               <<<<<   point de montage sur lequel on stockera / accedera au data depuis le container
      name: data-volume             <<<<    nom du volume utilisé   
  volume:                           <<<<< def de notre volume 
  - name: data-volume                  
    hostPath:                       <<<< definition d'un volume local à notre host 
      path: /data                   <<<< point de montage de notre host
      type: Directory               <<<< type de montage : ici c'est un repoertoire simple.

      

quand on aura ecrit un nombre aléatoire via la commande de notre pod : le fichier généré sera accessible au sein du pod dans /opt ..et forcement en local sur le host dans /data ..la suppression du pod n'affectera pas la persistance  de données.


- Options de volumes : 

Si le montage d'un volume local est interessant pour un pod sur un node ceci est absolument déconseillé sur un cluster de plusieurs nodes : les pods de chacun des nodes auront un volumes /opt propre a repoertoire /data de chaque node  : ils seront donc potentiellement différents : ce qui génere de l'inconsistance  de données ...

NB: On utilisera pas l'option :"hostPath" pour un systeme de production.

Dans le cas d'un cluster de plusieurs nodes ont va devoir utiliser des solutions de stockage repliquées ( de nombreuses solutions existent : glusterfs, ceph, 
nfs, google ...)

Pour renseigner un volume avec un systeme externe on va simplement renseigner le type dans notre déclaration.
ex avec un volume amazon :

apiVersion: v1
kind: Pod 
metadata:
  name : random-number
spec:

  containers:
  - name: alpine
    image: alpine
    command: ["/bin/sh", "-c"]
    args: ["shuff -i 0-100 n -1 >> /opt/numb.out";]
    volumeMounts:                   
      mountPath: /opt              
      name: data-volume           
  volume:                        
  - name: data-volume                  
    awsElasticBlocStore:               
      volumeID: < volume-id >           
      fsType: ext4                       

ch8 v2 -> done


= persistent volume : =


on a vu que la configuration d'un volume implique une définition dans chaque pod. Si nous avons plusieurs users chacun devra configurer le volume dans sa definition de pod ..et la modifier en cas de changement .

On va vouloir pouvoir gérer le volume de maniere plus centralisée. l'admin va créer un pool de storage qui seront a dispo pour les users qui donc pourront selectionner un volume pour le deploiment de leur appli . 
cest ce qu'on appelle un "persistant volume claims" : PVC 

ex :

pvc-definition.yaml

apiVersion: v1
type: PersistantVolume
metadata: 
  - name : pv-vol1
spec: 
  accessModes: 
    - ReadWriteOnce
  capacity: 
    storage: 1Gi
  awsElasticBlocStore:               
    volumeID: < volume-id >           
    fsType: ext4                       
    
kubectl create -f pvc-definition.yaml

pour examiner nos pvc :

kubectl get persistentvolume 

ch8 v3 -> done.

= persistant volume claims : =

on a vu comment declarer un pvc maintenant on va voir comment mettre a dispo se volume claim pour un node.

nous avons deux type d'objects : un pv : persistant volume que l'admin met a dispo et un pvc que le user crée pour utiliser le pv 
une fois que les objects sont crées kubernetes va binder les objects pv et pvc entre eux en fonction des properties definies dans les volumes.

Chaque pvc est bindé sur un pv . pendant l'operation de montage kubernetes va chercher un pv ayant la volumétrie, les modes d'access, le type de storage définis dans le pvc.
Si plusieurs volumes sont candidats : on pourra toujours sélectionner un pv particulier en utilisant les labels et selectors.

On pourra donc avoir un gros volume de monté sur un petit claim s'il n'y a pas d'autre meilleure option
Il y a une relation 1/1 entre les claims et les volumes.
Si aucun volume n'est dispo alors le volume claim restera en etat pending jusqu'a ce qu'un nouveau volume soit dispo : dans ce cas le montage sera automatique.


- declaration et creation de notre object : 
pvc-definition.yaml

apiVersion: v1
type: PersistantVolumeClaim
metadata:
  - name: my_claim
spec:
  accessModes:
    - ReadWriteOnce
  ressources:
    requests: 
      storage: 500Mi

kubectl create pvc-definition.yaml

on va pouvoir examiner nos pvc avec : 

kubectl get persistantvolumeclaim

Quand on va examiner le pvc et le volume precedement crée on va  voir que les options matchent :

pvc-definition.yaml

apiVersion: v1
type: PersistantVolume
metadata: 
  - name : pv-vol1
spec: 
  accessModes: 
    - ReadWriteOnce
  capacity: 
    storage: 1Gi
  awsElasticBlocStore:               
    volumeID: < volume-id >           
    fsType: ext4                       

on va maintenant pouvoir voir que le pvc a bien ete bindé sur notre pv 

kubectl get persistantvolumeclaim

NAME    STATUS  VOLUME  CAPACITY ACCESS MODES STORAGECLASS AGE
myclaim Bound   pv-vol1  1GBi    RWO                       3mnts

- pour supprimer un pvc :

kubectl delete persistantvolumeclaim myclaim

Nous avons plusieurs options pour gérer le volume.
De maniere native le volume est en mode :
persistantVolumeReclaimPolicy:  Retain 

--> le volume sera conservé jusqu'à ce qu'il soit détruit manuellement par l'admin.
Le volume ne sera pas réutilisable pour d'autres claims.

On peut sinon decider de supprimer le volume :
persistantVolumeReclaimPolicy:  Delete

dans ce cas des que le claim est supprimé le volume est egalement supprimé.

On peut egalement "wipper" les data a la suppression du claim :

persistantVolumeReclaimPolicy:  Recycle

ch8 v4 -> done

== networking : ==

= linux basic network /routing /coredns =

- switching :

pour permette la connex d'un pc1 a un pc2 on va connecter une interface de ces pc a un switch.

on va voir les ifaces avec la commande  : ip link 

ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: enp0s31f6: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc pfifo_fast state DOWN mode DEFAULT group default qlen 1000
    link/ether 18:db:f2:3a:2c:4a brd ff:ff:ff:ff:ff:ff

pour setter une ip on peut utiliser ip addr
sudo ip addr add 192.168.0.10/24 dev enp0s31f6

on va voir l'ip :

 ip a show enp0s31f6
2: enp0s31f6: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000
    link/ether 18:db:f2:3a:2c:4a brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.10/24 scope global enp0s31f6
       valid_lft forever preferred_lft forever

une fois qu'on a fait l'opération sur les deux pc ..ceux ci peuvent communiquer ensemble via le switch 
Celui ci peut delivrer les packets des hosts au sein d'un même network

si on a deux reseau :

192.168.0.1/24 et 192.168.0.2/24 comment faire pour leur permettre de communiquer ?

on va dans ce cas devoir ajouter un router dont un des jobs est de permettre à différents réseau de communiquer entre eux.

Pour relier deux reseaux differents le routeur doit donc avoir deux interfaces diférentes.

ex :


pc1 : 192.168.1.10
pc2 : 192.168.1.20

swich1 : 192.168.1.0/24 


routeur:if1: 192.168.1.1
routeur:if2: 192.168.2.1

pc3 : 192.168.2.10
pc1 : 192.168.2.10

swich2 : 192.168.0.2/24 

comment pc2 sait communiquer avec pc3 ? 
on va communiquer via une gateway.

Le reseau est une piece et la gateway est une porte permettant l'acces au monde exterrieur.
on va devoir emprumpter une route pour communiquer ...

la commande route permet de montrer les differentes routes 

 sudo route
Table de routage IP du noyau
Destination     Passerelle      Genmask         Indic Metric Ref    Use Iface
link-local      0.0.0.0         255.255.0.0     U     1000   0        0 br-110f911da12d
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
172.18.0.0      0.0.0.0         255.255.0.0     U     0      0        0 br-110f911da12d
172.19.0.0      0.0.0.0         255.255.0.0     U     0      0        0 br-9f7e0265b97b
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 enp0s31f6

pour permettre aux deux reseaux de communiquer on va configurer notre route disant au reseau 1 qu'il peut contacter le reseau2 en passant par l'interface 192.168.1.1 qui est l'inerface de notre routeur 

ip route add 192.168.2.0/24 via 192.168.1.1
biensur il faut le faire sur tous les systemes : pc3 doit pouvoir communiquer avec pc2 

ip route add 192.168.1.0/24 via 192.168.2.1


pour que nos pc sur 192.168.1.0/24 puissent acceder a internet (admettons une ip publique en 172.217.194.0 que l'on veut joindre pour google : on va rajouter une route de la meme maniere 
(il faut que notre routeur soit connecter a la sortie internet )

ip route add 172.217.194.0 via 192.168.1.1

biensur il n'est pas possible de rajouter toutes les routes pour tous les sites .

On va donc rajouter une route par defaut qui permettra de se connecter à tous les réseaux en passant par une interface  

ip route add default via 192.168.1.1
on peut definir à la place de default le 0.0.0.0 qui signifie toutes les destinations.

- comment definir un host linux en router : 

si on a trois pc 

a  192.168.1.2 
b  192.168.1.7 / 192.168.3.7 -> on doit avoir un host avec 2 interfaces  
c  192.168.3.2

sur a : 
ip route add 192.168.3.0/24 via 192.168.1.7


sur b : 
ip route add 192.168.1.0/24 via 192.168.3.7


de base les paquets sur linux ne sont pas forwarder d'une interface à une autre : par raison de securité.
on est sur un reseau privé donc pas de souci :

pour transformer notre pc en router on va définir a 1 le contenu du fichier :

echo 1 > /proc/sys/net/ipv4_forward

il faut modifier l'entree dans le fichier /etc/sysctl.conf pour garder la valeur :

net.ipv4_forward = 1

ch9 v2 -> done

= Dns configuration: (absolute beguiners) =

A : 192.168.0.1.10 

B : 192.168.0.1.11
on va mettre un alias avec db 


-> on ajoute une entrée dans le /etc/hosts de nos servers :

ping db depuis A est ok


biensur on met se que l'on veut dans la hosts : on peut mettre google pointant sur 192.168.1.11 

ceci ne verifie pas que l'on est bien  sur le bon hosts ...

biensur ceci est ingérable plus on multiplie les entrées.

-> on installe un dns 

ex : 192.168.1.100 

on va ajouter sur nos clients l'entrée de ce serveur dans le fichier /etc/resolv.conf 

nameserver 192.168.1.100

biensur on peut utiliser le /etc/hosts pour des tests.
si on a deux entrées (/etc/hosts et dns) : il y a un e priorité :

Il y a un ordre de priorité dans la lecture des infos :

/etc/nsswitch.conf 



 boogie   master ✚ 1 … 1  ~  Documents  stuff  cat /etc/nsswitch.conf
# /etc/nsswitch.conf
#
# Example configuration of GNU Name Service Switch functionality.
# If you have the `glibc-doc-reference' and `info' packages installed, try:
# `info libc "Name Service Switch"' for information about this file.

passwd:         compat systemd
group:          compat systemd
shadow:         compat
gshadow:        files

hosts:          files mdns4_minimal [NOTFOUND=return] dns myhostname
networks:       files

protocols:      db files
services:       db files
ethers:         db files
rpc:            db files

netgroup:       nis

pour avoir des résolutions externes on peut utiliser une entrée externe : ex :
/etc/resolv.conf 
nameserver 8.8.8.8

- domain names : 

top level domain : ex : .com / .org 


.  -< root
.com : tld 
google.com < domain
www / apps / mail  ...< sous domain 
(www.google.com) 

pour conserver les entrées on va les mettre en cache pour un moment.


pour eviter dans un domaine interne de rajouter le nom de domaine complet ( ex : web.mycompany.com ) 

on va vouloir saisir web directement 
pour cela dasn notre /etc/resolv.conf : on va rajouter un champ search qui va être automatiquement compléter quand on cherchera un nom 

ex : on saisi web > le nom web.mycompany.com sera automatiquement ajouté et la recherche dns se fera sur ce domaine.

- records types :

- A : faire pointer un nom de domaine vers une ip 
- AAAA : faire pointer un nom de domaine vers une ipv-
- CNAME : faire pointer un nom vers un autre nom : food.steack.com -> lunch.meat.com 

- nslookup 

pour requeter un nom : attention cet outil ne requette pas le fichier /etc/hosts.

- dig 

ch9 v3 done.


== network namespaces ==

ils sont utilisés pour la separation / isolation des containers.
On peut dans une maison par analogie les representer par les chambres d'une maison appartenant chacune par exemple à un enfant  de la famille.
Chaque personne ne peut voir que ce qu'il y a dans sa chambre quand elle y est.
En tant que parent on a la visibilité sur toutes les chambres et si on veut on peut permettre la communication entre deux chambres.

quand on crée un container on veut s'assurer qu'il soit parfaitement isolé.
on a pour cela un process namespace qui permet au container de ne voir que celui ci et donc penser qu'il est isolé sur son propre host.
on a depuis le container la vision que le pid 1 est le process init comme sur un host.
depuis le host on voit le process init du container comme n'importe quel autre process avec forcement un autre pid.

- network namespace :

chaque host a une iface connecté au lan.
chaque host dispose de sa table de routage et arp 
quand on crée un container  celui ci n'a aucune visibilité sur les informations de reseaux du host . Le container dispose de sa propre virtual iface ainsi que se propre table de routage et arp .

Pour créee un nouveau network namespace sous linux :

ip netns add notre_namespace

sudo ip netns  add blue
sudo ip netns  add red
sudo ip netns  list
red
blue

pour voir l'interface créee dans notre namespace : 
ip netns exec notre_ns link
ou

ip -n notre_namespace link 


sudo ip netns exec red ip link
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00

sudo ip -n red link
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00

on a donc une isolation dans notre namespace : aucune visibilité de l'iface du host.

- tables routage et  arp :

c'est le même principe pour les tables de roitages et de cache mac address :

sudo ip netns exec red route
Table de routage IP du noyau
Destination     Passerelle      Genmask         Indic Metric Ref    Use Iface
sudo ip netns exec red arp


- connexion de deux namespaces réseaux ensemble :

on va pouvoir raccorder plusieurs ns ensemble via un "cable virtuel"

- creation du cable  virtuel :
sudo ip link add veth-red type veth peer name veth-blue

- on attache maintenant ce cable aux namespaces dédiés : 

ip link set veth-red netns red 
ip link set veth-blue netns blue

- on va maintenant pouvoir assigner une ip à nos interfaces :

sudo ip -n red addr add 192.168.15.2 dev veth-red
sudo ip -n blue addr add 192.168.15.3 dev veth-blue

sudo ip -n red addr add 192.168.15.2 dev veth-red
sudo ip -n blue addr add 192.168.15.3 dev veth-blue
sudo ip -n red link set veth-red up 
sudo ip -n blue link set veth-blue up 


- on peut passer des commandes depuis notre namespace : 

sudo ip netns exec red ping 192.168.15.3
sudo ip netns  exec blue arp
sudo ip netns  exec blue route -n


Quand on a plusieurs machinnes cela devient compliqué a gérer : on va docn créer un virtual switch 
on a plusieurs solutions qui s'offrent à nous : linux bridge, openvswitch  ....

- linux bridge :
on va ajouter une nouvelle iface de type bridge pour gérer notre réseau 192.168.15.0/24

sudo ip link add v-net-0 type bridge

sudo ip link |grep v-net-0
9: v-net-0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000

 on l'active :

sudo ip link set dev v-net-0 up
sudo ip link |grep v-net-0
9: v-net-0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000

on va donc maintenant vouloir connecter tout nos namespaces sur notre switch :

on va delete le link entre les namespace que l'on a créée précédemment :

sudo ip -n red link del veth-red

il suffit de delete une extremité dans ce cas l'autre est delete automatiquement : 

udo ip -n blue link del veth-blue
Cannot find device "veth-blue"

on va maintenant créee des ifaces virtuelles qui vont pointer sur des ifaces virtuelles de birdge : 
sudo ip link add veth-red type veth peer name veth-red-br
sudo ip link add veth-blue type veth peer name veth-blue-br


on va maintenant raccorder notre namespace à l'iface puis 
l'interface virt bridge au bridge :

sudo ip link set veth-red netns red
sudo ip link set veth-red-br master v-net-0
sudo ip link set veth-blue netns blue
sudo ip link set veth-blue-br master v-net-0

on va maintenant setter une ip et permette au deux ns de communiquer : 

sudo ip -n red addr add 192.168.15.2 dev veth-red
sudo ip -n blue addr add 192.168.15.3 dev veth-blue
sudo ip -n red link set veth-red up
sudo ip -n blue link set veth-blue up



sudo ip netns exec red ping 192.168.15.3


sur notre host on rajoute une ip dans le reseau virtuel :

sudo ip addr add 192.168.15.5/24 dev v-net-0
et on peut donc joindre notre reseau virtuel depuis notre host :

ping 192.168.15.3

Il faut bien noter que ce réseau est entierement situé sur le host et restreint à son périmètre.
La seule porte d'acces au reste du monde est l'iface sur notre host.

Comment permettre donc l'acces au net et aux autres réseaux depuis nos namespaces ?

si on veut pinger une ip 192.168.1 depuis un namespace blue en 192.168.15 on y arrive pas .. il n'y a pas d'information dans la table de routage.
Il va falloir ajouter une gateway permettant aux ns de communiquer avec le réseau voulu.

LA seule iface qui est dans plusieurs reseaux est la virtuelle iface : v-net-0 192.168.15.5 qui comunique avec l'ip 192.168.1.2 de eth0 sur notre host

ip net exec blue ip route add 192.168.1.0/24 via 192.168.15.5

On ne pourra cependant toujours pas contacter l'exterrieur : il va falloir 'nater' nos flux : en transformant les ip 192.168.15.0/24 de nos namespaces qui vont sortir et chercher à communiquer avec d'autre réseaux . pour cela on utilise  iptables le nat et le masquerading 


iptables -t nat -A POSTROUTING -s 192.168.15.0/24  -j MASQUERADE 

> tous les flux des ns sortiront avec l'ip de notre gateway et les réseaux destinations n'auront en aucun cas connaissance de la source des ns initiaux.

Pour s'assurer que tous nos namespaces pourront communiquer avec le net et pas uniquement le réseau 192.168.1.0/24 on va rajouter une gateway par defaut :

ip netns blue exec ip route add default via 192.168.15.5 

cette fois le net est accessible.


Comment maintenant permettre aux autres réseaux de se connecter aux namespaces internes puisque les ip ne sont pas connues ?
admettons que  dans notre ns blue on ait une appli web comment faire pour l'atteindre depuis un host de notre lan ?

on a deux choix ajouter une route pour dire que tout le réseau 192.168.15.0/24 sera joignable via la 192.168.1.2
ou alors affiner notre filtrage : c'est ce qu'on privilégie 
on va s'assurer que les requettes sur le port 80 seront bien envoyées sur le container/ appli de l'ip 192.168.15.2 ecoutant sur le port 80 , en natant ce flux.

iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2:80 -J DNAT 

ch9 v4 -> done 

= docker networking =










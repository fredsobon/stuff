=== notes trow.io : ===

trow est une registry kube légere en rust qui va permettre de deployer des images en cli.
trow utilise a l'heure actuelle kustomized à la place de helm

= setup :

trow utilise le tls pour permettre de securiser les connections avec la registry.
un certificat ssl sera don nécéssaire pour attaquer le point d'entrée de notre appli : ce certificat sera lié à notre ingress.
trow va utiliser du stockage statique

- pre-requi

-> on va devoir installer un ingress-controller pour gérer l'acces à notre appli 

helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
on le deploi dans un namespace dédié :
kctl create ns ingress-controller
helm install ingress-controller -n ingress-controller ingress-nginx/ingress-nginx

->on va pour gérer la partie tls installer cert-manager et pour notre exemple definir une ca privée

kubectl create ns cert-manager


helm repo add jetstack https://charts.jetstack.io\n
helm install cert-manager jetstack/cert-manager --namespace cert-manager --version v0.15.0 --set installCRDs=true

on cree notre ca et on déploy notre secret 

openssl genrsa -out rootCA.key 2048
openssl req -new -x509 -key rootCA.key -out rootCA.crt

kubectl create secret tls ca-priv --namespace cert-manager --key rootCA.key --cert rootCA.crt

on crée une ressource de type clusterissuer :

apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: ca-issuer
spec:
  ca:
    secretName: ca-priv

 kctl create -f ca-issuer.yaml

on test :
kubectl get clusterissuers.cert-manager.io
NAME        READY   AGE
ca-issuer   True    4h59m

notre ca est prete on peut créer une ressource qui va générer un csr qui sera signée et un  certificate sera  créée :
on va renseigner le nom qui sera appellé et configuré dans l'ingress : notre point d'entrée dans le cluster depuis l'exterrieur pour atteindre notre appli.



apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: paasregistry.lapin.io
  namespace: trow
spec:
  # Secret names are always required. On va definir un nom pour identifier notre secret tls :
  secretName: trow-cert-tls
  duration: 51600h # 90d
  renewBefore: 36000h # 15d
  organization:
  - srs
  # The use of the common name field has been deprecated since 2000 and is
  # discouraged from being used.
  commonName: paasregistry.lapin.io
  isCA: false
  keySize: 2048
  keyAlgorithm: rsa
  keyEncoding: pkcs1
  usages:
    - server auth
    - client auth
  # At least one of a DNS Name, USI SAN, or IP address is required.
  dnsNames:
  - paasregistry.lapin.io
  - trow.lapin.io
  # Issuer references are always required.
  issuerRef:
    name: ca-issuer
    # We can reference ClusterIssuers by changing the kind here.
    # The default value is Issuer (i.e. a locally namespaced Issuer)
    kind: ClusterIssuer
    # This is optional since cert-manager will default to this value however
    # if you are using an external issuer, change this to that issuer group.
    group: cert-manager.io



-> on va devoir dans notre cas créer deux volumes qui seront utilisés par le statefullset qui incorpore aussi un pvc générator et un pvc pour la config 

le mode op d'install de trow nous fait partir depuis le repo du repertoire install :
on copie le rep overlays/example-overlay pour notre conf :
cp -r overlays/example-overlay overlays/paasregistry-lapin
 tree -d
.
├── base
└── overlays
    ├── cert-manager-nginx
    ├── example-overlay
    ├── gke
    ├── paasregistry-lapin

En principe nous ne touchons pas au rep base qui contient les manifests essentiels de l'appli :

tree base
base
├── kustomization.yaml
├── patch-trow-arg.yaml
├── patch-validator-domain.yaml
├── pvc.yaml
├── service.yaml
├── stateful-set.yaml
└── validate.yaml

/!\ a supprimer apres tests pas besoin de pvc supplementaire : on ajoutera juste un seul volume pour les data de trow puisque nous n'avons pas de volumes provisionner :

dans notre cas nous créons cependant un persistant volume claim dans ce rep ( nous n'avons pas tester la creation de cet objet dans le rep d'overlays/paasregistry-lapin 

cat base/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: trow-data-claim
spec:
    accessModes:
        - ReadWriteOnce
    resources:
        requests:
            storage: 10Gi

comme c'est une nouvelle ressource elle doit être intégrée au fichier kustomized ( c'est un des principes de customized ) :

cat base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: trow

# This will start Trow in a StatefulSet along with a persistent volume for data and a service that
# routes to it.
#
# A working install will require an ingress forwarding a (sub)domain you control to this service.
# This install assumes the ingress terminates TLS; internally Trow is running over http only.
#
# The overlay directories include examples of how to provision ingress on various types of
# Kubernetes cluster.


resources:
- stateful-set.yaml
- service.yaml
- pvc.yaml

#- validate.yaml # Enable for validation webhook

images:
- name: containersol/trow
  newTag: "0.2"

# The following patches update the domain name in the trow argument and validator without editing
# the YAML directly.
# Create your own version of the patch file with your domain name and reference in your
# overlay as below:


patchesJson6902:
#    - path: patch-trow-arg.yaml
#      target:
#        kind: StatefulSet
#        name: trow-set
#        group: apps
#        version: v1
#
#    - path: patch-validator-domain.yaml
#      target:
#        kind: ValidatingWebhookConfiguration
#        name: trow-validator
#        group: admissionregistration.k8s.io
#        version: v1


/!\ fin de section a delete apres tests. /!\

on va maintenant ajouter dans notre rep d'overlay dédié à notre cluster ajouter deux volumes que nous créons manuellement avec une strategie de hostpath : storage local sur les nodes 


update : on ne va créer qu'un seul volume :

cat pv-data-trow.yaml 
 apiVersion: v1
 kind: PersistentVolume
 metadata:
   name: trow-data-hostpath-pv
   labels:
     app: trow
 spec:
   accessModes:
   - ReadWriteOnce
   capacity:
     storage: 10Gi
   hostPath:
     path: /data


 cat pv-trow.yaml     
 apiVersion: v1
 kind: PersistentVolume
 metadata:
   name: trow-hostpath-pv
   labels:
     app: trow
 spec:
   accessModes:
   - ReadWriteOnce
   capacity:
     storage: 10Gi
   hostPath:
     path: /data


on va integrer nos deux volumes dans la conf kustomized pour qu'ils soient pris en compte :

cat kustomization.yaml                                        
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: trow-example

bases:
  - ../cert-manager-nginx

generatorOptions:
    disableNameSuffixHash: true

secretGenerator:
  - name: trow-pass
    literals:
    - pass=blabla
  - name: trow-cred
    type: docker-registry
    literals:
    - docker-server=example.registry.com
    - docker-username=example
    - docker-password=blabla

patchesJson6902:
  - path: patch-ingress-host.yaml
    target:
      kind: Ingress
      name: trow-ingress
      group: extensions
      version: v1beta1
  - path: patch-trow-arg.yaml
    target:
      kind: StatefulSet
      name: trow-set
      group: apps
      version: v1

resources:                 <<< on ajoute un bloc resources avec nos deux volumes 
  - pv-trow.yaml
  - pv-data-trow.yaml


on va maintenant modifier les fichiers :

- patch-ingress-host.yaml : on renseigne l'entree de l'ingress qui va nous servir 

cat patch-ingress-host.yaml
- op: replace
  path: /spec/rules/0/host
  value: paasregistry.lapin.io
- op: replace
  path: /spec/tls/0/hosts/0
  value: paasregistry.lapin.io

- patch-trow-arg.yaml
on renseigne le domaine et le user défini dans le kustomized de notre cluster :  

cat patch-trow-arg.yaml
- op: replace #domain name
  path: /spec/template/spec/containers/0/args/2
  value: paasregistry.sandbox.ilius.io
- op: replace #user name
  path: /spec/template/spec/containers/0/args/4
  value: trow-pass


On va devoir appliquer des modif dans les fichiers overlay de cert-manager egalement 
 
 tree cert-manager-nginx
cert-manager-nginx
├── ingress.yaml
├── kustomization.yaml
└── patch-ingress-host.yaml


- ingress 

dans la conf ingress on renseigne notre point d'entrée dans le cluster notre nom de domaine et on definie le nom du secret pour notre certificat tls . On va egalement définir une ip 

cat cert-manager-nginx/ingress.yaml 
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: trow-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx"
    certmanager.k8s.io/cluster-issuer: "ca-issuer"     <<<< on defini notre ca comme défini plus haut avec cert-manager ( letsencrypt apparait sinon ..)
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
spec:
  rules:
    - host: paasregistry.lapin.io
      http:
        paths:
          - path: /
            backend:
              serviceName: trow-svc
              servicePort: 8000
  tls: 
    - hosts:
        - paasregistry.lapin.io
      secretName: trow-cert-tls  <<< le nom de notre certif

- patch-ingress-host.yaml : on reseigne notre point d'entrée 

cat cert-manager-nginx/patch-ingress-host.yaml
- op: replace
  path: /spec/rules/0/host
  value: paasregistry.lapin.io
- op: replace
  path: /spec/tls/0/hosts/0
  value: paasregistry.lapin.io


on va maintenant appliquer le kustomized sur nos arbos respectives pour appliquer les confs :

kubectl apply -k base/
kctl apply -k overlays/cert-manager-nginx
kctl apply -k overlays/paas-lapin

on va maintenant modifier notre ingress-controller  pour changer le service en Loadbalancer et attribuer une  ip fixe pour atteindre notre point d'entrée depuis l'exterrieur de notre cluster : les requettes externes passeront par l'ingress controller et son ip externe et le dispatch sera fait ensuite vers les ressources ingress qui enverront vers les services et enfin les pods 


=== conf kustomized ok :


- recap : on ne touche pas à base : 

tree base                                                                                          [☸ |kubernetes-admin@sandbox:trow]
base
├── kustomization.yaml
├── patch-trow-arg.yaml
├── patch-validator-domain.yaml
├── service.yaml
├── stateful-set.yaml
└── validate.yaml

on ne touche pas à overlay/cert-manager :
tree overlays/cert-manager-nginx                                                                   [☸ |kubernetes-admin@sandbox:trow]
overlays/cert-manager-nginx
├── ingress.yaml
├── kustomization.yaml
└── patch-ingress-host.yaml

on ne modifie que la conf relative a notre cluster : 


tree paas-sandbox                                                                         [☸ |kubernetes-admin@sandbox:trow]
paas-sandbox
├── kustomization.yaml
├── patch-ingress-host.yaml
├── patch-trow-arg.yaml
├── patch-validator-domain.yaml
├── pv-data-trow.yaml
└── README.md


- on crée notre volume ( pas de provisionning dynamique ) :

cat overlays/paas-sandbox/pv-data-trow.yaml               
 apiVersion: v1
 kind: PersistentVolume
 metadata:
   name: trow-data-hostpath-pv
   labels:
     app: trow
 spec:
   accessModes:
   - ReadWriteOnce
   capacity:
     storage: 20Gi
   hostPath:
     path: /data


on "patch" notre ressource ingress : 

on modifie l'issuer de certificat ssl dans la ressource de cert-manager c'est letsencrypt-prod de défini :on remplace par notre issuer "ca-issuer"
on modifie le hostname de notre ressource ingress en définissant le nom de notre point d'entrée dans la section standart et tls de notree ressource ingress 

 cat overlays/paas-sandbox/patch-ingress-host.yaml                                               
- op: replace
  path: /metadata/annotations
  value:
    certmanager.k8s.io/cluster-issuer: ca-issuer
- op: replace
  path: /spec/rules/0/host
  value: paasregistry.sandbox.ilius.io
- op: replace
  path: /spec/tls/0/hosts/0
  value: paasregistry.sandbox.ilius.io


on modifie le fichier patch-trow.yaml dans lequel on renseigne les valeurs de notre domaine et user : on override donc ce qu'il y a dans base 

 cat overlays/paas-sandbox/patch-trow-arg.yaml                                                 
- op: replace #domain name
  path: /spec/template/spec/containers/0/args/2
  value: paasregistry.sandbox.ilius.io
- op: replace #user name
  path: /spec/template/spec/containers/0/args/4
  value: trow-pass

et on va donc finalement définir notre fichier kustomized.yaml qui va contenir les informations nécéssaire :

cat overlays/paas-sandbox/kustomization.yaml                                                 
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: trow

bases:
  - ../cert-manager-nginx
generatorOptions:
    disableNameSuffixHash: true

secretGenerator:
  - name: trow-pass
    literals:
    - pass=s3cr3tp@55
  - name: trow-cred
    type: docker-registry
    literals:
    - docker-server=example.registry.com
    - docker-username=example
    - docker-password=s3cr3tp@55
resources: 
  - pv-data-trow.yaml    <<<<< on ajoute ici notre fichier de volume comme nouvelle ressource.
  - ../../base/validate.yaml

patchesJson6902:                      <<<< on renseigne ici les deux fichiers patch qui serviront a modifier les manifests
  - path: patch-ingress-host.yaml           
    target:
      kind: Ingress
      name: trow-ingress
      group: extensions
      version: v1beta1
  - path: patch-trow-arg.yaml
    target:
      kind: StatefulSet
      name: trow-set
      group: apps
      version: v1

  - path: patch-validator-domain.yaml
    target:
      kind: ValidatingWebhookConfiguration
      name: trow-validator
      group: admissionregistration.k8s.io
      version: v1



....
  type: LoadBalancer
  externalIPs:
  - 192.168.1.250
...

kctl get svc ingress-controller-ingress-nginx-controller -o yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-05-26T13:21:03Z"
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-controller
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/version: 0.32.0
    helm.sh/chart: ingress-nginx-2.3.0
  name: ingress-controller-ingress-nginx-controller
  namespace: ingress-controller
  resourceVersion: "257557"
  selfLink: /api/v1/namespaces/ingress-controller/services/ingress-controller-ingress-nginx-controller
  uid: 4f42464c-69e7-4912-8ccf-9455304a9340
spec:
  clusterIP: 10.93.244.43
  externalIPs:
  - 192.168.1.250
  externalTrafficPolicy: Cluster
  ports:
  - name: http
    nodePort: 30677
    port: 80
    protocol: TCP
    targetPort: http
  - name: https
    nodePort: 30849
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-controller
    app.kubernetes.io/name: ingress-nginx
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer: {}


 kctl get svc -n ingress-controller 
NAME                                                    TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE
ingress-controller-ingress-nginx-controller             LoadBalancer   10.93.244.43   192.168.1.250  80:30677/TCP,443:30849/TCP   19h
ingress-controller-ingress-nginx-controller-admission   ClusterIP      10.90.197.52   <none>           443/TCP                      19h

on a de plus de renseigner dans notre ressource ingress l'ip externe :
kctl get ing -o yaml                                       
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      certmanager.k8s.io/cluster-issuer: ca-issuer
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{"certmanager.k8s.io/cluster-issuer":"ca-issuer","kubernetes.io/ingress.class":"nginx","nginx.ingress.kubernetes.io/proxy-body-size":"0","nginx.ingress.kubernetes.io/proxy-read-timeout":"600","nginx.ingress.kubernetes.io/proxy-send-timeout":"600"},"name":"trow-ingress","namespace":"trow"},"spec":{"rules":[{"host":"paasregistry.sandbox.ilius.io","http":{"paths":[{"backend":{"serviceName":"trow-svc","servicePort":8000},"path":"/"}]}}],"tls":[{"hosts":["paasregistry.sandbox.ilius.io"],"secretName":"trow-registry-tls"}]}}
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/proxy-body-size: "0"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
    creationTimestamp: "2020-05-26T16:45:17Z"
    generation: 1
    name: trow-ingress
    namespace: trow
    resourceVersion: "257565"
    selfLink: /apis/extensions/v1beta1/namespaces/trow/ingresses/trow-ingress
    uid: 3fa0ca29-49ae-43fa-af65-5aece3de44aa
  spec:
    rules:
    - host: paasregistry.lapin.io
      http:
        paths:
        - backend:
            serviceName: trow-svc
            servicePort: 8000
          path: /
    tls:
    - hosts:
      - paasregistry.lapin.io
      secretName: trow-registry-tls
  status:
    loadBalancer:
      ingress:
      - ip: 192.168.1.250
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""



kctl get ing         
NAME           HOSTS                           ADDRESS          PORTS     AGE
trow-ingress   paasregistry.lapin.io   10.121.253.211   80, 443   19h


on renseigne dans notre fichier hosts la conf 
 192.168.1.250 paasregistry.lapin.io 

et on peut maintenant de puis notre pc atteindre nos ressources trow : 

curl paasregistry.lapin.io
<html>
<head><title>308 Permanent Redirect</title></head>
<body>
<center><h1>308 Permanent Redirect</h1></center>
<hr><center>nginx/1.17.10</center>
</body>
</html>
 boogie@boogieland  ~/Documents/work/repos_work/trow/install/overlays/paas-sandbox   master ●  curl -k  https://paasregistry.lapin.io
<!DOCTYPE html><html><body>
<h1>Welcome to Trow, the cluster registry</h1>
</body></html>%

à la fin de notre install on va devoir changer les droits du rep /data avec le user 999 en proprio comme la doc l'indique ( c'est pour le user trow de l'appli) pour permettre d'avoir les bon acces pour notre pod 
chown -R 999:999 /data


/ attention cette proc à permi d'installer l'appli ..mais c'est bancal : pas de connaissance de kustomized pour assurer la propreté du travail \
De plus tous les nodes (meme  le master )  du cluster vont porter une ip 192.168.1.250 defini pour ingress : ceci est du à kube-proxy qui associe la conf ingress a chaque nodes.


todo 

faire du node affinity/ antiaffinity et mettre un label sur les nodes pour faire du node selector.

faire un repo qui va inclure le repo trow en dependance 

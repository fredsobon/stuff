==  minikube notes ===


= minikube start options : = 

L’instruction start permet deux choses : le démarrage du cluster et la création de la machine Minikube.
Cette instruction accepte un certain nombre d’options comme par exemple :
 --memory Mo : quantité de mémoire à donner à la machine (par défaut 2048, soit 2 Go).
 --kubernetes-version VERSION : version de Kubernetes à déployer (exemple : v1.14.2).
 --cpus Nbre : nombre de CPU à exposer à la machine (par défaut 2).
 --vm-driver Nom : nom du driver à utiliser (exemple : kvm2, vmware, none).

Ces options ne sont pas obligatoires sauf lors du déploiement sous KVM/libvirt. Dans ce cas, il est indispensable
d’ajouter l’option --vm-driver kvm2 .

boogie$ KVM_URL=https://github.com/kubernetes/minikube/releases                                                                                             [☸ N/A:N/A]
boogie$ wget $KVM_URL/download/v1.2.0/docker-machine-driver-kvm2                                                                                            [☸ N/A:N/A]
2019-11-25 22:10:48 (8,24 MB/s) — « docker-machine-driver-kvm2 » sauvegardé [37645040/37645040]
boogie$ sudo mv docker-machine-driver-kvm2 /usr/local/bin/                                                                                                  [☸ N/A:N/A]
boogie$ sudo chmod +x /usr/local/bin/docker-machine-driver-kvm2                                                                                             [☸ N/A:N/A]
boogie$ sudo usermod -a -G kvm,libvirt boogie   

-> demarrage minikube driver libvirt avec version specifique de kube : 

minikube start --kubernetes-version v1.16.3 --vm-driver kvm2

-> on peut définir notre driver par default :

minikube config set vm-driver kvm2


== minikube dashnboard : ==

on peut activer le dashboard avec minikube addons 
minikube addons enable dashboard

une fois activer pour acceder au dashboard il suffit de lancer :
minikube dashboard :

la commande lance un navigateur pointant sur l'url de type :  (une proxyfication est faite en background ) : 
boogie$ minikube dashboard                                                                                                                         [☸ minikube:default]
Verifying dashboard health ...
Launching proxy ...
Verifying proxy health ...
http://127.0.0.1:46097/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser..

== minikube - docker context : ==

il est possible de récupérer le contexte du démon Docker afin de pouvoir communiquer avec lui. Pour cela, lancez la commande
minikube suivie de l’instruction docker-env .
Ci­dessous la commande correspondante :

$ minikube docker-env
Cette commande renvoie un ensemble d’instructions sous la forme suivante :
export DOCKER_TLS_VERIFY="1"
export DOCKER_HOST="tcp://192.168.39.4:2376"
export DOCKER_CERT_PATH="/home/yannig/.minikube/certs"
export DOCKER_API_VERSION="1.35"
# Run this command to configure your shell:
# eval $(minikube docker-env)

Cette sortie ne sert pas directement à passer sur le contexte Docker de la machine Minikube.
Pour cela, il est nécessaire d’ajouter l’évaluation du résultat de cette commande de la manière suivante :
$ eval $(minikube docker-env)


== minikube cache : ===

on va pouvoir stocker des images dans le cache de minikube de manière à pouvoir travailler off line : 

minikube cache add ubuntu:16.04
minikube cache add mailhog/mailhog:latest

minikube cache list                                                                     [☸ minikube:default]
mailhog/mailhog:latest
ubuntu:16.04


== minikube build image : ==

il va être très utile de builder notre image dans la vm ( pas besoin de build sur notre host et pousser sur une registry docker par exemple) 

> avec docker on peut se connecter en ssh sur minikube et lancer un docker build
ou alors on peut réutiliser le daemon docker pour que notre client docker communique avec
: on peut donc builder avec le même daemon docker :

eval $(minikube docker-env)

Attention il faut désactiver le setting : 
to turn off the imagePullPolicy:Always -> sinon kube n'utilisera jamais les images locales. 



# Set docker env
eval $(minikube docker-env)

cat Dockerfile
FROM fredsobon/kuard

docker build -t boogie/kuard:0.0.1 .

# Build image
docker build -t foo:0.0.1 .   <<<<<<< /!\ attention tagger l'image est fondamentale sinon minikube ne retrouve pas  l'image


# Run in minikube   : on va indiquer a minikube de ne pas recupérer d'image distante :
kubectl run kuard1 --image=boogie/kuard:0.0.1 --image-pull-policy=Never
deployment "kuard1" created

# Check that it's running
kubectl get pods

NAME                             READY     STATUS    RESTARTS   AGE
hello-minikube-6c47c66d8-rczth   1/1       Running   9          227d
kuard1-68fb5b4df4-c426v          1/1       Running   0          6m
nodejs-demo                      1/1       Running   2          226d


# Tests :

on peut obtenir la description de notre pod :

kubectl describe pods kuard1-68fb5b4df4-c426v

toute une serie d'info apparaissent ..

on va pouvoir rediriger un port de notre hostt vers le port natif de notre pod :

kubectl port-forward kuard1-68fb5b4df4-c426v 8080:8080

en ouvrant un navigateur sur http://localhost:8080 on accede bien à la webapp de notre pod
kubectl port-forward kuard1-68fb5b4df4-c426v 8080:8080




== minikube montage host : ==

il est possible de monter un repertoire dans minikube :
ex: 


minikube mount <source directory>:<target directory>
minikube mount $HOME:/host

minikube mount "$(pwd):/minikube-host"

le rep courant sera donc dispo dans l'arbo minikube 


on peut réferencer ce point de montage dans un manifest kube :

{
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": {
    "name": "ubuntu"
  },
  "spec": {
        "containers": [
          {
            "name": "ubuntu",
            "image": "ubuntu:18.04",
            "args": [
              "bash"
            ],
            "stdin": true,
            "stdinOnce": true,
            "tty": true,
            "workingDir": "/host",
            "volumeMounts": [{
              "mountPath": "/host",
              "name": "host-mount"
            }]
          }
        ],
    "volumes": [
      {
        "name": "host-mount",
        "hostPath": {
          "path": "/host"
        }
      }
    ]
  }
}

== minikube filesync : ==

Place files to be synced in $MINIKUBE_HOME/files

For example, running the following will result in the deployment of a custom /etc/resolv.conf:

mkdir -p ~/.minikube/files/etc
echo nameserver 8.8.8.8 > ~/.minikube/files/etc/resolv.conf
minikube start





== utilisation images locale ==

eval $(minikube docker-env)

# Start minikube
minikube start

# Set docker env
eval $(minikube docker-env)

cat Dockerfile 
FROM fredsobon/kuard

docker build -t boogie/kuard:0.0.1 .

# Build image
docker build -t foo:0.0.1 .   <<<<<<< /!\ attention tagger l'image est fondamentale sinon minikube ne retrouve pas  l'image 


# Run in minikube   : on va indiquer a minikube de ne pas recupérer d'image distante : 
kubectl run kuard1 --image=boogie/kuard:0.0.1 --image-pull-policy=Never
deployment "kuard1" created

# Check that it's running
kubectl get pods

NAME                             READY     STATUS    RESTARTS   AGE
hello-minikube-6c47c66d8-rczth   1/1       Running   9          227d
kuard1-68fb5b4df4-c426v          1/1       Running   0          6m
nodejs-demo                      1/1       Running   2          226d


# Tests :

on peut obtenir la description de notre pod :

kubectl describe pods kuard1-68fb5b4df4-c426v 

toute une serie d'info apparaissent ..

on va pouvoir rediriger un port de notre hostt vers le port natif de notre pod :

kubectl port-forward kuard1-68fb5b4df4-c426v 8080:8080

en ouvrant un navigateur sur http://localhost:8080 on accede bien à la webapp de notre pod  
kubectl port-forward kuard1-68fb5b4df4-c426v 8080:8080



= images :

attention pour voir les images utilisées par minikube il faut charger l'env de son docker embarqué :

eval $(minikube docker-env)

docker images                                                         [☸ minikube:default]
REPOSITORY                                TAG                 IMAGE ID            CREATED             SIZE
gcr.io/kubernetes-helm/tiller             v2.14.3             2d0a693df3ba        4 days ago          94.2MB
jenkins/jenkins                           lts                 b137a5753eb1        2 weeks ago         567MB
k8s.gcr.io/kube-proxy                     v1.15.0             d235b23c3570        6 weeks ago         82.4MB
k8s.gcr.io/kube-apiserver                 v1.15.0             201c7a840312        6 weeks ago         207MB
k8s.gcr.io/kube-scheduler                 v1.15.0             2d3813851e87        6 weeks ago         81.1MB
k8s.gcr.io/kube-controller-manager        v1.15.0             8328bb49b652        6 weeks ago         159MB
k8s.gcr.io/kube-addon-manager             v9.0                119701e77cbc        6 months ago        83.1MB
k8s.gcr.io/coredns                        1.3.1               eb516548c180        6 months ago        40.3MB
k8s.gcr.io/kubernetes-dashboard-amd64     v1.10.1             f9aed6605b81        7 months ago        122MB
k8s.gcr.io/etcd                           3.3.10              2c4adeb21b4f        8 months ago        258MB
k8s.gcr.io/k8s-dns-sidecar-amd64          1.14.13             4b2e93f0133d        10 months ago       42.9MB
k8s.gcr.io/k8s-dns-kube-dns-amd64         1.14.13             55a3c5209c5e        10 months ago       51.2MB
k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64    1.14.13             6dc8ef8287d3        10 months ago       41.4MB
k8s.gcr.io/pause                          3.1                 da86e6ba6ca1        19 months ago       742kB
gcr.io/k8s-minikube/storage-provisioner   v1.8.1              4689081edb10        21 months ago       80.8MB

si on ne le fait pas ..on ne voit pas du tout le même pool d'images :

docker images
REPOSITORY                                            TAG                 IMAGE ID            CREATED             SIZE
artifact-docker-infra.lapin.lapin.net/debian-utils   1                   b03b2d9c736a        2 weeks ago         340MB
fredsobon/debian-utils                                1                   b03b2d9c736a        2 weeks ago         340MB
debian                                                testing             85b9d0cfd7ac        3 weeks ago         114MB
nginx-local                                           1                   ea1193fd3dde        4 weeks ago         20.6MB
nginx-local                                           latest              ea1193fd3dde        4 weeks ago         20.6MB
nginx                                                 alpine              ea1193fd3dde        4 weeks ago         20.6MB
kubetool                                              latest              958ff145c359        6 weeks ago         255MB
k8s.gcr.io/coredns                                    1.5.0               7987f0908caf        3 months ago        42.5MB
fredsobon/kuard                                       latest              72b285fe4d98        4 months ago        19.6MB



===== minikube addons : ===

on va pouvoir utiliser des plugins activables / desactivables pour le travail sur minikube.

une liste est disponible :

minikube addons list              
- addon-manager: enabled
- dashboard: enabled
- default-storageclass: enabled
- efk: disabled
- freshpod: disabled
- gvisor: disabled
- heapster: disabled
- ingress: enabled
- logviewer: disabled
- metrics-server: disabled
- nvidia-driver-installer: disabled
- nvidia-gpu-device-plugin: disabled
- registry: disabled
- registry-creds: disabled
- storage-provisioner: enabled
- storage-provisioner-gluster: disabled





- ingress :

exemple on va pouvoir activer ingress pour permettre d'acceder à nos services depuis l'exterrieur de notre cluster :

minikube addons enable ingress

on va verifier que l'ingress est bien monté avec :
kubectl get pods -n kube-system

kubectl get pods -n kube-system |grep ingress
nginx-ingress-controller-7b465d9cf8-h5vrx   0/1     ImagePullBackOff   0          6m42s

-> on peut deployer une appli :

kubectl run web --image=gcr.io/google-samples/hello-app:1.0 --port=8080
deployment.apps/web created

-> on va exposer le service de l'appli 

kubectl expose deployment web --target-port=8080 --type=NodePort
service/web exposed

-> on verifie que le service est crée et dispo en tant que node port 
kubectl get service web
Output:

NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
web       NodePort   10.104.133.249   <none>        8080:31637/TCP   12m
Visit the service via NodePort:

->  on va pouvoir récupérer l'url depuis laquelle le service sera  accessble 
minikube service web --url
http://172.17.0.15:31637


la premiere partie de notre déploiement est ok, mais on va maintenant créer une ressource ingress pour permettre un access "naturel" via url ...


-> on crée un fichier example-ingress.yaml 

apiVersion: networking.k8s.io/v1beta1 # for versions before 1.14 use extensions/v1beta1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
 rules:
 - host: hello-world.info
   http:
     paths:
     - path: /*
       backend:
         serviceName: web
         servicePort: 8080

-> on créer la ressource ingress avec
kubectl apply -f example-ingress.yaml
ingress.networking.k8s.io/example-ingress created

-> on verifie que le service est crée :

kubectl get ingress
NAME              HOSTS              ADDRESS       PORTS     AGE
example-ingress   hello-world.info   172.17.0.15   80        38s


-> on peut ajouter notre record dans le fichier hosts :
172.17.0.15 hello-world.info


cela envoi nos requette de l'ingress au minikube 

on peut tester naturellement notre résultat : 

curl hello-world.info

Hello, world!
Version: 1.0.0
Hostname: web-55b8c6998d-8k564



- minikube set up de cni :

on va pouvoir configurer minikube pour tester la mise en place de cni (container network interface )  :
minikube start --enable-default-cni --network-plugin=cni

on pourra ensuite installer calico par exemple :

Installation de Calico
Une fois le mécanisme CNI activé, il est nécessaire d’installer un pilote permettant de prendre en charge les polices réseau.
Pour installer le pilote Calico, lancez la commande suivante :
$ kubectl apply -f \
https://docs.projectcalico.org/v3.0/getting-started/kubernetes/
installation/hosted/kubeadm/1.7/calico.yaml

Cette commande procédera au déploiement de plusieurs éléments :
 un objet DaemonSet etcd pour Calico,
 un objet DaemonSet pour gérer chaque nœud,
 un déploiement pour le contrôle de Calico.
Scrutez l’état des pods associés à Calico dans l’espace de noms kube-system et attendez qu’ils  soient tous démarrés.

curl -O -L https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml






= libvirt / kvm : =

sudo apt install libvirt-clients libvirt-daemon-system qemu-kvm
sudo usermod -a -G libvirt $(whoami)
newgrp libvirt
minikube start --vm-driver kvm2

=== minikube docker registry : ===

Using the Docker registry
How to access the Docker registry within minikube
As an alternative to reusing the Docker daemon, you may enable the registry addon to push images directly into registry.

Steps are as follows:

For illustration purpose, we will assume that minikube VM has one of the ip from 192.168.39.0/24 subnet. If you have not overridden these subnets as per networking guide, you can find out default subnet being used by minikube for a specific OS and driver combination here which is subject to change. Replace 192.168.39.0/24 with appropriate values for your environment wherever applicable.

Ensure that docker is configured to use 192.168.39.0/24 as insecure registry. Refer here for instructions.

Ensure that 192.168.39.0/24 is enabled as insecure registry in minikube. Refer here for instructions..

Enable minikube registry addon:

minikube addons enable registry
Build docker image and tag it appropriately:

docker build --tag $(minikube ip):5000/test-img .
Push docker image to minikube registry:

docker push $(minikube ip):5000/test-img
Now run it in minikube:

kubectl run test-img --image=$(minikube ip):5000/test-img
Or if 192.168.39.0/24 is not enabled as insecure registry in minikube, then:

kubectl run test-img --image=localhost:5000/test-img


=== minikube debug : ===

minikube start --v=7 will start minikube and output all the important debug logs to stdout

v=0 
..
v=7 

minikube logs

=== minikube psp : pod security policies : ===
Using Minikube with Pod Security Policies
Overview
This tutorial explains how to start minikube with Pod Security Policies (PSP) enabled.

Prerequisites
Minikube 1.5.2 with Kubernetes 1.16.x or higher
Tutorial
Before starting minikube, you need to give it the PSP YAMLs in order to allow minikube to bootstrap.

Create the directory:
mkdir -p ~/.minikube/files/etc/kubernetes/addons

Copy the YAML below into this file: ~/.minikube/files/etc/kubernetes/addons/psp.yaml

Now start minikube:
minikube start --extra-config=apiserver.enable-admission-plugins=PodSecurityPolicy

---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: "*"
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
spec:
  privileged: true
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - "*"
  volumes:
  - "*"
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  hostIPC: true
  hostPID: true
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: psp:privileged
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - privileged
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: psp:restricted
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
rules:
- apiGroups: ['policy']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - restricted
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: default:restricted
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp:restricted
subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: default:privileged
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp:privileged
subjects:
- kind: Group
  name: system:masters
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: system:nodes
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: system:serviceaccounts:kube-system


== minikube ingress : ==

Ingress nginx for TCP and UDP services
How to set up a minikube ingress for TCP and UDP services
Overview
The minikube ingress addon enables developers to route traffic from their host (Laptop, Desktop, etc) to a Kubernetes service running inside their minikube cluster. The ingress addon uses the ingress nginx controller which by default is only configured to listen on ports 80 and 443. TCP and UDP services listening on other ports can be enabled.

Prerequisites
Latest minikube binary and ISO
Telnet command line tool
Kubectl command line tool
A text editor
Configuring TCP and UDP services with the nginx ingress controller
Enable the ingress addon
Enable the minikube ingress addon with the following command:

minikube addons enable ingress
Update the TCP and/or UDP services configmaps
Borrowing from the tutorial on configuring TCP and UDP services with the ingress nginx controller we will need to edit the configmap which is installed by default when enabling the minikube ingress addon.

There are 2 configmaps, 1 for TCP services and 1 for UDP services. By default they look like this:

apiVersion: v1
kind: ConfigMap
metadata:
  name: tcp-services
  namespace: ingress-nginx
apiVersion: v1
kind: ConfigMap
metadata:
  name: udp-services
  namespace: ingress-nginx
Since these configmaps are centralized and may contain configurations, it is best if we only patch them rather than completely overwrite them.

Let’s use this redis deployment as an example:

redis-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-deployment
  namespace: default
  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - image: redis
        imagePullPolicy: Always
        name: redis
        ports:
        - containerPort: 6379
          protocol: TCP
Create a file redis-deployment.yaml and paste the contents above. Then install the redis deployment with the following command:

kubectl apply -f redis-deployment.yaml
Next we need to create a service that can route traffic to our pods:

redis-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: default
spec:
  selector:
    app: redis
  type: ClusterIP
  ports:
    - name: tcp-port
      port: 6379
      targetPort: 6379
      protocol: TCP
Create a file redis-service.yaml and paste the contents above. Then install the redis service with the following command:

kubectl apply -f redis-service.yaml
To add a TCP service to the nginx ingress controller you can run the following command:

kubectl patch configmap tcp-services -n kube-system --patch '{"data":{"6379":"default/redis-service:6379"}}'
Where:

6379 : the port your service should listen to from outside the minikube virtual machine
default : the namespace that your service is installed in
redis-service : the name of the service
We can verify that our resource was patched with the following command:

kubectl get configmap tcp-services -n kube-system -o yaml
We should see something like this:

apiVersion: v1
data:
  "6379": default/redis-service:6379
kind: ConfigMap
metadata:
  creationTimestamp: "2019-10-01T16:19:57Z"
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
  name: tcp-services
  namespace: kube-system
  resourceVersion: "2857"
  selfLink: /api/v1/namespaces/kube-system/configmaps/tcp-services
  uid: 4f7fac22-e467-11e9-b543-080027057910
The only value you need to validate is that there is a value under the data property that looks like this:

  "6379": default/redis-service:6379
Patch the ingress-nginx-controller
There is one final step that must be done in order to obtain connectivity from the outside cluster. We need to patch our nginx controller so that it is listening on port 6379 and can route traffic to your service. To do this we need to create a patch file.

nginx-ingress-controller-patch.yaml

spec:
  template:
    spec:
      containers:
      - name: nginx-ingress-controller
        ports:
         - containerPort: 6379
           hostPort: 6379
Create a file called nginx-ingress-controller-patch.yaml and paste the contents above.

Next apply the changes with the following command:

kubectl patch deployment nginx-ingress-controller --patch "$(cat nginx-ingress-controller-patch.yaml)" -n kube-system
Test your connection
Test that you can reach your service with telnet via the following command:

telnet $(minikube ip) 6379
You should see the following output:

Trying 192.168.99.179...
Connected to 192.168.99.179.
Escape character is '^]'
To exit telnet enter the Ctrl key and ] at the same time. Then type quit and press enter.

If you were not able to connect please review your steps above.

Review
In the above example we did the following:

Created a redis deployment and service in the default namespace
Patched the tcp-services configmap in the kube-system namespace
Patched the nginx-ingress-controller deployment in the kube-system namespace
Connected to our service from the host via port 6379
You can apply the same steps that were applied to tcp-services to the udp-services configmap as well if you have a service that uses UDP and/or TCP

Caveats
With the exception of ports 80 and 443, each minikube instance can only be configured for exactly 1 service to be listening on any particular port. Multiple TCP and/or UDP services listening on the same port in the same minikube instance is not supported and can not be supported until an update of the ingress spec is released. Please see this document for the latest info on these potential changes


== minikube oidc : ==

OpenID Connect Authentication
Configuring minikube to use OpenID Connect Authentication
The kube-apiserver in minikube can be configured to support OpenID Connect Authentication.

Read more about OpenID Connect Authentication for Kubernetes here: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens

Configuring the API Server
Configuration values can be passed to the API server using the --extra-config flag on the minikube start command. See configuring_kubernetes.md for more details.

The following example configures your Minikube cluster to support RBAC and OIDC:

minikube start \
  --extra-config=apiserver.authorization-mode=RBAC \
  --extra-config=apiserver.oidc-issuer-url=https://example.com \
  --extra-config=apiserver.oidc-username-claim=email \
  --extra-config=apiserver.oidc-client-id=kubernetes-local
Configuring kubectl
You can use the kubectl oidc authenticator to create a kubeconfig as shown in the Kubernetes docs: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#option-1-oidc-authenticator

minikube start already creates a kubeconfig that includes a cluster, in order to use it with your oidc authenticator kubeconfig, you can run:

kubectl config set-context kubernetes-local-oidc --cluster=minikube --user username@example.com
Context "kubernetes-local-oidc" created.
kubectl config use-context kubernetes-local-oidc
For the new context to work you will need to create, at the very minimum, a Role and a RoleBinding in your cluster to grant permissions to the subjects included in your oidc-username-claim.





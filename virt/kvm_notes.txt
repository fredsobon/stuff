==== notes kvm / qemu ===

= pre-requi : virtualisation on sur pc / server =

-> check des flag cpu (ex : vmx pour intel ) : 
grep --color -Ew 'svm|vmx|lm' /proc/cpuinfo


->check module kvm loadé :

boogie@boogie-kube:~$ sudo lsmod |grep kvm
kvm_intel             192512  0
kvm                   589824  1 kvm_intel
irqbypass              16384  1 kvm


= set up 

-> debian minimal install (sans gui) :
apt install qemu-kvm libvirt-clients libvirt-daemon-system

-> set up user dans les groupes libvirt et libvirt-qemu :

boogie@boogie-kube:~$ sudo adduser boogie libvirt
Ajout de l'utilisateur « boogie » au groupe « libvirt »...
Adding user boogie to group libvirt
Fait.
boogie@boogie-kube:~$ sudo adduser boogie libvirt-qemu
Ajout de l'utilisateur « boogie » au groupe « libvirt-qemu »...
Adding user boogie to group libvirt-qemu
Fait.

-> test fonctionnement :

on va lister les "domaines" en root :

boogie@boogie-kube:~$ sudo -i
root@boogie-kube:~# virsh list --all
 Id    Name                           State
 ----------------------------------------------------

on va faire la même chose mais en user standart (on se connect a qemu pour recup les infos ) :

boogie@boogie-kube:~$ virsh --connect qemu:///system list --all
 Id    Name                           State
 ----------------------------------------------------

-> validation des composants avec outils integrés : 

root@boogie-kube:~# virt-host-validate 
  QEMU: Checking for hardware virtualization                                 : PASS
    QEMU: Checking if device /dev/kvm exists                                   : PASS
      QEMU: Checking if device /dev/kvm is accessible                            : PASS
        QEMU: Checking if device /dev/vhost-net exists                             : PASS


root@boogie-kube:~# virsh nodeinfo
CPU model:           x86_64
CPU(s):              4
CPU frequency:       1599 MHz
CPU socket(s):       1
Core(s) per socket:  2
Thread(s) per core:  2
NUMA cell(s):        1
Memory size:         8055880 KiB

exam des params possibles : 
root@boogie-kube:~# virsh domcapabilities |grep -i max
  <vcpu max='255'/>

--> ici on pourra definir maximum 255 vcpu sur notre hyperviseur pour les vms 

root@boogie-kube:~# virsh domcapabilities |grep -A5  diskdevice  -i
      <enum name='diskDevice'>
        <value>disk</value>
        <value>cdrom</value>
        <value>floppy</value>
        <value>lun</value>
      </enum>
-> ici on voit la liste des device que l'on pourra utiliser




-> sur poste client ayant un serveur x :
sudo apt install virt-manager
sudo apt install ssh-askpass-gnome 

creation de fichier permettant l'utilisation au user du groupe boogie de virt-manager : 
/etc/polkit-1/rules.d/70-libvirtd.rules
polkit.addRule(function(action, subject) {
if (action.id == "org.libvirt.unix.manage" && subject.local &&
subject.active && subject.isInGroup("boogie")) {
return polkit.Result.YES;
}
});

on peut voir sur l'hyperviseur la conf reseau utilisée ( de base c'est celle par defaut : qui est en nat nos vms peuvent sortir mais aucunes connections sur les vms de l'exterrieur )
Une interface bridge virbr0 192.168.122.1 est "natée" à l'interface physique. Le démon dnsmasq fournit le service DNS/DHCP.


Les conf reseaux sont stockées dans les arbos : 

/etc/libvirt/qemu/networks
ex :
/etc/libvirt/qemu/networks/default.xml .



root@boogie-kube:~# virsh net-list --all
 Name                 State      Autostart     Persistent
----------------------------------------------------------
 default              inactive   no            yes



- virt-manager  

par defaut les consoles des vms créees sont configurées pour être acceder depuis la machinne direct : avec "spice" de setté.
pour permettre le pilotage des vms sur un serveur sans x et donc depuis un client avec x , il faut sur notre poste client definir le mode d'affichage des consoles avec "vnc" 


== creation de vm :
en se connectant via virt-manager à notre  hyperviseur, on va pouvoir configurer les vms de plusieurs manieres : iso, pxe etc ..
pour une iso : il suffira de stocker notre image sur le serveur et pointer vers le fichier iso lors du sett up ( par default les images sont stockées dans : 
/var/lib/libvirt/images :

ex :

root@boogie-kube:/var/lib/libvirt/images# ls -la
total 6385060
drwx--x--x 2 root         root                4096 sept. 30 08:59 .
drwxr-xr-x 7 root         root                4096 sept. 29 20:16 ..
-rw-r--r-- 1 libvirt-qemu libvirt-qemu   305135616 juil. 14 13:12 debian-9.5.0-amd64-netinst.iso

une fois crée les "fichiers" blocs des vms sont aussi dans ce repertoire.

On va pouvoir si besoin cloner via virt-manager une vm (pour un back, dupliquer un modele ...) : dans ce cas  la mac adresse de la carte réseau est automatiquement modifiée . Il faudra cependant modifier la conf réseau sur les vms , ainsi que modifier les confs additionnelles sur les vms pour s'assurer de l'unicité  de chaque vms 

root@boogie-kube:/var/lib/libvirt/images# ls -lh
total 6,1G
-rw-r--r-- 1 libvirt-qemu libvirt-qemu 291M juil. 14 13:12 debian-9.5.0-amd64-netinst.iso
-rw------- 1 root         root          41G sept. 30 08:56 kube1.qcow2
-rw------- 1 root         root         1,4G sept. 30 08:57 kube2.qcow2
-rw------- 1 root         root         1,4G sept. 30 08:58 kube3.qcow2
-rw------- 1 root         root         1,4G sept. 30 08:59 kube4.qcow2



== creation de networks :

differents types de network sont dispos : nat (reseau natif  par default de kvm),  isolated (pas de comm entre le hosts et les vms : elles ne sont joignables qu'entre elles), bridge : mode utilisé en production à une ecrasante majorité : vm, hosts et machines dans la périphérie sont sur le même réseau.


- creation de reseau bridge :

paquet bridge-utils  : utilitaires destinés à configurer un pont Ethernet sous Linux

->creation d'un bridge nommé br0 : 
brctl addbr br0

-> Ajout d'une interface physique au bridge :

brctl addif br0 eno1

-> verif 

root :~# brctl show
bridge name bridge id   STP enabled interfaces
br0   8000.180373d3c695 no    eno1

cat /etc/network/interfaces
# This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

source /etc/network/interfaces.d/*

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
#allow-hotplug eno1
auto eno1
#iface eno1 inet manuel
# The bridge network interface
#allow-hotplug eno1
auto br0
iface br0   inet static
  address   192.168.0.7
  netmask   255.255.255.0
  gateway   192.168.0.254
  network   192.168.0.0
  broadcast 192.168.0.255
  dns-nameservers 192.168.0.254 8.8.8.8
  bridge_ports eno1
  bridge_stp off
  bridge_fd 0
  bridge_maxwait 0
# This is an autoconfigured IPv6 interface
#iface eno1 inet6 auto



- listing des réseaux :

virsh net-list --all


- examen de la conf des reseaux :

virsh net-dumpxml nom_de_reseau


- creation de reseau :

virsh net-create nom_reseau : creation de reseau NON permanente
-> il faudra donc systematiquement le recreer 

virsh net-define nom_reseau : creation du reseau permanent 


une fois crée avec les commandes dédiées on peut examiner les differentes configurations;
ex :

# cat /etc/libvirt/qemu/networks/isolated.xml
<!--
WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE
OVERWRITTEN AND LOST. Changes to this xml configuration should be made
using:
virsh net-edit isolated
or other application using the libvirt API.
<network>
  <name>isolated</name>
  <uuid>84147b7d-a95f-4bc2-a4d9-80baab391a18</uuid>
  <bridge name='virbr1' stp='on' delay='0'/>
  <mac address ='52:54:00:0e:c2:b5'/>
</network>

pour permettre au lan de se monter au boot :

virsh net-autostart isolated.


- ajout d'interface reseau au lan créée : on va essayer de selectionner en type de carte réseau "virtio" qui fournit les meilleurs perfs

- listing des interfaces de note vm :

virsh domiflist node_name

- Ajout d"une interface resseau a une vm sur un reseau cree au prealable : 

# virsh attach-interface --domain node_name  --source isolated --type network --model virtio --config --live
Interface attached successfully
# virsh domiflist node_name
Interface Type Source
Model
MAC
------------------------------------------------------------------
vnet2 network default virtio 52:54:00:b0:50:98
vnet3 network isolated
virtio 52:54:00:2b:0d:0c

on a donc ici attacher une interface virtuelle reseau de type virtio sur la vm node_name : elle utilise le reseau "isolated" . L'option config rend cette conf persistente au reboot de la machinne. L'option live elle permet de monter en live l'interface. 


on va pouvoir examiner la conf de notre reseau avec la conf net-dumpxml

on va pouvoir utiliser les commandes dédiées au bridge (paquet bridge-utils ) :

ex :
# brctl show virbr1
bridge name   bridge id  STP enabled interfaces
virbr1        8000.5254000ec2b5 yes  virbr1-nic
              vnet1
              vnet3

- modification de reseau :

pour editer une conf de reseau , il faut tout d'abord l'arreter : 
# virsh net-destroy notre_reseau
Network notre_reseau destroyed

ensuite on peut le modifier : 
# virsh net edit notre_reseau




== commandes diverses : ==

https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/virtualization/chap-virtualization-managing_guests_with_virsh


- demarrer une vm :
il faut tout d'abord s'assurer que le reseau est lancé :

virsh net-start default
virsh start node1
Domain node1 started

- pour se connecter a la vm :

1/ recuperer sa mac :

ps fauxww 

tap,fd=25,id=hostnet0,vhost=on,vhostfd=27 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:24:29:1e

2/ on recupere l'ip associée à la mac :

arp -n
Adresse                  TypeMap AdresseMat          Indicateurs           Iface
192.168.122.2            ether   52:54:00:24:29:1e   C                     virbr0

3/ connection en ssh 

root@boogie-kube:~# ssh 192.168.122.2 -l boogie
boogie@192.168.122.2's password: 
boogie@kube1:~$ 


- demarrage auto du reseau :
virsh net-autostart default


- demarrage auto de vm 
virsh autostart node_name

#virsh autostart node1
Domain node1 marked as autostarted


- eteindre une vm via acpi :

virsh shutdown node1

- eteindre une vm via alimentation :

virsh destroy node1

- Recupérer les infos d'une vm demarree :
 virsh dominfo node1

 Id:             2
 Name:           node1
 UUID:           bbf8d5d4-f2e6-4875-b649-a1b62c6deba9
 OS Type:        hvm
 State:          running
 CPU(s):         1
 CPU time:       10,1s
 Max memory:     2097152 KiB
 Used memory:    2097152 KiB
 Persistent:     yes
 Autostart:      disable
 Managed save:   no
 Security model: none
 Security DOI:   0


- Edition / modification de la conf d'une vm :

 virsh edit node1


- connection a l'hyperviseur : 
virsh connect hyperviseur

- recuperation de la conf d'un node : 
virsh dumpxml node_name 

- creation d'une vm via un fichier de conf :
virsh create configuration_file.xml


- exam :

virt-admin client-info node
virt-admin server-clients-info admin
virsh dominfo node



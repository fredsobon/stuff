=== commandes de base /troubleshooting pour stack elastic : ===

== commandes utiles / usuelles ( à passer directement dans la console dev tool de kibana ou en curl depuis un membre du cluster : 

Toutes les données qui commencent par "_" sont gérées par elastic. ce sont des "meta field" ajoutés et managés par elastic.

- test de fonectionnement de base : 
GET _search
{
  "query": {
    "match_all": {}
  }
}

- topologie du cluster : 
GET /_cluster/settings

- examiner les noeux du cluster : 
GET /_cat/nodes


- examiner la sante d'un node du cluster : 

GET /_nodes/<node_id>
GET /_nodes/lapin01

- santé du cluster : 

GET /_cluster/health?
GET /_cat/health?pretty
GET /_cluster/health?level=indices    <<< ici on regarde spécifiquement les index 


GET _cluster/state
GET _cluster/state/version

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "version" : 150,
  "state_uuid" : "ksJb1lOCRnKZ0sWlEGOvSw"
}


GET _cluster/state/master_node

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "master_node" : "8lCeAK5DQ1Wl4vgTTw8uRA"
}

GET _cluster/state/nodes

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "nodes" : {
    "zVl54Y-9RTyY6NLDaBiLyA" : {
      "name" : "elasticsearch-master-1",
      "ephemeral_id" : "VPCVrXvxRVmwpybe80bERg",
      "transport_address" : "172.17.0.11:9300",
      "attributes" : {
        "ml.machine_memory" : "512000000",
        "ml.max_open_jobs" : "20",
        "xpack.installed" : "true",
        "transform.node" : "true"
      }
    },

....


GET _cluster/state/routing_table

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "routing_table" : {
    "indices" : {
      ".apm-agent-configuration" : {
        "shards" : {
          "0" : [
            {
              "state" : "STARTED",
              "primary" : true,
              "node" : "zVl54Y-9RTyY6NLDaBiLyA",
              "relocating_node" : null,
              "shard" : 0,
              "index" : ".apm-agent-configuration",
              "allocation_id" : {
                "id" : "LPj2EYetS9qRrqvPLwNPvg"
              }
            },
            {
              "state" : "STARTED",
              "primary" : false,
              "node" : "8lCeAK5DQ1Wl4vgTTw8uRA",
              "relocating_node" : null,
              "shard" : 0,
              "index" : ".apm-agent-configuration",
              "allocation_id" : {
                "id" : "tfrlc1fOQ6q3C9Xn2a_XuQ"
              }
            }
          ]
        }
      },
      "kibana_sample_data_flights" : {
        "shards" : {
          "0" : [
            {
              "state" : "STARTED",
              "primary" : true,
              "node" : "FnbdoPhIS8qFi1GoO-vKTg",
              "relocating_node" : null,
              "shard" : 0,
              "index" : "kibana_sample_data_flights",
              "allocation_id" : {
                "id" : "vrMAGtylSRG0-qu5GWG8-A"
              }

....

GET _cluster/state/metadata


{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "metadata" : {
    "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
    "cluster_uuid_committed" : true,
    "cluster_coordination" : {
      "term" : 4,
      "last_committed_config" : [
        "zVl54Y-9RTyY6NLDaBiLyA",
        "8lCeAK5DQ1Wl4vgTTw8uRA",
        "FnbdoPhIS8qFi1GoO-vKTg"
      ],
      "last_accepted_config" : [
        "zVl54Y-9RTyY6NLDaBiLyA",
        "8lCeAK5DQ1Wl4vgTTw8uRA",
        "FnbdoPhIS8qFi1GoO-vKTg"
      ],
      "voting_config_exclusions" : [ ]
    },
    "templates" : {
      ".management-beats" : {
        "order" : 0,
        "version" : 70000,
        "index_patterns" : [
          ".management-beats"
        ],
        "settings" : {
          "index" : {
            "number_of_shards" : "1",
            "auto_expand_replicas" : "0-1",
            "codec" : "best_compression"
          }
        },
        "mappings" : {
          "_doc" : {
            "dynamic" : "strict",
            "properties" : {
              "beat" : {
                "properties" : {
                  "host_ip" : {
                    "type" : "ip"
                  },
                  "metadata" : {
                    "dynamic" : "true",
                    "type" : "object"
                  },
                  "active" :
...
.....

GET _cluster/state/blocks

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "blocks" : { }
}

- stats de cluster :

GET _cluster/stats


{
  "_nodes" : {
    "total" : 3,
    "successful" : 3,
    "failed" : 0
  },
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "timestamp" : 1611517217510,
  "status" : "green",
  "indices" : {
    "count" : 11,
    "shards" : {
      "total" : 22,
      "primaries" : 11,
      "replication" : 1.0,
      "index" : {
        "shards" : {
          "min" : 2,
          "max" : 2,
          "avg" : 2.0
        },
        "primaries" : {
          "min" : 1,
          "max" : 1,
          "avg" : 1.0
        },
        "replication" : {
          "min" : 1.0,
          "max" : 1.0,
          "avg" : 1.0
        }
..


- reroute :

on peut vouloir deplacer les shards d'un node vers un autre :

POST /_cluster/reroute
{
  "commands": [
    {
      "move": {
        "index": "test", "shard": 0,
        "from_node": "node1", "to_node": "node2"
      }
    },
    {
      "allocate_replica": {
        "index": "test", "shard": 1,
        "node": "node3"
      }
    }
  ]
}

- infos sur les nodes :

GET _nodes/_all

- api os :

GET _nodes/os 
GET _nodes/process
GET _nodes/http
GET _nodes/plugins
...
..


= index : 

- voir les index 
GET /_cat/indices


- creation d'index : 

curl -X PUT http://localhost:9200/test1
{"acknowledged":true,"shards_acknowledged":true,"index":"test1"}


- suppression d'index :

DELETE test1



= doc :


-> api à document unique

la creation d'un records dans notre index se fait assez facilement :

PUT testo/_doc/1
{
  "user": "bob",
  "age": 12

}

{
  "_index" : "testo",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}

si le param autocreate index est défini a true dans notre conf elastic, on peut donc créer un index et un premier document directement dans es ( sans passer par la creation d'un index puis ensuite dans un second temps la creation d'un doc pour cet index. )

on peut modifier cette conf via l'api :


PUT _cluster/settings
{

  "persistent":
  {
    "action.auto_create_index": "true"  < mettre à false si on veut désactiver cette option.
  }

}

- affichage de doc :

GET testo1/_doc/1

nous donne :
{
  "_index" : "testo1",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "_seq_no" : 0,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "user" : "bob",
    "age" : 12
  }
}

- suppression de doc :

DELETE testo1/_doc/1

- Mise à jour du doc :

on va pouvoir modifier un element de notre doc

ici le user bob present précedemment dans notre doc  va être remplacé par babar

POST testo1/_doc/1/_update
{
  "doc":
   {
  "user": "babar"
   }
}

qui nous donne donc :

{
  "_index" : "testo1",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 2,
  "_seq_no" : 4,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "user" : "babar",
    "age" : 12
  }
}

La methode a utilser est plutot maintenant :

POST testo1/_update/1/
{
  "doc":
   {
  "user": "bouli"
   }
}

qui nous donne :

GET testo1/_doc/1

{
  "_index" : "testo1",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 3,
  "_seq_no" : 5,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "user" : "bouli",
    "age" : 12
  }
}

- Multi documents :

permet d'intervenir sur plusieurs docs en même temps

GET index/_doc/_mget < recup tous les docs de l'index

GET /testo/_doc/_mget
/!\ ne marche pas ex sur site es


= shards : 
- voir les shards :

curl http://localhost:9200/_cat/shards

- Examiner l'allocation des shards : 

GET /_cat/allocation?v
GET /_cluster/allocation/explain?pretty

- Arrêter l'allocation de shard : utile quand on doit faire une inter sur un server elastic : on est sur qu'il ne prendra pas d'index pendant l'inter : 
PUT /_cluster/settings
{
  "persistent": {
"cluster.routing.allocation.enable": "none"
  }
}

- Remettre l'allocation de shard auto : apres une inter sur un /des serveurs du cluster : 
PUT /_cluster/settings
{
  "persistent": {
"cluster.routing.allocation.enable": null
  }
}

- forcer la reallocation de shards failed :

potentiellement un pb de reallocation de shard. on va examiner en recherchant par le nom de l'index qui nous interresse et le status "ALLOCATION FAILED" :


curl -s "http://127.0.0.1:9200/_cluster/state/routing_table" | jq '.routing_table.indices  | .[] | .shards | .[] | .[] | select(.index == "accesslog-private-2019.08.15.18") | select(.unassigned_info.reason == "ALLOCATION_FAILED")'
{
  "state": "UNASSIGNED",
  "primary": false,
  "node": null,
  "relocating_node": null,
  "shard": 3,
  "index": "accesslog-2019.08.15.18",
  "recovery_source": {
    "type": "PEER"
  },
  "unassigned_info": {
    "reason": "ALLOCATION_FAILED",
    "at": "2019-08-16T00:26:04.899Z",
    "failed_attempts": 5,
    "delayed": false,
    "details": "failed shard on node [cXQDsqDFSS]: failed recovery, failure RecoveryFailedException[[accesslog-2019.08.15.18][3]: Recovery failed from {logdb-idx01}{jfrP3TtTT6iLWNii0AZBCg}{YQSYfyIZQCKOn9uOZluucw}{192.168.0.5}{192.168.0.5:9300}{ml.machine_memory=67254464512, ml.max_open_jobs=20, datacenter=home, xpack.installed=true} into {logdb-idx04}{cXQDxuBAT76aftWvbztzWA}{ao5Fa5AJQF66YdxSik29Ww}{19.168.0.7}{192.168.0.7:9300}{ml.machine_memory=67254464512, xpack.installed=true, ml.max_open_jobs=20, datacenter=outside}]; nested: RemoteTransportException[[logdb-idx01][192.168.0.5:9300][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] prepare target for translog failed]; nested: RemoteTransportException[[logdb-idx04][192.168.0.7:9300][internal:index/shard/recovery/prepare_translog]]; nested: EngineCreationFailureException[failed to open reader on writer]; nested: FileSystemException[/var/lib/elasticsearch/logdb/nodes/0/indices/nEkAL4RWSASA/3/index/_2yi_Lucene50_0.pos: Too many open files]; ",
    "allocation_status": "no_attempt"
  }
}

on voit ici qu'on a un pb de nombre de fichiers ouvert. On va pouvoir executer un retry pour tenter une nouvelle allocation :

curl -s -XPOST "http://127.0.0.1:9200/_cluster/reroute?retry_failed" | jq '.state.routing_table.indices | .[] | .shards | .[] | .[]  | select(.unassigned_info.reason=="ALLOCATION_FAILED")'


on peut globalement forcer un retry sur la réallocation des shard : 

curl -X POST  http://localhost:9200/_cluster/reroute?retry_failed=true
- rerouting :

- routage de shards d'un index d'une machine 1 vers une machine 2 :

POST /_cluster/reroute
{
  "commands": [
    {
      "move": {
        "index": "test", "shard": 0,
        "from_node": "node1", "to_node": "node2"
      }
    },
    {
      "allocate_replica": {
        "index": "test", "shard": 1,
        "node": "node3"
      }
    }
  ]
}

= recherche / search :

l'operation de recherche est faite sur tous les shards. Les resultats sont recup et triés par le master.

https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html

différentes méthodes existent 

ex : template, shards ...

GET food/_search_shards

{
  "nodes" : {
    "FnbdoPhIS8qFi1GoO-vKTg" : {
      "name" : "elasticsearch-master-2",
      "ephemeral_id" : "QTlcV7zXQPWJFUnDtz76zw",
      "transport_address" : "172.17.0.10:9300",
      "attributes" : {
        "ml.machine_memory" : "512000000",
        "ml.max_open_jobs" : "20",
        "xpack.installed" : "true",
        "transform.node" : "true"
      }
    },
    "zVl54Y-9RTyY6NLDaBiLyA" : {
      "name" : "elasticsearch-master-1",
      "ephemeral_id" : "aKlmE9aDS2iJTA0I0xMeTQ",
      "transport_address" : "172.17.0.19:9300",
      "attributes" : {
        "ml.machine_memory" : "512000000",
        "xpack.installed" : "true",
        "transform.node" : "true",
        "ml.max_open_jobs" : "20"
      }
    }
  },
  "indices" : {
    "food" : { }
  },
  "shards" : [
    [
      {
        "state" : "STARTED",
        "primary" : true,
        "node" : "FnbdoPhIS8qFi1GoO-vKTg",
        "relocating_node" : null,
        "shard" : 0,
        "index" : "food",
        "allocation_id" : {
          "id" : "LOEuIpzQQ_C5BkeA8oWYbg"
        }
      },
      {
        "state" : "STARTED",
        "primary" : false,
        "node" : "zVl54Y-9RTyY6NLDaBiLyA",
        "relocating_node" : null,
        "shard" : 0,
        "index" : "food",
        "allocation_id" : {
          "id" : "4Lcaw597QV-L0I-iJFIikg"
        }
      }
    ]
  ]
}

COUNT :
 GET /my-index-000001/_count?q=product


{
  "count" : 0,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  }
}

- validate : 

on peut utiliser la fonction de validation d'un requette pour voir si la requette sera couteuse : 

GET my-index-000001/_validate/query?q=product

{
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "valid" : true
}




= mappings :

- creation de mapping :

on le fait en cli : l'index doit exister au préalable 

curl -XPUT "http://localhost:9200/twitter1?pretty"                                                                               [☸ |minikube:default]
{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "twitter1"
}

curl -XPUT "http://localhost:9200/twitter1/_mapping?pretty" -H "Content-Type: application/json" -d '{"properties": {"email": {"type": "keyword" }}}'
{
  "acknowledged" : true
}

on peut recupérer notre mapping :

curl -XGET "http://localhost:9200/twitter1/_mapping?pretty"                                                                      [☸ |minikube:default]
{
  "twitter1" : {
    "mappings" : {
      "properties" : {
        "email" : {
          "type" : "keyword"
        }
      }
    }
  }
}

 on peut mettre à jour un mapping :

 curl -XPUT "http://localhost:9200/twitter1/_mapping?pretty" -H "Content-Type: application/json" -d '{"properties": {"new_type": {"type": "integer" }}}'
{
  "acknowledged" : true
}

exam :

curl -XGET "http://localhost:9200/twitter1/_mapping?pretty"                                                                        [☸ |minikube:default]
{
  "twitter1" : {
    "mappings" : {
      "properties" : {
        "email" : {
          "type" : "keyword"
        },
        "new_type" : {
          "type" : "integer"
        }
      }
    }
  }
}



indexation > recoit le message le traite et le stocke dans un index
recherche > recherche d'info dans l'index
les deux phase sont liées : si l'index n'est pas bon : la recherche ne le sera pas.
es a un mapping explicite defini.
si on en met pas : un par defaut est associé à l'index.

Il est possible de définir et modifier un mapping.

- utilisation de la creation du mapping explicte 

si on compare l'index a une  bdd 
le mapping est similaire a la definition de la table

es est capable de comprendre la structure et est capable de créer le mapping

on cree un index :

PUT test_mapping

on va cree un doc dans notre index :

PUT test_mapping/_doc/1
{
  "prenom": "lapin",
  "nom": "nain"
}

pour voir le mapping implicte ( crée automatiquement par es sur notre index : )

GET test_mapping/_mapping

ce qui nous donne :

{
  "test_mapping" : {
    "mappings" : {
      "properties" : {
        "nom" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
        },
        "prenom" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
        }
      }
    }
  }
}

on voit que le type des champ est détecté automatiquement par es 
ici on a deux type text

on peut donc ici facilement et rapidement insérer des données dans es ..

- mapping de type de base :

pour avoir des bonnes perf d'indexation on va définir des mappings 

> reduction taille index sur disque
> indexation uniquement des champs interressants > gain de perfs
> defini correctement l'analysee du champ

es permet l'utilisant de champ de base


 PUT /my-index-000001
{
  "mappings": {
    "properties": {
      "age":    { "type": "integer" },
      "email":  { "type": "keyword"  },
      "name":   { "type": "text"  }
    }
  }
}

GET my-index-000001/_mapping

{
  "my-index-000001" : {
    "mappings" : {
      "properties" : {
        "age" : {
          "type" : "integer"
        },
        "email" : {
          "type" : "keyword"
        },
        "name" : {
          "type" : "text"
        }
      }
    }
  }
}

on peut avoir des setting

> store : garder sur disque ou pas ( booléen)
> index : savoir si on index ou pas le champ ( booleen )
..


= aggregation :



Aggregation :

https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html

on peut regrouper les data pour les présenter sous formes de graphs.Plusieurs type d'aggrégations :

> metrics
> bucket
> pipeline
...



GET /my-index-000001/_search
{
  "aggs": {
    "my-agg-name": {
      "terms": {
        "field": "my-field"
      }
    }
  }
}



Preparation du dataset pour travailler nos données : on injecte les datatests présentes dans kibana ex index kibana_sample_data_ecommerce

- on tri par type de produits avec un résultat de 10 records

POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "tag": {
      "terms": {
        "field": "products",
        "size": 10
      }
    }
  }

}

nous donne :

{
  "took" : 40,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "tag" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ ]
    }
  }
}

- aggregation de metriques

statistiques ; moyenne, cardinalité, max, min, centille

calcul des requettes sur l'age :
min / max /moyenne /somme

ici on va sortir les données statistiques sur le prix des produits :

POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "age": {                                       <<<<< nom que l'on donne
      "extended_stats": {                          <<<<< metrique d'aggregation
        "field": "products.base_unit_price"        <<<<< champ sur lequel on travaille.
      }
    }
  }
}


nous donne :

{
  "took" : 27,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "age" : {
      "count" : 10087,
      "min" : 5.98828125,
      "max" : 540.0,
      "avg" : 34.77499349410132,
      "sum" : 350775.359375,
      "sum_of_squares" : 1.8856063677703857E7,
      "variance" : 660.0429104339428,
      "variance_population" : 660.0429104339428,
      "variance_sampling" : 660.1083519281361,
      "std_deviation" : 25.69130028694427,
      "std_deviation_population" : 25.69130028694427,
      "std_deviation_sampling" : 25.692573867328594,
      "std_deviation_bounds" : {
        "upper" : 86.15759406798986,
        "lower" : -16.607607079787222,
        "upper_population" : 86.15759406798986,
        "lower_population" : -16.607607079787222,
        "upper_sampling" : 86.1601412287585,
        "lower_sampling" : -16.61015424055587
      }
    }
  }
}


on peut calculer uniquement la moyenne :


POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "avg-age": {
      "avg": {
        "field": "products.base_unit_price"
      }
    }
  }
}
nous donne donc :

{
  "took" : 7,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "avg-age" : {
      "value" : 34.77499349410132
    }
  }
}

- bucket aggregation :

chaque bucket a un type de doc qui s'y trouve
on peut avoir des sous aggregation de bucket ( bucket parent / enfant )

agreggation de terme, range , global, histogramme ...

- terme :

POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "tag": {          <<<<< on met le nom qu'on veut
      "terms": {      <<<<  ici on applique l'aggregation sur les termes
        "field": "customer_last_name.keyword" <<<<< le champ de notre table qu'on va parser
      }
    }
  }
}

nous donne :

{
  "took" : 16,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "tag" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 4140,
      "buckets" : [
        {
          "key" : "Perkins",
          "doc_count" : 59
        },
        {
          "key" : "Underwood",
          "doc_count" : 59
        },
        {
          "key" : "Tran",
          "doc_count" : 56
        },
        {
          "key" : "Rivera",
          "doc_count" : 54
        },
        {
          "key" : "Graham",
          "doc_count" : 53
        },
        {
          "key" : "Shaw",
          "doc_count" : 53
        },
        {
          "key" : "Byrd",
          "doc_count" : 51
        },
        {
          "key" : "Foster",
          "doc_count" : 51
        },
        {
          "key" : "Gregory",
          "doc_count" : 51
        },
        {
          "key" : "Bryant",
          "doc_count" : 48
        }
      ]
    }
  }
}


par defaut 10 records sont rencoyés par es ( on peut biensur définir ce nombre avec "size" )

- aggregation de range :

on va pouvoir aggreger nos resultats par intervals
ex : on tri les prix par interval de prix :

POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "prices": {
      "range": {
        "field": "products.price",
        "ranges": [
          {"to": 10},
          {"from": 10, "to": 100},
          {"from": 101, "to": 5000}
        ]
      }
    }
  }
}


 ce qui nous donne :
{
  "took" : 38,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "prices" : {
      "buckets" : [
        {
          "key" : "*-10.0",
          "to" : 10.0,
          "doc_count" : 357
        },
        {
          "key" : "10.0-100.0",
          "from" : 10.0,
          "to" : 100.0,
          "doc_count" : 4655
        },
        {
          "key" : "101.0-5000.0",
          "from" : 101.0,
          "to" : 5000.0,
          "doc_count" : 185
        }
      ]
    }
  }
}

on peut trier par interval de date :

POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "prices": {
      "date_range": {
        "field": "order_date",
        "format": "MM-yyy"
        , "ranges": [
            { "to": "now-10M/M" },
            { "from": "now-10M/M" }
         ]
      }
    }
  }
}POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "prices": {
      "date_range": {
        "field": "order_date",
        "format": "MM-yyy"
        , "ranges": [
            { "to": "now-10M/M" },
            { "from": "now-10M/M" }
         ]
      }
    }
  }
}

{
  "took" : 16,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "prices" : {
      "buckets" : [
        {
          "key" : "*-04-2020",
          "to" : 1.5856992E12,
          "to_as_string" : "04-2020",
          "doc_count" : 0
        },
        {
          "key" : "04-2020-*",
          "from" : 1.5856992E12,
          "from_as_string" : "04-2020",
          "doc_count" : 4675
        }
      ]
    }
  }
}
- agregation d'histogramme :


visualisation de données.

kibana > add visualisation > add vertical bar

il faut cependant aggreger les données.

on va trier le prix de nos produits par interval de 10$


POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "prices": {
      "histogram": {
        "field": "products.price",
        "interval": 10
      }
    }
  }
}

nous donne

{
  "took" : 32,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "prices" : {
      "buckets" : [
        {
          "key" : 0.0,         <<<< abcisse
          "doc_count" : 357    <<<<  ordonnée
        },
        {
          "key" : 10.0,
          "doc_count" : 2233
        },
        {
          "key" : 20.0,
          "doc_count" : 2356
        },
        {
          "key" : 30.0,
          "doc_count" : 969
        },
   ...

on peut donc maintenant mettre en place notre histogramme.

ajout d'index pattern

kibana > index pattern  > on cherche un index pattern qui match un index crée auparavant > on defini par example le tri de filtre par date .
on voit ensuite la structure de notre index

on va ensuite sur kibana vizualise > vertical bar > on choisi notre index

metric vont permettre de faire des count
bucket : histogramme, range




== troubleshooting : == 

 ==== reset d'offset d'index avec kafka comme intermediaire entre les log et les indexers : ===

Sion accumule du retard sur un cluster elk et qu'on veut vraimen avoir des logs dispos dans kibana par exemple  :  on va stopper logstash sur notre / nos serveur hebergeant kafka : 

dans notre exemple on va reset l'offset de l'index accesslog jusqu'à 11h 

 /opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group logstash --topic accesslog --reset-offsets --to-datetime 2019-12-04T11:00:00.000Z  --execute

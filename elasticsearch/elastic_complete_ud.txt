=== elasticsearch complete udfr===

== setup elastic : helm - minikube : == 

helm repo add elastic https://helm.elastic.co
helm install elasticsearch --version <version> elastic/elasticsearch
helm install elasticsearch --version 7.10.2 elastic/elasticsearch

# Minikube
This example deploy a 3 nodes Elasticsearch 7.10.2 cluster on [Minikube][]
using [custom values][].

If helm or kubectl timeouts occur, you may consider creating a minikube VM with
more CPU cores or memory allocated.

Note that this configuration should be used for test only and isn't recommended
for production.

## Requirements

In order to properly support the required persistent volume claims for the
Elasticsearch StatefulSet, the `default-storageclass` and `storage-provisioner`
minikube addons must be enabled.

```
minikube addons enable default-storageclass
minikube addons enable storage-provisioner
```

## Usage

* Deploy Elasticsearch chart with the default values: `make install`

* You can now setup a port forward to query Elasticsearch API:

  ```
  kubectl port-forward svc/elasticsearch-master 9200
  curl localhost:9200/_cat/indices
  ```

[custom values]: https://github.com/elastic/helm-charts/tree/7.10/elasticsearch/examples/minikube/values.yaml
[minikube]: https://minikube.sigs.k8s.io/docs/

---
# Permit co-located instances for solitary minikube virtual machines.
antiAffinity: "soft"

# Shrink default JVM heap.
esJavaOpts: "-Xmx128m -Xms128m"

# Allocate smaller chunks of memory per pod.
resources:
  requests:
    cpu: "100m"
    memory: "512M"
  limits:
    cpu: "1000m"
    memory: "512M"

# Request smaller persistent volumes.
volumeClaimTemplate:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: "standard"
  resources:
    requests:
      storage: 100M

setup avec les valeurs suivantes : 

 ✘ boogie@boogieland  ~/Documents/lab/elasticsearch/helm  helm install elasticsearch elastic/elasticsearch -f minikube-values.yaml
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
NAME: elasticsearch
LAST DEPLOYED: Sun Jan 24 08:45:01 2021
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Watch all cluster members come up.
  $ kubectl get pods --namespace=default -l app=elasticsearch-master -w
2. Test cluster health using Helm test.
  $ helm test elasticsearch

kubectl get pods --namespace=default -l app=elasticsearch-master -w
NAME                     READY   STATUS    RESTARTS   AGE
elasticsearch-master-0   1/1     Running   0          116s
elasticsearch-master-1   1/1     Running   0          116s
elasticsearch-master-2   1/1     Running   0          116s

✘ boogie@boogieland  ~/Documents/lab/elasticsearch/helm  helm test elasticsearch
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/boogie/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/boogie/.kube/config
NAME: elasticsearch
LAST DEPLOYED: Sun Jan 24 08:45:01 2021
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE:     elasticsearch-kvuab-test
Last Started:   Sun Jan 24 08:47:16 2021
Last Completed: Sun Jan 24 08:47:18 2021
Phase:          Succeeded
NOTES:
1. Watch all cluster members come up.
  $ kubectl get pods --namespace=default -l app=elasticsearch-master -w
2. Test cluster health using Helm test.
  $ helm test elasticsearch

on peut faire un port fwd de l'appli pour requetter en local sur notre laptop :

kubectl port-forward svc/elasticsearch-master 9200

curl  localhost:9200                                                                                                               [☸ |minikube:default]
{
  "name" : "elasticsearch-master-2",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "version" : {
    "number" : "7.10.2",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "747e1cc71def077253878a59143c1f785afa92b9",
    "build_date" : "2021-01-13T00:42:12.435326Z",
    "build_snapshot" : false,
    "lucene_version" : "8.7.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}


- set up kibana :

helm install kibana --version 7.10.2 elastic/kibana

on fait un port forward pour accéder à l'interface kibana : 
kubectl port-forward kibana-kibana-84579f77bc-vn8nh 5601


=== fin set up helm elastic sur minikube ===

= presention : =

- elasticsearch :
stocke les données sous forme de documents ( bdd orientée documents.) que l'on peut interroger avec une api rest en http. rechercher / recuperer / updater / supprimer se fait en requette http
recherche / analyse journaux / analyse megadonnées.

chaque base de donnée ou collection de donnée ayant des caractéristiques communes est appellé un index dans elasticsearch.
les tables de l'index sont nommées par type.
chaque ligne est un document
chaque colonne est un champ
la maniere dont les docs et champs sont stockés et indexés est un mapping 
un cluster est en ensemble de serveurs qui contient l'intégralité des données.
un index peut être constitué de fragments (shard)  qui contiennent des données.
L'index est stocké sous format json.
Les repliques sont les copies des fragments distribués.

document oriented :  insert /delete/retrieve/analyse/search 
c'est essentiellement un moteur de recherche qui va utiliser les termes de la recherche pour retrouver le document.
comme analogie on peut prendre l'index d'un livre.
Basé sur le moteur de recherche lucene , il utilise un index inversé "inverted index" (cf doc elasticsearch :
https://www.elastic.co/guide/en/elasticsearch/reference/current/documents-indices.html )
"An inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in."
Pour créer un index inversé : tous les mots /champs  du documents vont être isolés et associés à un term ou token ensuite une liste de tous les documents dans lesquels ces mots apparaissent est créée :

doc1 : le lapin nain joue
doc2: jouer avec le lapin 

terms    doc1  doc2
le        X     X
lapin     X     X
nain      X
jouer           X
avec            X

si on cherche le texte lapin nain dans quel document on a le plus de chance de le trouver ?
-> dans le document 1 : elasticsearch utilise un algorithme de score pour ses recherches : le doc 1 a deux entrées sur le mot lapin, le doc 1 a une seule entrée sur le mot nain ...

Elasticsearch est orienté document . Un document est l'unité de base qui sera indexée et présenté sous le format json.
https://www.elastic.co/guide/en/elasticsearch/reference/6.1/_basic_concepts.html

Dans un mode sgbd on stocke les données dans des tables, ligne et colonne.
Dans elastic :
-> les données sont stockées dans un index
-> l'equivalent des lignes est un document dans elastic 
-> l'equivalent d'une colonne est un champ dans elastic
on peut donc avoir une equivalence  de type : 
SGBD       ELASTIC
table      index
raw        document
column     field 
Attention cette comparaison est imagée et on a pas vraiment une correspondance stricte entre sgbd classic et elastic.

/!\ Attention depuis la version 6 : les types disparaissent progressivement  d'elastic pour etre retirés en version 7 

Quand on insere des données dans elastic on dis qu'on index.

- kibana :
permet de représenter les données sous forme de tableaux de bord et faire des recherches.

- logstash :
collecte les données de différentes sources les traite et les envoient à elasticsearch
on peut ajouter des plugins . transforme les données 
configuration modulaire pour creation de pipeline 

- beats :
expéditeur de données légers.
installés sur des serveurs qui vont envoyer les logs vers elasticsearch ou logstash ( plusieurs type filebeat, logbeat ...) 

== Analyse de docs et creation de mapping : ==

= recherche :

2 types : terme exact et "plain text"

les synonymes ou légères fautes de frappes doivent être recupéres.
on va avoir des poids données sur le "text frequency"  ( par rapport au total de text global )

index inversé : coeur du moteur de recherche 

tous les termes uniques sont stockés dans l'index avec leur position dans le text
ex :

doc1 :
je deteste quand les araignées sont assises sur un mur

doc2: 
je deteste quand l'araignée reste assise la.

on aura donc un index inversé du type suivant utilisé pour les recherches de texte : 
terme         doc position
je            1:1, 2:1
deteste       1:2, 2:2
quand         1:3, 2:3
les araignées 1:4
asseoir       1:5, 2:5
araignée      2:4
sur           1:6
mur           1:7
la            2:6 

on voit ici que le moteur de recherche stocke l'emplacement du mot et le mot avec les synonymes ou même sens ( assises = assise ==> stocker comme asseoir dans le moteur)

Cela est essentiellent lié au fonctionnement du moteur lucene qui va embarquer des analyseurs ( simple, whitespace, keyword, language) qui vont filtrer le document recu , le découper via le process de tokenizer , eventuellement appliquer des filtres de tokenizers (ex supprimer des mots de jonction ex : dans, vers ...) ecrire le document et créer l'index inversé 


> creation d'index :

curl -X PUT http://localhost:9200/test1                                                     
{"acknowledged":true,"shards_acknowledged":true,"index":"test1"}

> recherche avec utilisation d'analyzer :

- standard :

curl -X GET "http://localhost:9200/_analyze?pretty" -H "Content-Type: application/json" -d '
{ "analyzer": "standard",
"text": "Quick Brown Foxes !"}'
{
  "tokens" : [
    {
      "token" : "quick",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "brown",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "foxes",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}

on voit que chaque mot est considéré comme une suite alaphanum , le "!" n'est pas analysé et on a la position des mots

- whitespace : 

curl -X GET "http://localhost:9200/_analyze?pretty" -H "Content-Type: application/json" -d '                                      [☸ |minikube:default]
{ "analyzer": "whitespace",
"text": "Quick Brown Foxes !"}'
{
  "tokens" : [
    {
      "token" : "Quick",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "Brown",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "Foxes",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "!",
      "start_offset" : 18,
      "end_offset" : 19,
      "type" : "word",
      "position" : 3
    }
  ]
}

ici on voit que tous les termes sont enregistrés ( même le "!" ) et considérés comme des mots.

Il va être possible de créer nos analyzers personaliser 
on peut aussi personaliser les analyzers
on peut changer un analyzer par defaut

- mapping :

 une fois qu'on enregistre un document et un type pour un champ on ne peut pas le modifier par la suite : 
 ex ici on crée un clé "valid" en entier : 
curl -XPOST "http://localhost:9200/myindex/type1/1" -H "Content-Type: application/json" -d '{"valid": 5}'                        
{"_index":"myindex","_type":"type1","_id":"1","_version":1,"result":"created","_shards":{"total":2,"successful":2,"failed":0},"_seq_no":0,"_primary_term":1}%

ici on veut créer dans un autre document d'un autre index un champ "valid" en string :

curl -XPOST "http://localhost:9200/myindex/type2/1" -H "Content-Type: application/json" -d '{"valid": "40"}'                       [☸ |minikube:default]
{"error":{"root_cause":[{"type":"illegal_argument_exception","reason":"mapper [valid] cannot be changed from type [long] to [text]"}],"type":"illegal_argument_exception","reason":"mapper [valid] cannot be changed from type [long] to [text]"},"status":400}%


- metadonnée des doc elasticsearch :

données de structuration

_id : 
identifiant unique du doc 

_source :
champ généré par es peut être désactivé. contient les données json réelle du doc . A chaque requette de recherche le champ source est renvoyé au user 
attention aux comportement es si c'est fait : 
curl -XPUT index_name/_mapping/doc_type { "_source":{"enable": false}}

_all :
tous les champs sont indexés séparemment lors de l'indexation d'un doc
on peut le désactiver :
curl -XPUT index_name/_mapping/doc_type { "_all":{"enable": false}}

_ttl:

on peut vouloir qu'un doc soit supprimé automatiquement de l'index au bout d'un certain temps.

ex :
$ curl http://localhost:9200/myindex/mytype/_mapping
{"mytype":{"_timestamp":{"enabled":true},"_ttl":
{"enabled":true,"default":86400000},"properties":{}}}


dynamic :

on veut pouvoir restreindre l'indexation dynamique des champs

= type de données et options d'analyse d'index : 


- type de base :
int, string ..

- type de données complexe :
ip, geoloc, tableau .. 

= attributs communs :

- index :

analyzed : on peut utiliser un analyzer particulier pour indexer un champ
not_analyzed : si on a positionner le not analyzed : le text ne sera pas indexer et donc pas de recherche possible.

store : 
prend comme valeur oui ou non. valeur par defaut non
boost : 
valeur par defaut 1 : precise l'importance du champ dans le document

null_value :
definir une valeur par default a indexer 


=attributs sur les chaines :

- term_vector :

liste des termes du document et leur nombre d'occurence
utilisé pour les requettes de haut niveaux ( pas fréquents.)

- omit_norms

- analyzer :
permet de définir un nalyzer pour la chaine.

- index_analyzer :
analyser utilisé pour l'indexation 

- search_analyzer :
analyser utilisé pour la recherche

= attributs pour les numeriques :

byte
short
long
floatset
double 

{"price": {"type": "float"}, "age": {"type": "integer"}}

= attributs sur les dates : 

nombreux formats
on doit s'assurer du bon format à la création 

ex :
{ "creation_time":{"type": "date", "format": "YYYY-MM-DD"}, "indexing_time": {"type": "date", "format": "date_optional_time"}}

= attributs booleen :
vrai / false

= attributs tableaux :

par default les champs sont en tableaux
toutes les valeurs de données du tableaux doivent être du même type.
les ordres ne sont pas conservés dans la requette

= attributs objects :
objects internes a es 
on ne peut pas modifier le type.

on peut indexer le même champ de différentes manières mais il faut définir un mapping spécifique

= creation de mapping :

on le fait en cli : l'index doit exister au préalable 

curl -XPUT "http://localhost:9200/twitter1?pretty"                                                                               [☸ |minikube:default]
{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "twitter1"
}

curl -XPUT "http://localhost:9200/twitter1/_mapping?pretty" -H "Content-Type: application/json" -d '{"properties": {"email": {"type": "keyword" }}}'
{
  "acknowledged" : true
}

on peut recupérer notre mapping :

curl -XGET "http://localhost:9200/twitter1/_mapping?pretty"                                                                      [☸ |minikube:default]
{
  "twitter1" : {
    "mappings" : {
      "properties" : {
        "email" : {
          "type" : "keyword"
        }
      }
    }
  }
}

 on peut mettre à jour un mapping :

 curl -XPUT "http://localhost:9200/twitter1/_mapping?pretty" -H "Content-Type: application/json" -d '{"properties": {"new_type": {"type": "integer" }}}'
{
  "acknowledged" : true
}

exam :

curl -XGET "http://localhost:9200/twitter1/_mapping?pretty"                                                                        [☸ |minikube:default]
{
  "twitter1" : {
    "mappings" : {
      "properties" : {
        "email" : {
          "type" : "keyword"
        },
        "new_type" : {
          "type" : "integer"
        }
      }
    }
  }
}


== administration es  : ==

chaque serveur est considéré comme un node 

on peut avoir des noeuds master et des noeuds data
un ensemble de noeuds forme un cluster

es stocke des doc json et stocke dans les index. On fait des recherches sur les champs en utilisant le moteur de recherche lucene
on peut faire des requettes simples ou plus complexes 

on peut avoir des points d'api _cluster 
on peut avoir des filtres sur les nodes
_all , _local , _master ...

dans les noeuds master on a juste la notion de haute dispo et d'ordonnancement pas de data.

= conventions api :

api rest 
on peut idealement tout faire via l'api rest

- gestion de plusieurs index :

_all
ignore_no_available : true ou false
allow_no_indices : true ou false
expand_wildcards : open ou close
<static_name{date_math_expr{date_format|timezone}}>

on peut requetter via kibana en passant par le devtool kibana 

options courantes de l'api :

si on veut modifier certains champs

?pretty=true < utilisé pour le debugage

on  peut ajouter des data de tests > kibana > add data [Flights] Global Flight Dashboard par exemple. une fois installé > view dashboard.

on peut ensuite requetter via la partie devtools :


GET _cat/indices?pretty   : voir nos index

GET kibana_sample_data_flights/_search?pretty : examen de l'index kibana_sample_data_flights avec affichage pretty ( pour formattage json visuel )

on peut afficher sous differents formats 

> format yaml :

GET kibana_sample_data_flights/_search?format=yaml

---
took: 18
timed_out: false
_shards:
  total: 1
  successful: 1
  skipped: 0
  failed: 0
hits:
  total:
    value: 10000
    relation: "gte"
  max_score: 1.0
  hits:
  - _index: "kibana_sample_data_flights"
    _type: "_doc"
    _id: "ajbFNXcBkWffCDp4Jd83"
    _score: 1.0
    

> format human readable :

GET kibana_sample_data_flights/_search?human=true

{
  "took" : 23,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 10000,
      "relation" : "gte"
    },
    "max_score" : 1.0,


on peut filtrer les champs que l'on veut recupérer :

-ex : on veut uniquement des champs :  hits FlightNum et carrier :

GET kibana_sample_data_flights/_search?filter_path=hits.hits._source.FlightNum,hits.hits._source.Carrier

{
  "hits" : {
    "hits" : [
      {
        "_source" : {
          "FlightNum" : "8I8E792",
          "Carrier" : "Kibana Airlines"
        }
      },
      {
        "_source" : {
          "FlightNum" : "DHMO1B5",
          "Carrier" : "ES-Air"
        }
      },
      ..


- modif de l'affichage :

on peut activer le flatsetting ( qui est a false par default ) :

GET  kibana_sample_data_flights/_settings?flat_settings=true


{
  "kibana_sample_data_flights" : {
    "settings" : {
      "index.auto_expand_replicas" : "0-1",
      "index.creation_date" : "1611514841305",
      "index.number_of_replicas" : "1",
      "index.number_of_shards" : "1",
      "index.provided_name" : "kibana_sample_data_flights",
      "index.routing.allocation.include._tier_preference" : "data_content",
      "index.uuid" : "kWV2G9CHTGCNr2-7_beJuQ",
      "index.version.created" : "7100299"
    }
  }
}

par default on a  :

GET  kibana_sample_data_flights/_settings?flat_settings=false

{
  "kibana_sample_data_flights" : {
    "settings" : {
      "index" : {
        "routing" : {
          "allocation" : {
            "include" : {
              "_tier_preference" : "data_content"
            }
          }
        },
        "number_of_shards" : "1",
        "auto_expand_replicas" : "0-1",
        "provided_name" : "kibana_sample_data_flights",
        "creation_date" : "1611514841305",
        "number_of_replicas" : "1",
        "uuid" : "kWV2G9CHTGCNr2-7_beJuQ",
        "version" : {
          "created" : "7100299"
        }
      }
    }
  }
}


= administration cluster : 

- etat de santé : 
intégrité du cluster : 

GET _cluster/health

{
  "cluster_name" : "elasticsearch",  <<< nom du cluster 
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 11,   <<< nombre de shard (fragment ) primaire 
  "active_shards" : 22,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}

GET _cluster/state

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "version" : 150,
  "state_uuid" : "ksJb1lOCRnKZ0sWlEGOvSw",
  "master_node" : "8lCeAK5DQ1Wl4vgTTw8uRA",
  "blocks" : { },
  "nodes" : {
    "zVl54Y-9RTyY6NLDaBiLyA" : {
      "name" : "elasticsearch-master-1",
      "ephemeral_id" : "VPCVrXvxRVmwpybe80bERg",
      "transport_address" : "172.17.0.11:9300",
      "attributes" : {
        "ml.machine_memory" : "512000000",
        "ml.max_open_jobs" : "20",
        "xpack.installed" : "true",
        "transform.node" : "true"
      }
    },
..
...


GET _cluster/state/version

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "version" : 150,
  "state_uuid" : "ksJb1lOCRnKZ0sWlEGOvSw"
}


GET _cluster/state/master_node

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "master_node" : "8lCeAK5DQ1Wl4vgTTw8uRA"
}


GET _cluster/state/nodes

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "nodes" : {
    "zVl54Y-9RTyY6NLDaBiLyA" : {
      "name" : "elasticsearch-master-1",
      "ephemeral_id" : "VPCVrXvxRVmwpybe80bERg",
      "transport_address" : "172.17.0.11:9300",
      "attributes" : {
        "ml.machine_memory" : "512000000",
        "ml.max_open_jobs" : "20",
        "xpack.installed" : "true",
        "transform.node" : "true"
      }
    },
    "FnbdoPhIS8qFi1GoO-vKTg" : {
      "name" : "elasticsearch-master-2",
      "ephemeral_id" : "JKVaSDF6RJmSe9kEkWFA9A",
      "transport_address" : "172.17.0.12:9300",
      "attributes" : {
        "ml.machine_memory" : "512000000",
        "ml.max_open_jobs" : "20",
        "xpack.installed" : "true",
        "transform.node" : "true"
      }
    },
    "8lCeAK5DQ1Wl4vgTTw8uRA" : {
      "name" : "elasticsearch-master-0",
      "ephemeral_id" : "80fVwGrVRYSkeahu3L_uww",
      "transport_address" : "172.17.0.13:9300",
      "attributes" : {
        "ml.machine_memory" : "512000000",
        "ml.max_open_jobs" : "20",
        "xpack.installed" : "true",
        "transform.node" : "true"
      }
    }
  }
}


GET _cluster/state/routing_table

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "routing_table" : {
    "indices" : {
      ".apm-agent-configuration" : {
        "shards" : {
          "0" : [
            {
              "state" : "STARTED",
              "primary" : true,
              "node" : "zVl54Y-9RTyY6NLDaBiLyA",
              "relocating_node" : null,
              "shard" : 0,
              "index" : ".apm-agent-configuration",
              "allocation_id" : {
                "id" : "LPj2EYetS9qRrqvPLwNPvg"
              }
            },
            {
              "state" : "STARTED",
              "primary" : false,
              "node" : "8lCeAK5DQ1Wl4vgTTw8uRA",
              "relocating_node" : null,
              "shard" : 0,
              "index" : ".apm-agent-configuration",
              "allocation_id" : {
                "id" : "tfrlc1fOQ6q3C9Xn2a_XuQ"
              }
            }
          ]
        }
      },
      "kibana_sample_data_flights" : {
        "shards" : {
          "0" : [
            {
              "state" : "STARTED",
              "primary" : true,
              "node" : "FnbdoPhIS8qFi1GoO-vKTg",
              "relocating_node" : null,
              "shard" : 0,
              "index" : "kibana_sample_data_flights",
              "allocation_id" : {
                "id" : "vrMAGtylSRG0-qu5GWG8-A"
              }

....

GET _cluster/state/metadata


{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "metadata" : {
    "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
    "cluster_uuid_committed" : true,
    "cluster_coordination" : {
      "term" : 4,
      "last_committed_config" : [
        "zVl54Y-9RTyY6NLDaBiLyA",
        "8lCeAK5DQ1Wl4vgTTw8uRA",
        "FnbdoPhIS8qFi1GoO-vKTg"
      ],
      "last_accepted_config" : [
        "zVl54Y-9RTyY6NLDaBiLyA",
        "8lCeAK5DQ1Wl4vgTTw8uRA",
        "FnbdoPhIS8qFi1GoO-vKTg"
      ],
      "voting_config_exclusions" : [ ]
    },
    "templates" : {
      ".management-beats" : {
        "order" : 0,
        "version" : 70000,
        "index_patterns" : [
          ".management-beats"
        ],
        "settings" : {
          "index" : {
            "number_of_shards" : "1",
            "auto_expand_replicas" : "0-1",
            "codec" : "best_compression"
          }
        },
        "mappings" : {
          "_doc" : {
            "dynamic" : "strict",
            "properties" : {
              "beat" : {
                "properties" : {
                  "host_ip" : {
                    "type" : "ip"
                  },
                  "metadata" : {
                    "dynamic" : "true",
                    "type" : "object"
                  },
                  "active" :
...
.....

GET _cluster/state/blocks

{
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "blocks" : { }
}

- stats de cluster :

GET _cluster/stats


{
  "_nodes" : {
    "total" : 3,
    "successful" : 3,
    "failed" : 0
  },
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "b6WLtdlNSzyVJMl6ZqiGYQ",
  "timestamp" : 1611517217510,
  "status" : "green",
  "indices" : {
    "count" : 11,
    "shards" : {
      "total" : 22,
      "primaries" : 11,
      "replication" : 1.0,
      "index" : {
        "shards" : {
          "min" : 2,
          "max" : 2,
          "avg" : 2.0
        },
        "primaries" : {
          "min" : 1,
          "max" : 1,
          "avg" : 1.0
        },
        "replication" : {
          "min" : 1.0,
          "max" : 1.0,
          "avg" : 1.0
        }
..


- reroute :

on peut vouloir deplacer les shards d'un node vers un autre :

POST /_cluster/reroute
{
  "commands": [
    {
      "move": {
        "index": "test", "shard": 0,
        "from_node": "node1", "to_node": "node2"
      }
    },
    {
      "allocate_replica": {
        "index": "test", "shard": 1,
        "node": "node3"
      }
    }
  ]
}

- infos sur les nodes :

GET _nodes/_all

- api os :

GET _nodes/os 
GET _nodes/process
GET _nodes/http
GET _nodes/plugins
...
..


== indexation :

un index est un endroit dans lequel on stocke les données
il pointe vers un ou plusieurs shard
shard : unite de travail qui stock une quantite de données dans l'index
shard division de données de l'index
Les shards sont repartis sur l'ensemble des noeuds du cluster
a la creation de l'index un nombre defini de shard est défini

on peut creer un index facilement :

PUT index_tuto
{
  "settings":
    {
      "number_of_shards": 3,
      "number_of_replicas": 1
    }

}

> nous donne :

{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "index_tuto"
}

on peut maintenant requetter notre index :

GET index_tuto

{
  "index_tuto" : {
    "aliases" : { },
    "mappings" : { },
    "settings" : {
      "index" : {
        "routing" : {
          "allocation" : {
            "include" : {
              "_tier_preference" : "data_content"
            }
          }
        },
        "number_of_shards" : "3",
        "provided_name" : "index_tuto",
        "creation_date" : "1611653514254",
        "number_of_replicas" : "1",
        "uuid" : "XnpwE0lbSk2JSof9215M3w",
        "version" : {
          "created" : "7100299"
        }
      }
    }
  }
}


- on peut creer notre mapping en même temps que l'index :

curl -X PUT "localhost:9200/test?pretty" -H 'Content-Type: application/json' -d'
{
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "_doc" : {
            "properties" : {
                "field1" : { "type" : "text" }
            }
        }
    }
}
'
/!\ attention example pri sur site es ..ne fonctionne pas ...


- suppression d'index :

DELETE test1

{
  "acknowledged" : true
}

- api de document :

-> api à document unique

la creation d'un records dans notre index se fait assez facilement : 

PUT testo/_doc/1
{
  "user": "bob",
  "age": 12

}

{
  "_index" : "testo",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}

si le param autocreate index est défini a true dans notre conf elastic, on peut donc créer un index et un premier document directement dans es ( sans passer par la creation d'un index puis ensuite dans un second temps la creation d'un doc pour cet index. )

on peut modifier cette conf via l'api :


PUT _cluster/settings
{

  "persistent":
  {
    "action.auto_create_index": "true"  < mettre à false si on veut désactiver cette option.
  }

}

- affichage de doc :

GET testo1/_doc/1

nous donne : 
{
  "_index" : "testo1",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "_seq_no" : 0,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "user" : "bob",
    "age" : 12
  }
}

- suppression de doc :

DELETE testo1/_doc/1

- Mise à jour du doc :

on va pouvoir modifier un element de notre doc 

ici le user bob present précedemment dans notre doc  va être remplacé par babar

POST testo1/_doc/1/_update
{
  "doc":
   {
  "user": "babar"
   }
}

qui nous donne donc :

{
  "_index" : "testo1",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 2,
  "_seq_no" : 4,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "user" : "babar",
    "age" : 12
  }
}

La methode a utilser est plutot maintenant :

POST testo1/_update/1/
{
  "doc":
   {
  "user": "bouli"
   }
}

qui nous donne :

GET testo1/_doc/1

{
  "_index" : "testo1",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 3,
  "_seq_no" : 5,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "user" : "bouli",
    "age" : 12
  }
}

- Multi documents :

permet d'intervenir sur plusieurs docs en même temps 

GET index/_doc/_mget < recup tous les docs de l'index 

GET /testo/_doc/_mget
/!\ ne marche pas ex sur site es

= Gestion de mapping :

indexation > recoit le message le traite et le stocke dans un index
recherche > recherche d'info dans l'index
les deux phase sont liées : si l'index n'est pas bon : la recherche ne le sera pas.
es a un mapping explicite defini.
si on en met pas : un par defaut est associé à l'index.

Il est possible de définir et modifier un mapping.

- utilisation de la creation du mapping explicte 

si on compare l'index a une  bdd 
le mapping est similaire a la definition de la table

es est capable de comprendre la structure et est capable de créer le mapping

on cree un index :

PUT test_mapping

on va cree un doc dans notre index :

PUT test_mapping/_doc/1
{
  "prenom": "lapin",
  "nom": "nain"
}

pour voir le mapping implicte ( crée automatiquement par es sur notre index : )

GET test_mapping/_mapping

ce qui nous donne :

{
  "test_mapping" : {
    "mappings" : {
      "properties" : {
        "nom" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
        },
        "prenom" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
        }
      }
    }
  }
}

on voit que le type des champ est détecté automatiquement par es 
ici on a deux type text

on peut donc ici facilement et rapidement insérer des données dans es ..

- mapping de type de base :

pour avoir des bonnes perf d'indexation on va définir des mappings 

> reduction taille index sur disque
> indexation uniquement des champs interressants > gain de perfs
> defini correctement l'analysee du champ

es permet l'utilisant de champ de base


 PUT /my-index-000001
{
  "mappings": {
    "properties": {
      "age":    { "type": "integer" },
      "email":  { "type": "keyword"  },
      "name":   { "type": "text"  }
    }
  }
}

GET my-index-000001/_mapping

{
  "my-index-000001" : {
    "mappings" : {
      "properties" : {
        "age" : {
          "type" : "integer"
        },
        "email" : {
          "type" : "keyword"
        },
        "name" : {
          "type" : "text"
        }
      }
    }
  }
}

on peut avoir des setting

> store : garder sur disque ou pas ( booléen)
> index : savoir si on index ou pas le champ ( booleen )
..

- mapping de tableaux :

en sql il faut plusieurs tables et faire des jointures.
es en json prend nativement en charge les tableaux.

on peut donc avoir un mapping 

{
  "properties": 
    {
      "name": 
        {
          "type": "keyword"
        }
      "tag:"
        {
          "type": "keyword",
          "store": "true"
        }

    }

}

et deux docs différents :

{"name": "doc1", "tag": "yes" } 
{"name": "doc2", "tag": ["cool", "array", "story"] } 


}

== Analyse de text et mapping : 

aspect important pour préparer les données afin d'avoir une analyse ensuite précise de données.

le mapping dynamic detecte automatique le type mais peut se tromper 
il peut donc être important de créer son mapping

- analyse de text :

correspond à la convertion de text en token qui sont ensuite ajoutés à l'index inversé et utilisés pour la recherche.
chaque analyse de text est fait par un analyzer.

ex : 
Hello world! This is my first program and these are my first lines.

devient après le passage de l'analyzer  :

[hello, world, first, program, these, line]

les mots les plus courant sont supprimés, les maj transformées en min et les pluriels passés en singulier.

on peu spécifier l'analyzer dans le mapping.

PUT my-test-analizer
{

  "settings":{
      "analysis":{
         "analyzer":{
            "my_analyzer":{
               "type":"custom",
               "tokenizer":"standard",
               "filter":[
                  "lowercase"
               ]
            }
         }
      }
  }
}
 
nous donne : 
{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "my-test-analizer"
}

si rien n'est défini c'est l"analyzer standard qui sera defini par default.
L'analyzer est utilisé pendant l'indexation et la recherche.

= structure d'analyzer

un analyzer est composé de trois grands éléments :

> caracter filter
> tokenizer
> token filter 

- caracter filter :
convertit le texte en un flux de caracter et peut proceder a des ajouts, suppressions ou modifs de caracteres ( ex remplacer un & du text en "and" ) ..
- tokenizer :
va decouper le flux de données en token.
il peut par exemple utiliser l'espace pour separateur 
Hello World! 
devient apres le passage au tokenizer 
[hello,world]
- token filter : 
convertisseur de flux de token (ajout, suppression, convertion de token)
ex dans Hello World! > qui devient [hello, world] > c'est le filter de token qui a modifié en min.

- Utilisation d'un analyzer :

on peut utiliser le point d'entrée _analyze de l'api pour comprendre.

POST _analyze
{
  "analyzer": "whitespace",
  "text": "Hello World! How are you ?"
}

nous donne :

{
  "tokens" : [
    {
      "token" : "Hello",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "World!",
      "start_offset" : 6,
      "end_offset" : 12,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "How",
      "start_offset" : 13,
      "end_offset" : 16,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "are",
      "start_offset" : 17,
      "end_offset" : 20,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "you",
      "start_offset" : 21,
      "end_offset" : 24,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "?",
      "start_offset" : 25,
      "end_offset" : 26,
      "type" : "word",
      "position" : 5
    }
  ]
}

example avec un filter :
POST _analyze
{
  "tokenizer": "standard",
  "filter": ["lowercase"],
  "text": "Hello World! How are you ?"
}

nous donne :

{
  "tokens" : [
    {
      "token" : "hello",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "how",
      "start_offset" : 13,
      "end_offset" : 16,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "are",
      "start_offset" : 17,
      "end_offset" : 20,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "you",
      "start_offset" : 21,
      "end_offset" : 24,
      "type" : "<ALPHANUM>",
      "position" : 4
    }
  ]
}

- analyzer customisé :
on peut construire un analyzer personnalisé pour rechercher dans notre index :

PUT custom_analyser
{
   "settings":{
      "analysis":{
         "analyzer":{
            "my_analyzer":{ 
               "type":"custom",
               "tokenizer":"standard",
               "filter":[
                  "lowercase"
               ]
            },
            "my_stop_analyzer":{ 
               "type":"custom",
               "tokenizer":"standard",
               "filter":[
                  "lowercase",
                  "english_stop"
               ]
            }
         },
         "filter":{
            "english_stop":{
               "type":"stop",
               "stopwords":"_english_"
            }
         }
      }
   },
   "mappings":{
       "properties":{
          "title": {
             "type":"text",
             "analyzer":"my_analyzer", 
             "search_analyzer":"my_stop_analyzer", 
             "search_quote_analyzer":"my_analyzer" 
         }
      }
   }
}

{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "custom_analyser"
}

on pourra ensuite rechercher dans un index en utilisant et spécifiant l'analyzer custom qu'on a créer 

GET custom_analyser
  {
    "analyzer": "my_analyzer",
    "text": "LAPIN NAIN"

  }

{
  "custom_analyser" : {
    "aliases" : { },
    "mappings" : {
      "properties" : {
        "title" : {
          "type" : "text",
          "analyzer" : "my_analyzer",
          "search_analyzer" : "my_stop_analyzer",
          "search_quote_analyzer" : "my_analyzer"
        }
      }
    },
    "settings" : {
      "index" : {
        "routing" : {
          "allocation" : {
            "include" : {
              "_tier_preference" : "data_content"
            }
          }
        },
        "number_of_shards" : "1",
        "provided_name" : "custom_analyser",
        "creation_date" : "1611826352794",
        "analysis" : {
          "filter" : {
            "english_stop" : {
              "type" : "stop",
              "stopwords" : "_english_"
            }
          },
          "analyzer" : {
            "my_analyzer" : {
              "filter" : [
                "lowercase"
              ],
              "type" : "custom",
              "tokenizer" : "standard"
            },
            "my_stop_analyzer" : {
              "filter" : [
                "lowercase",
                "english_stop"
              ],
              "type" : "custom",
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_replicas" : "1",
        "uuid" : "KtMQDe87TJ2ZOv7Q5Fxn2A",
        "version" : {
          "created" : "7100299"
        }
      }
    }
  }
}
  
= analyzer standard : 

basé sur unicode et utilisé par défaut si rien n'est spécifié.
Il accepte certains params :

max_token_lenght > taille maxi de caractere dans le token ( si le mot dépasse la taille max : il est coupé. )
stopword : definit une liste de mot d'arret predefini ( mots vides de sens "à , vers ..) on peut conserver ou nettoyé  . On peut definir le param english utilise les mots anglais pour le tri


PUT custom_analyser2
{
   "settings":{
      "analysis":{
         "analyzer":{
            "my_analyzer":{
               "type":"standard",
               "max_token_lenght": 10,
               "stopwords": "english"
            }
         }
      }
   }
}

POST custom_analyser2/_analyze
{
  "analyzer": "my_analyzer",
  "text": "grandmotdeplusdedix caracteres"

}

- analyseur simple :

divise le texte en interval a chaque fois qu'il rencontre des caracteres nons alphabetiques.
il ne peut pas etre configuré pour du numerique et contient des tokenizer de type lowercase.

si on cree un analyzer simple et que dans notre text on a par example des chiffres : ceux ci ne seront donc pas pri en compte et isolé dans un token :

PUT analyzer_simple
{
   "settings":{
      "analysis":{
         "analyzer":{
            "my_simple":{
               "tokenizer": "lowercase",
               "filter": ["lowercase", "asciifolding"]
            
              
            }
         }
      }
   }
} 
en envoyant du text contenant des chiffres : 
POST analyzer_simple/_analyze
{
  "analyzer": "my_simple",
  "text": "les 2 lapins"
  
}

on voit que le text analysé ne contient que 2 tokens :

{
  "tokens" : [
    {
      "token" : "les",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "lapins",
      "start_offset" : 6,
      "end_offset" : 12,
      "type" : "word",
      "position" : 1
    }
  ]
}

- analyser whitespace :

decompose le texte des qu'il rencontre un espace blanc
on peut eventuellement le customiser 

- analyser stop : 

va supprimer les mots "vides" en anglais.

n peut customiser et definirt nous même les mots "vides" :

ex :


PUT analyzer_stopword
{
   "settings":{
      "analysis":{
         "analyzer":{
            "my_stop":{
               "type": "stop",
               "stopwords": ["rabbit", "and", "english"]


            }
         }
      }
   }
}




PUT analyzer_stopword
{
   "settings":{
      "analysis":{
         "analyzer":{
            "my_stop":{
               "type": "stop",
               "stopwords": ["rabbit", "and", "_english_"]
            
              
            }
         }
      }
   }
} 

POST analyzer_stopword/_analyze
{
   "analyzer": "my_stop",
   "text": "1 rabbit and or 1 rat"
  
}

nous donne :
{
  "tokens" : [
    {
      "token" : "or",
      "start_offset" : 13,
      "end_offset" : 15,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "rat",
      "start_offset" : 18,
      "end_offset" : 21,
      "type" : "word",
      "position" : 3
    }
  ]
}


- analyzer keyword :

 renvoit la chaine entiere sous forme de token 

- analyzer de pattern :

divise le text en terme en fonction de regex .
la regex doit correspondre au séparateur de token.

l'utilisation de base : 
POST _analyze
{
  "analyzer": "pattern",
  "text": "This is my text 1"

}

nous donne 

{
  "tokens" : [
    {
      "token" : "this",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "my",
      "start_offset" : 8,
      "end_offset" : 10,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "text",
      "start_offset" : 11,
      "end_offset" : 15,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "1",
      "start_offset" : 16,
      "end_offset" : 17,
      "type" : "word",
      "position" : 4
    }
  ]
}

on peut definir un pattern personalisé :
example ici on considere que tous les caracteres non alphanum et le "_" sont des separateurs pour isoler les mots d'une adresse mail par ex  

PUT analyzer_pattern
{
   "settings":{
      "analysis":{
         "analyzer":{
            "email_pattern":{
               "type": "pattern",
               "pattern": "\\W|_",
               "Lowercase": true

            }
         }
      }
   }
}

POST analyzer_pattern/_analyze
{
  "analyzer": "email_pattern",
  "text": "John_Do@bob-net.com"

nous donne :

{
  "tokens" : [
    {
      "token" : "john",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "do",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "bob",
      "start_offset" : 8,
      "end_offset" : 11,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "net",
      "start_offset" : 12,
      "end_offset" : 15,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "com",
      "start_offset" : 16,
      "end_offset" : 19,
      "type" : "word",
      "position" : 4
    }
  ]
}


il y a encore d'autres analysers.


= normalizer :

type d'analyzer : au lieu de produire plusieurs token : ils n'en produisent qu'un.
filtre :
asciifolding, lowercase, uppercase ...

= tokenizer : 

comme on l'a vu , ils prennent le flux de text rentrant et par default definissent en token chaque mot séparé par un ou des espaces.

- standard :
utilise la segmentation de base pour diviser le texte :

POST  _analyze
{
  "tokenizer": "standard",
  "text": "Who's in the kitchen ?"
}

{
  "tokens" : [
    {
      "token" : "Who's",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "in",
      "start_offset" : 6,
      "end_offset" : 8,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "the",
      "start_offset" : 9,
      "end_offset" : 12,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "kitchen",
      "start_offset" : 13,
      "end_offset" : 20,
      "type" : "<ALPHANUM>",
      "position" : 3
    }
  ]
}

on peut mettre des paramètres 
max token lenght ... 

- tokenizer : de lettres 

chaque fois que du texte n'est pas une lettre c'est exclu :

POST  _analyze
{
  "tokenizer": "letter",
  "text": "Who's in the kitchen ?"
}


{
  "tokens" : [
    {
      "token" : "Who",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "s",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "in",
      "start_offset" : 6,
      "end_offset" : 8,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "the",
      "start_offset" : 9,
      "end_offset" : 12,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "kitchen",
      "start_offset" : 13,
      "end_offset" : 20,
      "type" : "word",
      "position" : 4
    }
  ]
}

on voit que le "'" a disparu.

- tokenizer de lowercase :

transforme les caracteres maj en min 

POST  _analyze
{
  "tokenizer": "lowercase",
  "text": "Who's in the kitchen ?"
}


{
  "tokens" : [
    {
      "token" : "who",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "s",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "in",
      "start_offset" : 6,
      "end_offset" : 8,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "the",
      "start_offset" : 9,
      "end_offset" : 12,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "kitchen",
      "start_offset" : 13,
      "end_offset" : 20,
      "type" : "word",
      "position" : 4
    }
  ]
}

- tokenizer : whitespace 

a chaque fois qu'on a un espace > token / mot individuel 

- tokenizer keyword :

genere le texte en un seul token.

POST  _analyze
{
  "tokenizer": "keyword",
  "text": "Who's in the kitchen ?"
}

{
  "tokens" : [
    {
      "token" : "Who's in the kitchen ?",
      "start_offset" : 0,
      "end_offset" : 22,
      "type" : "word",
      "position" : 0
    }
  ]
}
on peut definir le nombre de caracteres lu dans le buffer size 

- tokenizer de pattern :

utilisation de regex 

le pattern de base sans précision est : \W+ : separateur habituel d'espace blanc.

POST  _analyze
{
  "tokenizer": "pattern",
  "text": "Who's in the kitchen ?"
}

{
  "tokens" : [
    {
      "token" : "Who",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "s",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "in",
      "start_offset" : 6,
      "end_offset" : 8,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "the",
      "start_offset" : 9,
      "end_offset" : 12,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "kitchen",
      "start_offset" : 13,
      "end_offset" : 20,
      "type" : "word",
      "position" : 4
    }
  ]
}

on peut parametrer exemple ici séparer chaque mot en considérant "," comme séparateur.

PUT token_pattern

{
 "settings": {
    "analysis": {
       "analyzer": {
          "my_analyzer_pattern": {
           "tokenizer": "my_token_pattern"
           }
         },
        "tokenizer": {
          "my_token_pattern": {
              "type": "pattern",
              "pattern": ","
          }
         }
      }
    }
}

POST token_pattern/_analyze

{
  "analyzer": "my_analyzer_pattern",
  "text": "Who, is , it ?"


}

/!\ Exemple ne fonctionne pas   


- filtres de tokenizer :

peut ajouter, modifier, supprimer les données recues 
lowercase, uppercase, stop, reverse (inversion des tokens), elision (ex: l'avion devient avion) ,unique , truncate  ....

- filtres de caracteres :

filtre les données en entrées avant d'envoyer au tokenizer

>html_strip : supprimer les caracteres des éléments html

on peut ajouter un filtre de tokenizer html_strip

PUT cust_html
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "char_filter": [
            "html_strip"
          ]
        }
      }
    }
  }
}



GET cust_html/_analyze
{
  "tokenizer": "keyword",
  "char_filter": [
    "html_strip"
  ],
  "text": "<p>Oh yeah <b>happy</b>!</p>"
}


{
  "tokens" : [
    {
      "token" : """
Oh yeah happy!
""",
      "start_offset" : 0,
      "end_offset" : 28,
      "type" : "word",
      "position" : 0
    }
  ]
}

- filtre de caractere de mapping :



- filtre de pattern :



== Recherche dans elasticsearch : 

il peut être important de comprendre le contexte et ajuster correctement la recherche de données.
on peut modifier des paramétrages pour accelerer la recherche 

des éléments interviennent dans la recherche :

> temps de reponse entre noeud master et noeuds contenant les données
> temps de reponse sur les requettes de recherche précédentes effectuées sur un noeud data
> taille de fille d'attente des requettes envoyées aux noeuds master

on peut definir 
cluster_routing adaptative > true 

https://www.elastic.co/guide/en/elasticsearch/reference/current/search-shard-routing.html


Adaptive replica selection
By default, Elasticsearch uses adaptive replica selection to route search requests. This method selects an eligible node using shard allocation awareness and the following criteria:

Response time of prior requests between the coordinating node and the eligible node
How long the eligible node took to run previous searches
Queue size of the eligible node’s search threadpool
Adaptive replica selection is designed to decrease search latency. 
However, you can disable adaptive replica selection by setting cluster.routing.use_adaptive_replica_selection to false using the cluster settings API. If disabled, Elasticsearch routes search requests using a round-robin method, which may result in slower searches.

cluster.routing.use_adaptive_replica_selection  >  true de base
on peut parametre le temps de recherche max :
search.default.search.timeout 

beacoup de params sont a prendre en compte et sont potentiellement a ajuster en fonction de notre infra matérielle et de nos data 
search.low_level_cancellation
max_concurrent_shard_request
action.search_shard_count.limit

Api de recherche :

> _search
contruction de requette 

_search?
q > query
clé:valeur recherchée 

GET  kibana_sample_data_ecommerce/_search?q=user:jason

{
  "took" : 8,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 65,
      "relation" : "eq"
    },
    "max_score" : 4.268148,
    "hits" : [
      {
        "_index" : "kibana_sample_data_ecommerce",
        "_type" : "_doc",
        "_id" : "iC3XUncB_UDcOEMa5Rdi",
        "_score" : 4.268148,
        "_source" : {
          "category" : [
            "Men's Accessories",
            "Men's Clothing"
          ],
          "currency" : "EUR",
          "customer_first_name" : "Jason",
          "customer_full_name" : "Jason Cummings",
          "customer_gender" : "MALE",
          "customer_id" : 16,
          "customer_last_name" : "Cummings",
          "customer_phone" : "",
          "day_of_week" : "Wednesday",
          "day_of_week_i" : 2,
          "email" : "jason@cummings-family.zzz",
          "manufacturer" : [
            "Low Tide Media",
            "Elitelligence"
          ],
...
.....

on peut egalement rechercher des infos avec des tags 



GET  kibana_sample_data_ecommerce/_search?q=tag:lapin

on peut chercher dans tous les index :
exemple : 
GET _all/_search?q=order_id:*

on peut passer des params à notre recherche 

q > query comme on l'a vu
df > champ par default
analyser > analyser utilisé pour la recherche
explain > contient l'explication du détail du score retrouvé.
stored_field > 
timeout > temps max de la requette

https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html


on peut definir le nbr d'elément retourné par la requette :

https://www.elastic.co/guide/en/elasticsearch/reference/current/paginate-search-results.html

par defaut le nbr d'entrées retournées est de 10 mais on peut définir à partir de quel élément on veut le retour et le nbr d'élément 


GET /_search
{
  "from": 5,
  "size": 20,
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}

on peut trier par documents en utilisant des fonctions ( sort ...) 

https://www.elastic.co/guide/en/elasticsearch/reference/current/sort-search-results.html

ex :

on cree deux index contenant un doc contenant avec une valeur de prix pour chocloat 

on peut ensuite établir notre recherche avec des criteres : ex tri num decroissant  : 

ce qui nous donne :

 PUT /my-index-000001/_doc/1?refresh
{
   "product": "chocolate",
   "price": 7
}

PUT /my-index-000002/_doc/1?refresh
{
   "product": "chocolate",
   "price": 4
}


POST /_search
{
   "query" : {
      "term" : { "product" : "chocolate" }
   },
   "sort" : [
      {"price" : {"order" : "desc", "mode" : "avg"}}
   ]
}   
  }
}

le resultat est bien celui attendu par ordre de tri : 
      {
        "_index" : "my-index-000001",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : null,
        "_source" : {
          "product" : "chocolate",
          "price" : 7
        },
        "sort" : [
          7
        ]
      },
      {
        "_index" : "my-index-000002",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : null,
        "_source" : {
          "product" : "chocolate",
          "price" : 4
        },
        "sort" : [
          4
        ]
      }
    ]
  }
}




- doc_values
Most fields are indexed by default, which makes them searchable. The inverted index allows queries to look up the search term in unique sorted list of terms, and from that immediately have access to the list of documents that contain the term.

Sorting, aggregations, and access to field values in scripts requires a different data access pattern. Instead of looking up the term and finding documents, we need to be able to look up the document and find the terms that it has in a field.

Doc values are the on-disk data structure, built at document index time, which makes this data access pattern possible. They store the same values as the _source but in a column-oriented fashion that is way more efficient for sorting and aggregations. Doc values are supported on almost all field types, with the notable exception of analyzed string fields.

All fields which support doc values have them enabled by default. If you are sure that you don’t need to sort or aggregate on a field, or access the field value from a script, you can disable doc values in order to save disk space:

PUT my_index
{
  "mappings": {
    "_doc": {
      "properties": {
        "status_code": {                      <<< on ne precise pas le doc_value : enable par default
          "type":       "keyword"
        },
        "session_id": { 
          "type":       "keyword",
          "doc_values": false                <<<< ici on désactive volontairement
        }
      }
    }
  }
}

> The status_code field has doc_values enabled by default.
>The session_id has doc_values disabled, but can still be queried.




- recherche par champ field :

Fields
The stored_fields parameter is about fields that are explicitly marked as stored in the mapping, which is OFF by default and generally NOT recommended. Use source filtering instead to select subsets of the original source document to be returned.




Doc value Fieldsedit
Allows to return the doc value representation of a field for each hit, for example:

GET /_search
{
    "query" : {
        "match_all": {}
    },
    "docvalue_fields" : [
        {
            "field": "my_ip_field",   >>> the name of the field
            "format": "use_field_mapping" >>> the special use_field_mapping format tells Elasticsearch to use the format from the mapping
        },
        {
            "field": "my_date_field",      >>> date fields may use a custom format
            "format": "epoch_millis"
        }
    ]
}

Doc value fields can work on fields that are not stored.

* can be used as a wild card, for example:

GET /_search
{
    "query" : {
        "match_all": {}
    },
    "docvalue_fields" : [
        {
            "field": "*field",
            "format": "use_field_mapping"
        }
    ]
}


= type de recherche :

l'operation de recherche est faite sur tous les shards. Les resultats sont recup et triés par le master.


https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html

différentes méthodes existent 

ex : template, shards ...

GET food/_search_shards

{
  "nodes" : {
    "FnbdoPhIS8qFi1GoO-vKTg" : {
      "name" : "elasticsearch-master-2",
      "ephemeral_id" : "QTlcV7zXQPWJFUnDtz76zw",
      "transport_address" : "172.17.0.10:9300",
      "attributes" : {
        "ml.machine_memory" : "512000000",
        "ml.max_open_jobs" : "20",
        "xpack.installed" : "true",
        "transform.node" : "true"
      }
    },
    "zVl54Y-9RTyY6NLDaBiLyA" : {
      "name" : "elasticsearch-master-1",
      "ephemeral_id" : "aKlmE9aDS2iJTA0I0xMeTQ",
      "transport_address" : "172.17.0.19:9300",
      "attributes" : {
        "ml.machine_memory" : "512000000",
        "xpack.installed" : "true",
        "transform.node" : "true",
        "ml.max_open_jobs" : "20"
      }
    }
  },
  "indices" : {
    "food" : { }
  },
  "shards" : [
    [
      {
        "state" : "STARTED",
        "primary" : true,
        "node" : "FnbdoPhIS8qFi1GoO-vKTg",
        "relocating_node" : null,
        "shard" : 0,
        "index" : "food",
        "allocation_id" : {
          "id" : "LOEuIpzQQ_C5BkeA8oWYbg"
        }
      },
      {
        "state" : "STARTED",
        "primary" : false,
        "node" : "zVl54Y-9RTyY6NLDaBiLyA",
        "relocating_node" : null,
        "shard" : 0,
        "index" : "food",
        "allocation_id" : {
          "id" : "4Lcaw597QV-L0I-iJFIikg"
        }
      }
    ]
  ]
}

COUNT :
 GET /my-index-000001/_count?q=product


{
  "count" : 0,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  }
}

- validate : 

on peut utiliser la fonction de validation d'un requette pour voir si la requette sera couteuse : 

GET my-index-000001/_validate/query?q=product

{
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "valid" : true
}


== Aggregation :

https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html

on peut regrouper les data pour les présenter sous formes de graphs.Plusieurs type d'aggrégations :

> metrics
> bucket 
> pipeline
...



GET /my-index-000001/_search
{
  "aggs": {
    "my-agg-name": {
      "terms": {
        "field": "my-field"
      }
    }
  }
}



Preparation du dataset pour travailler nos données : on injecte les datatests présentes dans kibana ex index kibana_sample_data_ecommerce

- on tri par type de produits avec un résultat de 10 records 

POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "tag": {
      "terms": {
        "field": "products",
        "size": 10
      }
    }
  }

}

nous donne : 

{
  "took" : 40,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "tag" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ ]
    }
  }
}

- aggregation de metriques 

statistiques ; moyenne, cardinalité, max, min, centille

calcul des requettes sur l'age :
min / max /moyenne /somme 

ici on va sortir les données statistiques sur le prix des produits :

POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "age": {                                       <<<<< nom que l'on donne 
      "extended_stats": {                          <<<<< metrique d'aggregation
        "field": "products.base_unit_price"        <<<<< champ sur lequel on travaille.
      }
    }
  }
}


nous donne :

{
  "took" : 27,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "age" : {
      "count" : 10087,
      "min" : 5.98828125,
      "max" : 540.0,
      "avg" : 34.77499349410132,
      "sum" : 350775.359375,
      "sum_of_squares" : 1.8856063677703857E7,
      "variance" : 660.0429104339428,
      "variance_population" : 660.0429104339428,
      "variance_sampling" : 660.1083519281361,
      "std_deviation" : 25.69130028694427,
      "std_deviation_population" : 25.69130028694427,
      "std_deviation_sampling" : 25.692573867328594,
      "std_deviation_bounds" : {
        "upper" : 86.15759406798986,
        "lower" : -16.607607079787222,
        "upper_population" : 86.15759406798986,
        "lower_population" : -16.607607079787222,
        "upper_sampling" : 86.1601412287585,
        "lower_sampling" : -16.61015424055587
      }
    }
  }
}


on peut calculer uniquement la moyenne :


POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "avg-age": {
      "avg": {
        "field": "products.base_unit_price"
      }
    }
  }
}
nous donne donc :

{
  "took" : 7,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "avg-age" : {
      "value" : 34.77499349410132
    }
  }
}

- bucket aggregation :

chaque bucket a un type de doc qui s'y trouve
on peut avoir des sous aggregation de bucket ( bucket parent / enfant )

agreggation de terme, range , global, histogramme ...

- terme : 

POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "tag": {          <<<<< on met le nom qu'on veut 
      "terms": {      <<<<  ici on applique l'aggregation sur les termes 
        "field": "customer_last_name.keyword" <<<<< le champ de notre table qu'on va parser 
      }
    }
  }
}

nous donne :

{
  "took" : 16,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "tag" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 4140,
      "buckets" : [
        {
          "key" : "Perkins",
          "doc_count" : 59
        },
        {
          "key" : "Underwood",
          "doc_count" : 59
        },
        {
          "key" : "Tran",
          "doc_count" : 56
        },
        {
          "key" : "Rivera",
          "doc_count" : 54
        },
        {
          "key" : "Graham",
          "doc_count" : 53
        },
        {
          "key" : "Shaw",
          "doc_count" : 53
        },
        {
          "key" : "Byrd",
          "doc_count" : 51
        },
        {
          "key" : "Foster",
          "doc_count" : 51
        },
        {
          "key" : "Gregory",
          "doc_count" : 51
        },
        {
          "key" : "Bryant",
          "doc_count" : 48
        }
      ]
    }
  }
}


par defaut 10 records sont rencoyés par es ( on peut biensur définir ce nombre avec "size" )

- aggregation de range :

on va pouvoir aggreger nos resultats par intervals 
ex : on tri les prix par interval de prix :

POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "prices": {
      "range": {
        "field": "products.price",
        "ranges": [
          {"to": 10},
          {"from": 10, "to": 100},
          {"from": 101, "to": 5000}
        ]
      }
    }
  }
}


 ce qui nous donne :
{
  "took" : 38,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "prices" : {
      "buckets" : [
        {
          "key" : "*-10.0",
          "to" : 10.0,
          "doc_count" : 357
        },
        {
          "key" : "10.0-100.0",
          "from" : 10.0,
          "to" : 100.0,
          "doc_count" : 4655
        },
        {
          "key" : "101.0-5000.0",
          "from" : 101.0,
          "to" : 5000.0,
          "doc_count" : 185
        }
      ]
    }
  }
}

on peut trier par interval de date :

POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "prices": {
      "date_range": {
        "field": "order_date",
        "format": "MM-yyy"
        , "ranges": [
            { "to": "now-10M/M" },
            { "from": "now-10M/M" }
         ]
      }
    }
  }
}POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "prices": {
      "date_range": {
        "field": "order_date",
        "format": "MM-yyy"
        , "ranges": [
            { "to": "now-10M/M" },
            { "from": "now-10M/M" }
         ]
      }
    }
  }
}

{
  "took" : 16,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "prices" : {
      "buckets" : [
        {
          "key" : "*-04-2020",
          "to" : 1.5856992E12,
          "to_as_string" : "04-2020",
          "doc_count" : 0
        },
        {
          "key" : "04-2020-*",
          "from" : 1.5856992E12,
          "from_as_string" : "04-2020",
          "doc_count" : 4675
        }
      ]
    }
  }
}

- agregation d'histogramme :


visualisation de données.

kibana > add visualisation > add vertical bar

il faut cependant aggreger les données.

on va trier le prix de nos produits par interval de 10$


POST kibana_sample_data_ecommerce/_search?size=0
{
  "aggs": {
    "prices": {
      "histogram": {
        "field": "products.price",
        "interval": 10
      }
    }
  }
}

nous donne 

{
  "took" : 32,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 4675,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "prices" : {
      "buckets" : [
        {
          "key" : 0.0,         <<<< abcisse 
          "doc_count" : 357    <<<<  ordonnée
        },
        {
          "key" : 10.0,
          "doc_count" : 2233
        },
        {
          "key" : 20.0,
          "doc_count" : 2356
        },
        {
          "key" : 30.0,
          "doc_count" : 969
        },
   ...

on peut donc maintenant mettre en place notre histogramme.

ajout d'index pattern 

kibana > index pattern  > on cherche un index pattern qui match un index crée auparavant > on defini par example le tri de filtre par date .
on voit ensuite la structure de notre index

on va ensuite sur kibana vizualise > vertical bar > on choisi notre index

metric vont permettre de faire des count
bucket : histogramme, range 

- aggregation histogramme de date :

pas mal de travail pour ces types : conversin fuseau horaires ....

POST kibana_sample_data_logs/_search?size=0
{
  "aggs": {
    "dates": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "day"
      }
    }
  }
}

nous donne 

{
  "took" : 32,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 10000,
      "relation" : "gte"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "dates" : {
      "buckets" : [
        {
          "key_as_string" : "2021-01-17T00:00:00.000Z",
          "key" : 1610841600000,
          "doc_count" : 249
        },
        {
          "key_as_string" : "2021-01-18T00:00:00.000Z",
          "key" : 1610928000000,
          "doc_count" : 231
        },
        {
          "key_as_string" : "2021-01-19T00:00:00.000Z",
          "key" : 1611014400000,
          "doc_count" : 230
        },
..
....
les intervalles de temps sont fixes mais plusieurs valeurs sont définies (hour, month, year, timezone ....)

kibana > visualization > create > histogramme > choisi notre index > bucket > x > aggregation : date_histogram > field : timestamp par exemple.




